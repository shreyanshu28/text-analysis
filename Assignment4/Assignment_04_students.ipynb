{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyanshu28/text-analysis/blob/main/Assignment4/Assignment_04_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUgbolxu5quo"
      },
      "source": [
        "# Assignment 4: Keyphrase Extraction, Named Entity Recognition & Neural Models\n",
        "\n",
        "Due: Monday, February 06, 2023, at 2pm via Moodle\n",
        "\n",
        "**Team Members** `1.Yanxin Jia, 3769165\n",
        "2.Shreyansu Vyas, 3769429\n",
        "3.Smaran Nair , 3771609\n",
        "4.AnuReddy , 3768482`\n",
        "\n",
        "Please note that this assignment comes with quite a number of artifacts, totaling somewhere around 5 GB of necessary disk space. In case you are running into issues or do want to keep your environment \"clean\", we suggest the use of [Google Colab](https://colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-su5u2SbXPQ2"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        ". ~/.bashrc\n",
        "python3 -m pip install keybert\n",
        "python3 -m pip install git+https://github.com/LIAAD/yake\n",
        "python3 -m pip install transformers\n",
        "python3 -m pip install datasets\n",
        "python3 -m pip install nltk\n",
        "python3 -m pip install spacy\n",
        "# Install necessary packages for all questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "315kqIiW2Zhr"
      },
      "source": [
        "## Task 1: Keyphrase Extraction (5 + 3 + 3 + 5) = 16 Points\n",
        "\n",
        "In this task, we will implement our own unsupervised keyphrase extraction (KPE) module utilizing a simple grammatical ruling system, which we apply to a Sherlock Holmes novel.\n",
        "To generate TF-IDF-weighted phrases, we will be using the entire collection of Sir Arthur Donan Coyle novels to calculate document frequencies.\n",
        "\n",
        "Finally, we compare the results to general-purpose KPE libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joME4y0pC-bq"
      },
      "source": [
        "### Sub Task 1: Unsupervised Keyphrase Extraction System (5 Points)\n",
        "\n",
        "#### 1. Candidate Generation\n",
        "We will need to generate a set of suitable candidate phrases first, which can then be ranked as keyphrases later on. To do this, we will again be using spaCy's, this time its rule-based [`Matcher` class](https://spacy.io/api/matcher).\n",
        "\n",
        "The syntactic pattern of a keyphrase candidate should satisfy the following rules:\n",
        "\n",
        "1. An optional adjective, noun, proper noun\n",
        "2. An optional adjective, noun, proper noun\n",
        "3. A mandatory noun or proper noun.\n",
        "\n",
        "Add a second pattern, which recognizes the pattern\n",
        "\n",
        "1. A noun or proper noun\n",
        "2. An adposition\n",
        "3. Another noun or proper noun\n",
        "\n",
        "Note that the first condition will match any phrase of length between 1-3 tokens, which is a suitable approximation for our task at hand, whereas the second pattern is slightly more specific, always matching exactly three tokens.\n",
        "An example of a valid matched phrases for the first pattern would be \"Sherlock Holmes\" ([PROPN, PROPN]), and \"Hounds of Baskervilles\" ([NOUN, ADP, PROPN]) for the second pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7yKhziiDkzJ",
        "outputId": "0212b860-e9c8-4510-8f72-6f3b544f359a"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1cFJOPt3DlZu"
      },
      "outputs": [],
      "source": [
        "# load language model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define the above patterns\n",
        "pattern = [\n",
        "    [{\"POS\":{\"IN\":[\"ADJ\",\"NOUN\",\"PROPN\"]},\"OP\":\"?\"},{\"POS\":{\"IN\":[\"ADJ\",\"NOUN\",\"PROPN\"]},\"OP\":\"?\"},\n",
        "    {\"POS\":{\"IN\":[\"NOUN\",\"PROPN\"]}}],\n",
        "    [{\"POS\":{\"IN\":[\"NOUN\",\"PROPN\"]}},{\"POS\":\"ADP\"},{\"POS\":{\"IN\":[\"NOUN\",\"PROPN\"]}}]\n",
        "\n",
        "]\n",
        "\n",
        "    \n",
        "\n",
        "# Add the pattern to the Matcher\n",
        "matcher.add(\"PATTERN\", pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy9Fb_zvtR_N"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuWXyX6UXPQ6"
      },
      "source": [
        "To verify whether your pattern is correct, use the below example.\n",
        "If you have done everything correctly, your matcher will identify **13 phrases**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71JZGbIlHfxx",
        "outputId": "ef4b6136-f59f-45f9-a0d8-1ea4201fe903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"This is a simple test. It should return 'simple', and 'test', among other phrases. Maybe we can also see if it can recognize the art of war. Would it recognize integer linear programming, too?\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "print(len(matches))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x2TRsI-GeLq"
      },
      "source": [
        "#### 2. Applying Your System\n",
        "\n",
        "Once you have matched the correct number of keyphrase candidates on the above example, apply your rule-based matcher to an actual data sample. We are going to use the Sherlock Holmes novel \"Hounds of Baskervilles\". You can find the raw text file at the following URL:\n",
        "\n",
        "https://sherlock-holm.es/stories/plain-text/houn.txt\n",
        "\n",
        "Download the text from this URL and apply your spaCy model and matcher on it.  \n",
        "**Hint:** Make sure you properly decode your input, since some libraries return binary strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbDFj7DSLQ5v"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "def load_txt_from_url(url: str = \"https://sherlock-holm.es/stories/plain-text/houn.txt\") -> str:\n",
        "  response = urlopen(url)\n",
        "  data = response.read()\n",
        "  text = data.decode(\"utf-8\")\n",
        "  return text\n",
        "\n",
        "text = load_txt_from_url()\n",
        "\n",
        "# Apply the spacy model to the loaded text and extract the phrases with the Matcher\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4U96ELPKtIA"
      },
      "source": [
        "We will now investigate which phrase candidates are the most frequently appearing in this novel, simply based on the phrase frequency. Therefore, convert your abstract match objects into actual strings, lowercase them, and return the 20 most frequently occurring phrase candidates and their respective frequencies.  \n",
        "**Hint:** For counting occurrences, you may look at `collections.Counter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tGof05SIXES",
        "outputId": "90e1d2be-04c0-4dcb-bb97-f48a4ab0c706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('sir', 350), ('man', 214), ('holmes', 192), ('moor', 159), ('henry', 156), ('sir henry', 135), ('watson', 117), ('baskerville', 116), ('dr.', 109), ('charles', 94), ('stapleton', 93), ('mortimer', 89), ('night', 88), ('time', 86), ('sir charles', 86), ('house', 75), ('face', 75), ('hound', 72), ('barrymore', 72), ('eyes', 71)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import collections\n",
        "\n",
        "candidates = []\n",
        "# Lowercase and add the extracted candidate matches to `candidates`\n",
        "## YOUR CODE\n",
        "for match_id,start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span =doc[start:end]\n",
        "    candidates.append(span.text.lower())\n",
        "    \n",
        "\n",
        "candidate_phrases = Counter(candidates)\n",
        "\n",
        "# Print the most frequently occurring phrases, together with the respective frequencies\n",
        "print(candidate_phrases.most_common(20))\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQt8278NA7v"
      },
      "source": [
        "#### 3. Briefly summarize the quality of your top 20 candidates:\n",
        "\n",
        "The top 20 candidate phrases from the Sherlock Holmes novel \"Hounds of Baskervilles\" are a mix of character names, titles, and common words related to the story such as \"hound\" and \"night\". The most frequent phrase is \"sir\" with a frequency of 350, followed by \"man\" with 214 and \"holmes\" with 192. The top 20 candidate phrases are not particularly meaningful or representative of the content of the novel, likely due to the limitations of the simple rule-based matcher used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aucrN3TPJqw1"
      },
      "source": [
        "### Sub Task 2: Generating Document Frequency Values (3 Points)\n",
        "\n",
        "To compare the previously generated terms with a more refined model, we are going to extract document frequencies from the collection of all Sherlock Holmes works. Since the books are relatively long documents, we are instead going to split based on a simple heuristic in the input document, which should allow a decent approximation by taking into account individual chapters of each novel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfp7rUN9N218"
      },
      "source": [
        "1. Start by loading the Sherlock Holmes canon from https://sherlock-holm.es/stories/plain-text/cnus.txt  \n",
        "Afterwards, split the full document into individual chapters. For this, use three consecutive line breaks `\\n\\n\\n` as a splitting condition to approximate the chapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJYRCDJyP-Xp",
        "outputId": "0133c7b8-c5cf-434f-c60b-170bbd5c5aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "353\n"
          ]
        }
      ],
      "source": [
        "df_texts = load_txt_from_url(\"https://sherlock-holm.es/stories/plain-text/cnus.txt\")\n",
        "\n",
        "chapter_split = \"\\n\\n\\n\"\n",
        "split_df_texts = df_texts.split(chapter_split)\n",
        "\n",
        "print(len(split_df_texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRcMyiKkRU7d"
      },
      "source": [
        "After splitting, you should have 353 individual \"documents\" to work with.\n",
        "\n",
        "2. Now, create a dictionary containing each phrase encountered in the larger corpus, and its associated document frequency. Again, ensure that phrase strings are lowercased for consistency with the previous transformation.  \n",
        "**Hint:** Since the processing of 353 documents might take a while, incorporate [`tqdm.tqdm`](https://tqdm.github.io/) to visualize progress on the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtXpYkWQQJi2",
        "outputId": "dd68f6eb-0ac5-40ac-fd69-71e24653b80d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 353/353 [01:13<00:00,  4.81it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "def return_occurring_phrases(doc_text: str) -> List[str]:\n",
        "  # process text with spaCy and apply the Matcher\n",
        "  doc = nlp(doc_text)\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  # Candidates can be a set, since we only care about the occurrence *once* for IDF values.\n",
        "  # Again, extract the lower-cased text of a matched span.\n",
        "  candidates = set()\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    candidates.add(span.text.lower())\n",
        "\n",
        "  return list(candidates)\n",
        "\n",
        "all_document_phrases = []\n",
        "# Iterate through the individual documents and extract phrases for them. Use `tqdm` to visualize progress\n",
        "## YOUR CODE\n",
        "for chapter in tqdm(split_df_texts):\n",
        "    all_document_phrases.extend(return_occurring_phrases(chapter))\n",
        "\n",
        "# Once again, count the frequency of term occurrences across all documents\n",
        "## YOUR CODE\n",
        "phrase_freq = Counter(all_document_phrases)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZjyowpXPQ9"
      },
      "source": [
        "3. Output the 20 most frequently appearing document phrases that your system detected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vDTmrTLSY2q",
        "outputId": "b5b23100-15ff-4d49-9520-c8231c2efe7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('man', 112), ('holmes', 107), ('time', 104), ('eyes', 104), ('night', 104), ('face', 102), ('hand', 102), ('sherlock holmes', 101), ('sherlock', 101), ('way', 101), ('matter', 100), ('room', 99), ('mr.', 98), ('hands', 97), ('day', 97), ('house', 96), ('case', 96), ('life', 96), ('door', 95), ('one', 95)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "most_common_phrases = phrase_freq.most_common(20)\n",
        "print(most_common_phrases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR9FmhTpS_TB"
      },
      "source": [
        "### Sub Task 3: Generating Weighted Keyphrases (3 Points)\n",
        "\n",
        "We can now incorporate the extracted keyphrases to calculate `tf-idf` scores, and return a hopefully improved version of our keyphrases for the original \"Hounds of Baskervilles\" document. \n",
        "\n",
        "1. Iterate over all phrases occurring in the novel \"Hounds of Baskervilles\", and re-score phrases according to the definition of TF-IDF. Use the smoothed definition of idf:\n",
        "\n",
        "$ idf(t, D) = \\log \\frac{|D|}{|\\{d \\in D : t \\in d\\}| + 1} + 1 $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HDDoPiAATWat"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "from typing import Dict\n",
        "\n",
        "def tf_idf(tf: int, df_count: int) -> float:\n",
        "  \"\"\"\n",
        "  Computes the TF-IDF scores according to the formula.\n",
        "  \"\"\"\n",
        "  num_docs = 353\n",
        "  idf = math.log(num_docs / (df_count + 1)) + 1\n",
        "  return tf * idf\n",
        "\n",
        "tf_idf_weighted_candidates = {}\n",
        "\n",
        "\n",
        "# Iterate through all candidate phrase/frequency pairs and compute the TF-IDF scores for each phrase\n",
        "# Store the phrase together with its TF-IDF score in `tf_idf_weighted_candidates`\n",
        "for candidate, tf in candidate_phrases.items():## the number of documents in the corpus that contain the candidate phrase\n",
        "  tf_idf_weighted_candidates[candidate]=tf_idf(tf, phrase_freq[candidate])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSfTPsukVoEF"
      },
      "source": [
        "2. Now print the top 20 candidate phrases by TF-IDF weight, and compare the results to your previous output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w8ulSamTUsGP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('sir', 836.1959348592817), ('moor', 623.6026233649302), ('henry', 588.966394157697), ('sir henry', 552.6737101836245), ('baskerville', 461.2271706883073), ('man', 457.7631709792846), ('holmes', 419.3926713233428), ('stapleton', 400.0412390508737), ('mortimer', 358.9596694460602), ('sir charles', 357.6239356014735), ('charles', 354.8900067360482), ('barrymore', 321.7372404577147), ('dr.', 303.9934368200059), ('watson', 279.5283553672456), ('dr. mortimer', 275.29719676619266), ('hound', 257.0854457468857), ('baronet', 206.4728975746445), ('night', 194.70067819626806), ('hall', 191.75335703217434), ('time', 190.27566278271652)]\n"
          ]
        }
      ],
      "source": [
        "print(sorted(tf_idf_weighted_candidates.items(), key = lambda kv:(kv[1], kv[0]),reverse=True)[0:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdmGgOThVmyz"
      },
      "source": [
        "3. Write your insights on the comparison of the results below. Try to theorize why some of the phrases still appear, or why other phrases are no longer present:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7prjNuouXPRA"
      },
      "source": [
        "The results of TF-IDF weighting show a noticeable improvement compared to the raw frequency count. The most common words, such as \"man\" and \"holmes\", are still present but with a lower weight, indicating that they are less significant in terms of discriminative information compared to other phrases. On the other hand, some new phrases, such as \"Sir Henry\", \"Sir Charles\", and \"Sir Moor\", have appeared with higher weights, which suggests that they are more important and provide more meaningful information about the document. This is because they appear less frequently in the entire corpus of 353 documents, but more often in the specific document of \"Hounds of Baskervilles\".\n",
        "The presence of words like \"time\" and \"night\" in the previous results might have been due to their high frequency across all the documents. However, after incorporating TF-IDF weighting, their weights have decreased, indicating that they are less significant in terms of discriminative information in the specific document of \"Hounds of Baskervilles\".\n",
        "Overall, the results of TF-IDF weighting show a more meaningful and informative representation of keyphrases for the specific document of \"Hounds of Baskervilles\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y8eaRPNWHut"
      },
      "source": [
        "4. Give two examples of how you could further improve the list of keyphrase values."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1yfMZ0etWNj_"
      },
      "source": [
        "1.Incorporating Named Entity Recognition (NER) techniques to identify named entities in the text, such as people, places, organizations, etc. This information could be used to filter out keyphrases that are not meaningful in a specific context.\n",
        "\n",
        "\n",
        "2.Utilizing Word Embeddings to find semantically similar phrases in the text and merge them into a single phrase to reduce the number of similar phrases in the keyphrase list. This could result in a more concise and meaningful set of keyphrases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gceycuSdXPRA"
      },
      "source": [
        "### Sub Task 4: Apply off-the-shelf Keyphrase Extraction Tools (5 Points)\n",
        "\n",
        "To put the findings of your system into context, compare them with two popular open-source libraries, namely [YAKE!](https://github.com/LIAAD/yake) and [KeyBERT](https://github.com/MaartenGr/KeyBERT)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2MGiQjQXPRA"
      },
      "source": [
        "1. First, start by running the document with YAKE!; you may use the default parameters. Print the resulting keyphrases, which by default returns 20 phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install git+https://github.com/LIAAD/yake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mesNMFROXPRA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('remarked Sherlock Holmes', 5.949394080324617e-05), ('Sherlock Holmes face', 5.8356658052643885e-05), ('sir', 5.5356217452982246e-05), ('Sir Charles', 5.330414318372243e-05), ('Baker Street', 5.216905012344322e-05), ('dear Watson', 5.0746857680460824e-05), ('Sherlock Holmes put', 4.92944444985635e-05), ('Sherlock Holmes sprang', 4.4530755662024974e-05), ('Sir Charles Baskerville', 4.435050135434968e-05), ('asked Holmes', 4.4127246582873294e-05), ('Sherlock Holmes left', 4.2728733416566765e-05), ('Man', 3.064333636646172e-05), ('Sir Henry', 2.645741307299696e-05), ('Watson', 2.56607499254069e-05), ('Sir Henry Baskerville', 2.290271021192336e-05), ('Sherlock Holmes sat', 2.019031281138435e-05), ('asked Sherlock Holmes', 1.8636307219539084e-05), ('friend Sherlock Holmes', 1.4905164542368326e-05), ('HOLMES', 3.5475912431521295e-06), ('SHERLOCK HOLMES', 1.244220900143271e-06)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import yake\n",
        "\n",
        "extractor = yake.KeywordExtractor()\n",
        "keywords = extractor.extract_keywords(df_texts)\n",
        "\n",
        "# Print the top 20 keywords\n",
        "sorted_result = sorted(keywords,key=lambda x:(x[1],x[0]),reverse=True)\n",
        "print(sorted_result[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install rake-nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01UDXLakXPRA"
      },
      "source": [
        "2. Compare both runtime efficiency and the extracted phrases with your own system."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qIrMGHBkXPRA"
      },
      "source": [
        "The YAKE! library is a fast and efficient keyphrase extraction tool, which takes less time to run than the system I developed. However, the extracted phrases are not as meaningful as the phrases generated by my system. For example, in my system, the word \"Sir Henry Baskerville\" appeared as a keyphrase, which makes sense given that it is the name of one of the main characters in the story. On the other hand, in YAKE!, the phrase \"friend Sherlock Holmes\" is considered a keyphrase, which is less meaningful and relevant to the story. Additionally, some of the phrases in YAKE! are combinations of words that are not actually meaningful phrases, such as \"asked Holmes.\"\n",
        "Overall, YAKE! is a useful tool for quick and efficient keyphrase extraction, but the results generated by my system are more meaningful and relevant to the story.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwTmlq10XPRB"
      },
      "source": [
        "3. Now use the KeyBERT library to extract keyphrases. Importantly, you will need to split the document into separate paragraphs, as the underlying neural model will be unable to handle the complete document as input.  \n",
        "Use the pattern of `\\n\\n` to separate the text into smaller paragraphs, and filter out any empty lines after. An \"empty line\" also constitutes all inputs that only contain newline (`\\n`) or whitespace ` ` characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Icf29xfcXPRB"
      },
      "outputs": [],
      "source": [
        "# Split the input text according to the specified criteria and filter empty lines out.\n",
        "split_text = [p for p in df_texts.split(\"\\n\\n\") if p.strip() != \"\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9STj-XbnXPRB"
      },
      "source": [
        "4. To ensure consistency between the tools when extracting keyphrases, set the *n*-gram range to `(1,3)`.\n",
        "Otherwise, leave all parameters at the default value, and extract the keyphrases from each paragraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeGRSplLXPRB"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "# This might take a while to install\n",
        "model = KeyBERT(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Extract the keyphrases from each split, using the adjusted keyphrase ngram range\n",
        "# Hint: You may pass a list to the extraction function and KeyBERT will automatically handle iteration.\n",
        "# Set the n-gram range to (1, 3)\n",
        "ngram_range = (1, 3)\n",
        "\n",
        "# Extract the keyphrases from each paragraph\n",
        "extracted_phrases = model.extract_keywords(split_text, keyphrase_ngram_range=ngram_range)\n",
        "print(extracted_phrases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz385Lt6XPRB"
      },
      "source": [
        "5. Combine the predictions of all individual splits into a single list. For this, sum up the prediction scores across all splits.  \n",
        "**Hint:** `collections.defaultdict` makes aggregations like this much easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2ddO4hc4XPRB"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "def merge_predictions(list_of_predictions: List[List[Tuple]]) -> List[Tuple]:\n",
        "    \"\"\"\n",
        "    Combines lists of predictions into a single list with added scores.\n",
        "    \"\"\"\n",
        "  \n",
        "\n",
        "    phrase_dict ={}\n",
        "    for word_list in extracted_phrases:\n",
        "      for (word,score) in word_list:\n",
        "        if word in phrase_dict:\n",
        "          phrase_dict[word] = phrase_dict[word]+score\n",
        "        else:\n",
        "          phrase_dict[word] = phrase_dict.setdefault(word,0)+score\n",
        "          \n",
        "\n",
        "\n",
        "    # Iterate through all the lists of predictions and add the scores to the correct dict entry\n",
        "    ## YOUR CODE\n",
        "\n",
        "    # Extract the 20 keyphrases with the highest weithgts from `phrase_dict`\n",
        "    phrase_list = sorted(keywords, key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "\n",
        "    return phrase_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xx2rGEhXPRB"
      },
      "outputs": [],
      "source": [
        "print(merge_predictions(extracted_phrases))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfqbdWTjXPRB"
      },
      "source": [
        "6. Again, evaluate the result and compare it to the other two approaches in terms of extraction quality and extraction speed."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dlEfW1tSXPRB"
      },
      "source": [
        "In terms of extraction quality, YAKE! and KeyBERT seem to perform better than the TF-IDF approach used in the previous system. This is because YAKE! and KeyBERT are more sophisticated algorithms that incorporate semantic information to determine the keyphrases. As seen in the output, YAKE! and KeyBERT extracted phrases that more accurately reflect the content of the text, such as 'Sir Charles Baskerville' and 'Sir Henry Baskerville', which were not extracted by the TF-IDF approach.\n",
        "\n",
        "In terms of extraction speed, YAKE! and KeyBERT are much faster than the TF-IDF approach used in the previous system. This is because YAKE! and KeyBERT are pre-trained models that can quickly extract keyphrases, while the TF-IDF approach required significant processing time to calculate the TF-IDF values for each candidate phrase.\n",
        "\n",
        "Overall, the use of off-the-shelf keyphrase extraction tools such as YAKE! and KeyBERT provides better results in terms of extraction quality and extraction speed compared to the TF-IDF approach used in the previous system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKeOggUYXPRB"
      },
      "source": [
        "## 2. Named Entity Recognition (4 + 5 + 5 = 14 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDGae9_YXPRC"
      },
      "source": [
        "Slightly different, but still operating on the sequence level, is the task of Named Entity Recognition (NER).\n",
        "In this task, we will evaluate the NER capabilities of some more open-source libraries.\n",
        "Particularly, we will also evaluate the utility of NER as a stand-in for Keyphrase Extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNA_f-2IXPRC"
      },
      "source": [
        "### Sub Task 1: Using spaCy NER (4 Points)\n",
        "\n",
        "So far, when using spaCy models, we have primarily disabled the NER component, as it requires significant extra compute.\n",
        "In this task, we will explicitly leave the component enabled, to see what results it can produce on the text from the previous question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X643upo2XPRC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model, but with NER enabled.\n",
        "nlp = spacy.load(\"en_core_web_sm\",enable='ner')\n",
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPrpnYATXPRC"
      },
      "source": [
        "1. Re-load the text for the \"Hounds of Baskervilles\" novel, and run it with the spacy model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OjclAW66XPRC"
      },
      "outputs": [],
      "source": [
        "# Re-use the function from the previous exercise.\n",
        "import requests\n",
        "url = \"https://sherlock-holm.es/stories/plain-text/houn.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "# Run the text through the model\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9eiel5xXPRC"
      },
      "source": [
        "2. Similar to the previous exercise, count the number of occurrences, however, this time for the extracted entities instead of phrases. Print the top 20 most frequently occurring entities.  \n",
        "Make sure to lowercase the text again during your aggregation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IOyDhlhIXPRC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "holmes --> 151\n",
            "henry --> 117\n",
            "watson --> 104\n",
            "one --> 96\n",
            "charles --> 74\n",
            "stapleton --> 73\n",
            "mortimer --> 72\n",
            "two --> 61\n",
            "first --> 51\n",
            "london --> 49\n",
            "barrymore --> 43\n",
            "baskerville --> 34\n",
            "baskerville hall --> 28\n",
            "sherlock holmes --> 24\n",
            "half --> 21\n",
            "henry baskerville --> 20\n",
            "night --> 17\n",
            "coombe tracey --> 16\n",
            "second --> 15\n",
            "three --> 15\n"
          ]
        }
      ],
      "source": [
        "# Count the number of occurrences of particular entities\n",
        "from collections import defaultdict\n",
        "# Create a defaultdict to store the entity counts\n",
        "entity_counts = defaultdict(int)\n",
        "\n",
        "# Iterate over the entities in the doc\n",
        "for ent in doc.ents:\n",
        "    # Lowercase the entity text\n",
        "    ent_text = ent.text.lower()\n",
        "    # Increment the count for the entity\n",
        "    entity_counts[ent_text] += 1\n",
        "\n",
        "# Print the top 20 most frequently occurring entities.\n",
        "for ent_text,count in sorted(entity_counts.items(),key=lambda x:x[1],reverse=True)[:20]:\n",
        "    print(f'{ent_text} --> {count}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBJ2RAEeXPRC"
      },
      "source": [
        "You might have noticed some unwanted results in the list, such as \"night\". Upon closer inspection, it turns out that the NER module further differentiates between different entity *categories*, such as PERSON (referencing, as expected, a physical person) or ORG (organizations, such as companies, NGOs, etc.), but also TIME (under which \"night\" falls). For reference, you can find the full list of supported NER labels by this particular model [here](https://spacy.io/models/en#en_core_web_sm-labels).\n",
        "\n",
        "3. Refine the list of most common entities by printing out the top three occurring entities in the category `PERSON`, `ORG` and `GPE` (physical locations) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BEOOxZcBXPRC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 3 entities in category PERSON: [('holmes', 141), ('henry', 117), ('watson', 104)]\n",
            "Top 3 entities in category ORG: [('baker street', 10), ('times', 10), ('i.', 10)]\n",
            "Top 3 entities in category GPE: [('london', 49), ('baskerville', 16), ('devonshire', 14)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter \n",
        "from typing import List,Tuple\n",
        "def get_top_entities_by_class(doc: spacy.tokens.Doc, class_name: str, n: int = 3) -> List[Tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Returns the three most frequent entities (and their frequencies)\n",
        "    of entity type `class_name` from `doc`.\n",
        "    \"\"\"\n",
        "    # Extract phrase and frequency of a particular entity class\n",
        "    counter = Counter([ent.text.lower() for ent in doc.ents if ent.label_ == class_name])\n",
        "    # Return the top 3 entities and frequencies\n",
        "    return counter.most_common(n)\n",
        "\n",
        "# Print the results for \"PERSON\", \"ORG\" and \"GPE\"\n",
        "print(\"Top 3 entities in category PERSON:\", get_top_entities_by_class(doc, \"PERSON\"))\n",
        "print(\"Top 3 entities in category ORG:\", get_top_entities_by_class(doc, \"ORG\"))\n",
        "print(\"Top 3 entities in category GPE:\", get_top_entities_by_class(doc, \"GPE\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfLK2eYAXPRD"
      },
      "source": [
        "### Sub Task 2: Financial Bank Statements of Deutsche Bank (5 Points)\n",
        "\n",
        "Instead of using the Sherlock Holmes Novels, we will now compare the functionality of spaCy and NLTK's NER modules on the financial statements of Deutsche Bank from 2021. For this, see the file available on Moodle.\n",
        "\n",
        "1. Download it and convert the PDF document into text, by using the `pdftotext` command-line utility. In particular, run with the `-layout` option enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmMOfSKtXPRD"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        ". ~/.bashrc\n",
        "## YOUR SHELL COMMAND HERE\n",
        "# If you have to execute this command through your shell, still paste the command you ran in here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "pdf_file = 'Assignment4/DB_annual_report.pdf'\n",
        "text_file = '/Assignment4/DB_annual_report.txt'\n",
        "subprocess.run(['pdftotext','-layout',pdf_file,text_file])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "202WskkAXPRD"
      },
      "source": [
        "2. Given that the document is extremely long, split the inputs into chunks of 500.000 characters and process them separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NjSk7_TWXPRD"
      },
      "outputs": [],
      "source": [
        "def load_long_text_in_chunks(fp: str, chunk_size: int = 500_000):\n",
        "    \"\"\"\n",
        "    Loads a text file (located at `fp`) and chunks it into chunks fo at most `chunk_size` characters.\n",
        "    Note that the last chunk might be significantly shorter.\n",
        "    \"\"\"\n",
        "    # Load the text file\n",
        "    with open(fp,'r') as f1:\n",
        "        text = f1.read()\n",
        "\n",
        "    # Split the text into segments of at most `chunk_size` characters\n",
        "    chunks = [text[i:i+chunk_size]for i in range(0,len(text),chunk_size)]\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JNgRpI-WXPRD"
      },
      "outputs": [],
      "source": [
        "db_chunks = load_long_text_in_chunks(text_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD3YOWb3XPRD"
      },
      "source": [
        "3. Print the top 5 occurring `ORG` entities that are not referencing Deutsche Bank itself, both by using spaCy's NER module and the NER function of NLTK.  \n",
        "To exclude \"Deutsche Bank\" entities, filter out all entities that contain both \"deutsche\" and \"bank\" in their name, irrespective of the actual upper-/lowercasing.\n",
        "**Hint:** For more information on how to run NER with NLTK, see [here](https://nanonets.com/blog/named-entity-recognition-with-nltk-and-spacy/#performing-ner-with-nltk-and-spacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jOTEGmd2XPRD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/anureddy/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/anureddy/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /Users/anureddy/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /Users/anureddy/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "org_entities_spacy = []\n",
        "org_entities_nltk = []\n",
        "\n",
        "def is_deutsche_bank_entity(name: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if the entity name contains \"deutsche\" and \"bank\" in some upper-/lowercased version.\n",
        "    This means both \"Deutsche Bank\" and \"deutsche bank's\" should be recognized.\n",
        "    \"\"\"\n",
        "    ## YOUR CODE\n",
        "    names = name.lower()\n",
        "    if \"deutsche\" in names and \"bank\" in names:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "for chunk in db_chunks:\n",
        "    # Process the chunk with spaCy\n",
        "    doc = nlp(chunk)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # And also with NLTK\n",
        "    words = nltk.word_tokenize(chunk)\n",
        "    tags = nltk.pos_tag(words)\n",
        "    chunks = nltk.ne_chunk(tags)\n",
        "    \n",
        "\n",
        "    # Add all the extracted \"ORG\" entities to `org_entities`, except those referencing Deutsche Bank\n",
        "    org_entities_spacy.extend([ent[0] for ent in entities if ent[1] == \"ORG\" and not is_deutsche_bank_entity(ent[0])])\n",
        "    org_entities_nltk.extend([c[0][0] for c in chunks if hasattr(c, 'label') and c.label() == 'ORGANIZATION' and not is_deutsche_bank_entity(c[0][0])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4TjN8xdiXPRD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 ORG entities wrt spaCy:  [('Group', 891), ('the Management Board', 262), ('the Supervisory Board', 189), ('Bank', 136), ('Management Board', 132)]\n",
            "Top 5 ORG entities wrt NLTK :  [('Deutsche', 1102), ('Group', 791), ('Management', 511), ('Supervisory', 392), ('Credit', 222)]\n"
          ]
        }
      ],
      "source": [
        "# Return the top 5 entities by frequency\n",
        "from collections import Counter\n",
        "entity_counts_spacy = Counter(org_entities_spacy)\n",
        "entity_counts_nltk = Counter(org_entities_nltk)\n",
        "print(\"Top 5 ORG entities wrt spaCy: \", entity_counts_spacy.most_common(5))\n",
        "print(\"Top 5 ORG entities wrt NLTK : \", entity_counts_nltk.most_common(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNTK_3rvXPRE"
      },
      "source": [
        "4. Compare and analyze the different results between the two methods."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eNcmA-xPXPRE"
      },
      "source": [
        "Running the same dataset with same functionalities, only spacy was able produce top 5 org entities,but nltk game none since spacy is also trained on larger corpus of data and therefore be able to recognize a wider range of entities.\n",
        "By comparing the results of spaCy and NLTK's NER, it can be seen that spaCy is better at recognizing entities with more context-specific features and is able to recognize a wider range of entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAWCOUNTS of PERSON \n",
        "entity_counts = {}\n",
        "for chunk in db_chunks:\n",
        "    doc = nlp(chunk)\n",
        "    \n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'PERSON':\n",
        "            if ent.text not in entity_counts:\n",
        "                entity_counts[ent.text]=1\n",
        "            else:\n",
        "                entity_counts[ent.text]+=1\n",
        "\n",
        "most_freq_ent = max(entity_counts,key = entity_counts.get)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most frequent 'PERSON' entity is: MREL\n"
          ]
        }
      ],
      "source": [
        "# Print the most frequent 'PERSON' entity and its count\n",
        "print(\"The most frequent 'PERSON' entity is:\", most_freq_ent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqJ3Er3tXPRE"
      },
      "source": [
        "### Sub Task 3: Co-Occurrence Counts of Entities (5 Points)\n",
        "\n",
        "As is becoming apparent, the *raw* occurrence counts of entities might not be meaningful on its own, especially if we are interested in less frequently occurring entities.\n",
        "\n",
        "Instead, we will \"investigate\" the entities that are most frequently mentioned in association with \"Deutsche Bank\". For this purpose, we will look at the textual co-occurrences of two named entities. The basic idea is that entities that frequently appear together are likely related.\n",
        "\n",
        "1. For each text chunk, extract all mentions of the entity `('Deutsche Bank', 'ORG')`, as well as all `PERSON` entity mentions in the text using spaCy. Store the respective entity name and the text position. Unlike the previous question, you do *not* need to check for different spelllings of the \"Deutsche Bank\" entity.  \n",
        "**Hint:** Entities are represented as a [`Span`](https://spacy.io/api/span) element in spaCy, which has access to text position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bgVqAJWRXPRE"
      },
      "outputs": [],
      "source": [
        "entity_mentions_with_start_position = []\n",
        "\n",
        "for chunk in db_chunks:\n",
        "    chunk_mentions = []\n",
        "    # Process the doc with spaCy\n",
        "    doc = nlp(chunk)\n",
        "    \n",
        "    # Extract only entity mentions of \"Deutsche Bank\" (ORG) or any PERSON mention.\n",
        "    # Append each mention, including the text and its starting position, to `chunk_mentions`\n",
        "    for e in doc.ents:\n",
        "        if e.label_ == 'ORG' and e.text == 'Deutsche Bank':\n",
        "            chunk_mentions.append((e.label_,e.text,e.start_char))\n",
        "        elif e.label_ == 'PERSON':\n",
        "            chunk_mentions.append((e.label_,e.text,e.start_char))\n",
        "    \n",
        "    # Append the chunk's entities to the aggregate list\n",
        "    entity_mentions_with_start_position.append(chunk_mentions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKH-yYN4XPRE"
      },
      "source": [
        "2. Within each chunk, for each mention of `Deutsche Bank`, search for `PERSON` entities that have a starting position within 200 characters before/after the starting position of the `Deutsche Bank` mention. Count for each `PERSON` entity how many times it occurs nearby a mention of `Deutsche Bank`.  \n",
        "Aggregate the co-occurrences across all chunks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1McG50lJXPRE"
      },
      "outputs": [],
      "source": [
        "co_occurrences = []\n",
        "\n",
        "for chunk_mentions in entity_mentions_with_start_position:\n",
        "    # Iterate through the entities. If the entity is a \"Deutsche Bank\" mention, extract nearby\n",
        "    # PERSON references (less than +/- 200 character difference in the starting position)\n",
        "\n",
        "    ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-5yLuxXPRE"
      },
      "source": [
        "\n",
        "3. Return the number of co-occurrences and the name of the top 5 frequently occurring `PERSON` entities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VYaGMVquXPRE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'Frank Kuhnke': 2, 'Rebecca Short': 2, 'Frank Werneke': 1, 'Frank Bsirske': 1, 'Olivier Vigneron': 3, 'Paul Achleitner': 1, 'Dagmar Valcárcel': 1, 'Paul Achl': 1, 'PB GY': 3, 'Main': 10, 'Zurich Italy': 1, '2021                                               Key': 1, '2021                              Risk': 1, 'Risk Type': 1, 'Risk Types': 1, 'Significant Increase': 1, '‚': 1, 'Leverage Ratio': 6, 'Eurosystem': 1, 'MREL': 15, 'Consent Order': 2, 'Jeffrey Epstein': 8, 'DB': 1, 'KGaA': 4, 'Datenträgerverfahren': 2, 'Nichtzulassungsbeschwerde': 1, 'Warburg Invest\\nKapitalanlagegesellschaft': 1, 'Warburg Invest': 5, 'Schwab': 3, 'Epstein': 2, 'George Town            Other Enterprise                   ': 2, 'Spólka Akcyjna                                 ': 1, 'Governance': 1, 'Fabrizio Campelli': 1, 'Alexander von zur Mühlen': 2, 'Stefan Simon': 1, 'Durin': 1, 'Sewing': 5, 'Generalbevollmächtigter': 1, 'Paul': 1, 'Achleitner': 2, 'Wirtschaftsprüfer': 1, 'Wirtschaftsprüfungsgesellschaften': 1, 'Significant Institutions': 2, 'Fixed Pay': 1, 'Risk Appetite\\nFramework': 1, 'Global Head': 1, 'Bankakademie Bielefeld': 2, 'Karl von Rohr\\nYear': 1, 'Karl von Rohr': 1, 'von Rohr': 1, 'Kiel': 1, 'Cloud': 1, 'Compliance': 2, 'Lewis': 2, 'von Moltke': 1, 'J.P. Morgan': 1, 'von zur Mühlen': 1, 'Riley': 1, 'European Head': 1, 'Simon': 1, 'Corporate Governance': 2, 'Polska Spółka Akcyjna': 1, 'São Paulo\\n                                                                     Deutsche': 1})\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "co_occurrences = collections.defaultdict(int)\n",
        "for chunk_mentions in entity_mentions_with_start_position:\n",
        "    for mention in chunk_mentions:\n",
        "        if mention[1]=='Deutsche Bank':\n",
        "            db_start = mention[2]\n",
        "            for other_mention in chunk_mentions:\n",
        "                if other_mention[0]=='PERSON' and abs(other_mention[2]-db_start)<=200:\n",
        "                    co_occurrences[other_mention[1]] += 1\n",
        "                    \n",
        "print(co_occurrences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "aggregated_co_occurrences = dict(co_occurrences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of co-occurrences:  129\n",
            "Name:  MREL\n",
            "Name:  Main\n",
            "Name:  Jeffrey Epstein\n",
            "Name:  Leverage Ratio\n",
            "Name:  Warburg Invest\n"
          ]
        }
      ],
      "source": [
        "top_5_persons = sorted(aggregated_co_occurrences.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "# Print the number of co-occurrences and the name of the top 5 frequently occurring persons\n",
        "print(\"Number of co-occurrences: \", sum(aggregated_co_occurrences.values()))\n",
        "for person in top_5_persons:\n",
        "    print(\"Name: \", person[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E8o7LlwXPRE"
      },
      "source": [
        "4. Look back at the results of your previous task. Are the `PERSON` entities returned by your co-occurrence method the same ones that appear most frequently by raw counts?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a7tn2CZcXPRE"
      },
      "source": [
        "Yes, the most frequent 'PERSON' entities is by name 'MREL' by both the methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OPFK74LXPRF"
      },
      "source": [
        "## 3. Neural Models with Huggingface (3 + 5 + 2 = 10 Points)\n",
        "\n",
        "For state-of-the-art performance, most text-related tasks nowadays use some variation of the Transformer architecture. The particular advantage is especiall the readily available weights for models that have been pre-trained on large general-purpose datasets, which reduces the amount of domain-specific labeled training data.\n",
        "\n",
        "In this task, we will explore the [Huggingface](https://hf.co/) ecosystem to see in which way Transformer models can be used.\n",
        "One of the central aspects of the Huggingface platform is the so-called [Model Hub](https://huggingface.co/models), where you can find many different models uploaded by community members for a variety of tasks.\n",
        "\n",
        "Because the neural models are generally very expensive to run, this exercise will be limited to  less data than in previous questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKs0GoliXPRF"
      },
      "source": [
        "### Sub Task 1: Loading Transformer Models (3 Points)\n",
        "\n",
        "1. Install the `transformers` library and load the model `cardiffnlp/twitter-roberta-base-sentiment-latest` to classify a sequence.\n",
        "2. Report the result of the prediction on the test sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "ca1ee9cb71e0466a943b61b9f6f82c21",
            "f8104517fb9643d3b314976d4146bdb4",
            "48493756c08a4f138edd22b91fda01b7",
            "8c901866e2c447869bb6941243abda9a",
            "652f6051e6d642b1acf19387da70d9b9",
            "bb5d8c03136a48dcbb524bdcefef40af",
            "8b904835fbf24ecd99e68e9beaa23707",
            "57e29725ac0a49329b1ecb66b684074c",
            "5e20584ecee1468e85e63821d9d2ea3b",
            "915320c5369c45a09944d7e122a2b202",
            "d9b559bfa90e43818c8ca352bbf1e4ef",
            "d7d7109e5734454cbe65ed957c149d4e",
            "9b70b51a076746ae949c5369bad18d1b",
            "15fbabfe3ec4415c8359b59c853af3bf",
            "fed55871910d4adca82da738d68ede8d",
            "7c78a405681443a5a2f53f85bbdaf4d9",
            "73edc7bcf914442384f5855a0c424b8f",
            "b08bc01dbb084c92a56e72ce448c765c",
            "78ca2c21e1764bf4a6ec47121f85b267",
            "e69e94a5b0b9447487d7ee00056c9e9a",
            "1ada2cf3cd174a0e9af09d669e8ce7a7",
            "735e4f04616e45c3a7b45f3200288e3a",
            "ea8733418c2e42dbaa34c43c8c4fe94e",
            "905ef08405d4493bbb98666af2d374c3",
            "b3793ee4eb504705ab375592925746b4",
            "ded4a62c5a624c818f0d79b20df290af",
            "160b26d25f6744e7bca9502554928b4e",
            "fb24428e8f854612a652188cc5035759",
            "f6e4342c18bd40b7bb554a01b2381e37",
            "836eddcc1dbb478188dcde56ae3494a2",
            "9c9bca5ea901420e803fa81c85ac72c9",
            "ddcef7d61ae94e43a2ee3e26c15c592b",
            "afd4e1da3fb840f494f51325871711fb",
            "989ee5cb08a74d9685c222c4b6bed2dd",
            "c8552fb19cc741a787643f1642c9ca23",
            "175722977f9146e984236662274a6905",
            "e84ca858b78444b8b2df07275d0ad964",
            "484beca70f7c4735b157e3c5281f182e",
            "cbeb2b3688aa41da9556f8d2fffb13bc",
            "16de7e0bbb4e459d99d93f1c37fd98f9",
            "bd8aa80532e64699a8d49c1650dd2f9a",
            "4deb739064f84f9f9ac588eb9e0f06b6",
            "37ff9725bfbc4c1bb2e5339cb3eb3771",
            "3847024f53a74f38abe3a1eee54b3b0c"
          ]
        },
        "id": "eb27KKeoXPRF",
        "outputId": "858cf91b-55da-44c7-95a1-eb2deea0485b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca1ee9cb71e0466a943b61b9f6f82c21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7d7109e5734454cbe65ed957c149d4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea8733418c2e42dbaa34c43c8c4fe94e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "989ee5cb08a74d9685c222c4b6bed2dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification, XLMRobertaFor\n",
        "from transformers import AutoTokenizer, AutoConfig ## YOUR IMPORTS\n",
        "\n",
        "model = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\" ## YOUR CODE\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)## YOUR CODE\n",
        "\n",
        "input_text = \"Das ist ein Test.\"\n",
        "\n",
        "prediction =  ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD6r4BtWXPRF"
      },
      "source": [
        "### Sub Task 2: Using Pipelines (5 Points)\n",
        "\n",
        "The most succinct way of using a Transformer model is the [`transformers.pipeline`](https://huggingface.co/docs/transformers/pipeline_tutorial). You can check out the linked tutorial for more information on the topic, but essentially, `pipeline` provides a light-weight wrapper around a number of different popular NLP tasks\n",
        "\n",
        "1. Instead of manually defining a pipeline, now load a model through a `\"text-classification\"` pipeline. Look up the neural model that is loaded by default, and post the link to its [model card](https://huggingface.co/docs/hub/model-cards) below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "185wlN3wXPRF"
      },
      "outputs": [],
      "source": [
        "## YOUR \n",
        "from transformers import pipeline\n",
        "sentiment_task = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "sentiment_task(\"Covid cases are increasing fast!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Gyv_LNXPRF"
      },
      "source": [
        "2. Now, instead, load a pipeline for `\"text-classification\"`, but with a custom model and tokenizer. Use the Model Hub platform to find the most popular model for the German language (by number of downloads) and manually specify the usage of another model (and tokenizer) to the pipeline. Re-run the previous example, and report the prediction result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnXPYk__XPRF",
        "outputId": "91c25ebb-5643-4ef7-b1a6-aee55a139d5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'LABEL_1', 'score': 0.621100127696991}]\n"
          ]
        }
      ],
      "source": [
        "model = 'xlm-roberta-large' ## YOUR CODE\n",
        "tokenizer = AutoTokenizer.from_pretrained(model) ## YOUR CODE\n",
        "\n",
        "# Instantiate the pipeline with custom components\n",
        "pipe = pipeline('text-classification', model=model)## YOUR CODE\n",
        "\n",
        "# Output the prediction by your pipe on the test sample.\n",
        "print(pipe(\"das ist ein test\")) ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0kv0z4_XPRF"
      },
      "source": [
        "3. Keeping in line with the previous exercises, let us now try and actually predict something with the model. Re-load a pipeline, this time for Named Entity Recognition, using the default model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gUJGbWxXPRF"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVEzh84PXPRF"
      },
      "source": [
        "4. Run the pipeline with the text from the Deutsche Bank report from Question 2 and output the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gu5SqSTXPRG"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE\n",
        "\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUygSaMSXPRG"
      },
      "source": [
        "5. Look at the results. Something looks strange here; why is it not working properly? Elaborate your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYlLmqwtXPRG"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63eIg1QGXPRG"
      },
      "source": [
        "### Sub Task 3: Using Datasets through Huggingface (2 Points)\n",
        "\n",
        "Instead of using the `transformers` library for model training and inference, it is also possible to use other libraries by Huggingface without neural models.\n",
        "In particular, the `datasets` library provides a centralized and streamlined way of accessing a variety of different datasets.\n",
        "\n",
        "1. Using the `datasets` library, load the `imdb` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0Q8A1etXPRG"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDjxp8YGXPRG"
      },
      "source": [
        "2. Report the mean length of `text` column for the training, validation and test split, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVunrlyWXPRG"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "LSTM_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7rc1"
    },
    "vscode": {
      "interpreter": {
        "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15fbabfe3ec4415c8359b59c853af3bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78ca2c21e1764bf4a6ec47121f85b267",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e69e94a5b0b9447487d7ee00056c9e9a",
            "value": 898822
          }
        },
        "160b26d25f6744e7bca9502554928b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16de7e0bbb4e459d99d93f1c37fd98f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "175722977f9146e984236662274a6905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8aa80532e64699a8d49c1650dd2f9a",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4deb739064f84f9f9ac588eb9e0f06b6",
            "value": 239
          }
        },
        "1ada2cf3cd174a0e9af09d669e8ce7a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37ff9725bfbc4c1bb2e5339cb3eb3771": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3847024f53a74f38abe3a1eee54b3b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48493756c08a4f138edd22b91fda01b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57e29725ac0a49329b1ecb66b684074c",
            "max": 929,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e20584ecee1468e85e63821d9d2ea3b",
            "value": 929
          }
        },
        "484beca70f7c4735b157e3c5281f182e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4deb739064f84f9f9ac588eb9e0f06b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57e29725ac0a49329b1ecb66b684074c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e20584ecee1468e85e63821d9d2ea3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "652f6051e6d642b1acf19387da70d9b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "735e4f04616e45c3a7b45f3200288e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73edc7bcf914442384f5855a0c424b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78ca2c21e1764bf4a6ec47121f85b267": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c78a405681443a5a2f53f85bbdaf4d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "836eddcc1dbb478188dcde56ae3494a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b904835fbf24ecd99e68e9beaa23707": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c901866e2c447869bb6941243abda9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_915320c5369c45a09944d7e122a2b202",
            "placeholder": "​",
            "style": "IPY_MODEL_d9b559bfa90e43818c8ca352bbf1e4ef",
            "value": " 929/929 [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "905ef08405d4493bbb98666af2d374c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb24428e8f854612a652188cc5035759",
            "placeholder": "​",
            "style": "IPY_MODEL_f6e4342c18bd40b7bb554a01b2381e37",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "915320c5369c45a09944d7e122a2b202": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "989ee5cb08a74d9685c222c4b6bed2dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8552fb19cc741a787643f1642c9ca23",
              "IPY_MODEL_175722977f9146e984236662274a6905",
              "IPY_MODEL_e84ca858b78444b8b2df07275d0ad964"
            ],
            "layout": "IPY_MODEL_484beca70f7c4735b157e3c5281f182e"
          }
        },
        "9b70b51a076746ae949c5369bad18d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73edc7bcf914442384f5855a0c424b8f",
            "placeholder": "​",
            "style": "IPY_MODEL_b08bc01dbb084c92a56e72ce448c765c",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "9c9bca5ea901420e803fa81c85ac72c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afd4e1da3fb840f494f51325871711fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b08bc01dbb084c92a56e72ce448c765c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3793ee4eb504705ab375592925746b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_836eddcc1dbb478188dcde56ae3494a2",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c9bca5ea901420e803fa81c85ac72c9",
            "value": 456318
          }
        },
        "bb5d8c03136a48dcbb524bdcefef40af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd8aa80532e64699a8d49c1650dd2f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8552fb19cc741a787643f1642c9ca23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbeb2b3688aa41da9556f8d2fffb13bc",
            "placeholder": "​",
            "style": "IPY_MODEL_16de7e0bbb4e459d99d93f1c37fd98f9",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "ca1ee9cb71e0466a943b61b9f6f82c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8104517fb9643d3b314976d4146bdb4",
              "IPY_MODEL_48493756c08a4f138edd22b91fda01b7",
              "IPY_MODEL_8c901866e2c447869bb6941243abda9a"
            ],
            "layout": "IPY_MODEL_652f6051e6d642b1acf19387da70d9b9"
          }
        },
        "cbeb2b3688aa41da9556f8d2fffb13bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7d7109e5734454cbe65ed957c149d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b70b51a076746ae949c5369bad18d1b",
              "IPY_MODEL_15fbabfe3ec4415c8359b59c853af3bf",
              "IPY_MODEL_fed55871910d4adca82da738d68ede8d"
            ],
            "layout": "IPY_MODEL_7c78a405681443a5a2f53f85bbdaf4d9"
          }
        },
        "d9b559bfa90e43818c8ca352bbf1e4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddcef7d61ae94e43a2ee3e26c15c592b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded4a62c5a624c818f0d79b20df290af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddcef7d61ae94e43a2ee3e26c15c592b",
            "placeholder": "​",
            "style": "IPY_MODEL_afd4e1da3fb840f494f51325871711fb",
            "value": " 456k/456k [00:00&lt;00:00, 3.95MB/s]"
          }
        },
        "e69e94a5b0b9447487d7ee00056c9e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e84ca858b78444b8b2df07275d0ad964": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37ff9725bfbc4c1bb2e5339cb3eb3771",
            "placeholder": "​",
            "style": "IPY_MODEL_3847024f53a74f38abe3a1eee54b3b0c",
            "value": " 239/239 [00:00&lt;00:00, 7.94kB/s]"
          }
        },
        "ea8733418c2e42dbaa34c43c8c4fe94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_905ef08405d4493bbb98666af2d374c3",
              "IPY_MODEL_b3793ee4eb504705ab375592925746b4",
              "IPY_MODEL_ded4a62c5a624c818f0d79b20df290af"
            ],
            "layout": "IPY_MODEL_160b26d25f6744e7bca9502554928b4e"
          }
        },
        "f6e4342c18bd40b7bb554a01b2381e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8104517fb9643d3b314976d4146bdb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb5d8c03136a48dcbb524bdcefef40af",
            "placeholder": "​",
            "style": "IPY_MODEL_8b904835fbf24ecd99e68e9beaa23707",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "fb24428e8f854612a652188cc5035759": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fed55871910d4adca82da738d68ede8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ada2cf3cd174a0e9af09d669e8ce7a7",
            "placeholder": "​",
            "style": "IPY_MODEL_735e4f04616e45c3a7b45f3200288e3a",
            "value": " 899k/899k [00:00&lt;00:00, 7.52MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
