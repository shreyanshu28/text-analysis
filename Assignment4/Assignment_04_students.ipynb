{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyanshu28/text-analysis/blob/main/Assignment4/Assignment_04_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUgbolxu5quo"
      },
      "source": [
        "# Assignment 4: Keyphrase Extraction, Named Entity Recognition & Neural Models\n",
        "\n",
        "Due: Monday, February 06, 2023, at 2pm via Moodle\n",
        "\n",
        "**Team Members** `1.Yanxin Jia, 3769165\n",
        "2.Shreyansu Vyas, 3769429\n",
        "3.Smaran Nair , 3771609\n",
        "4.AnuReddy , 3768482`\n",
        "\n",
        "Please note that this assignment comes with quite a number of artifacts, totaling somewhere around 5 GB of necessary disk space. In case you are running into issues or do want to keep your environment \"clean\", we suggest the use of [Google Colab](https://colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-su5u2SbXPQ2",
        "outputId": "369bca8a-c75c-49df-8076-a22fa0386620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.7.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting sentence-transformers>=0.3.8\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.0/86.0 KB 5.7 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.8/dist-packages (from keybert) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from keybert) (1.21.6)\n",
            "Collecting rich>=10.4.0\n",
            "  Downloading rich-13.2.0-py3-none-any.whl (238 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 238.9/238.9 KB 12.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keybert) (4.4.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n",
            "Collecting markdown-it-py<3.0.0,>=2.1.0\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 KB 5.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 78.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.14.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 59.1 MB/s eta 0:00:00\n",
            "Collecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.4/182.4 KB 13.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 57.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.12.7)\n",
            "Building wheels for collected packages: keybert, sentence-transformers\n",
            "  Building wheel for keybert (setup.py): started\n",
            "  Building wheel for keybert (setup.py): finished with status 'done'\n",
            "  Created wheel for keybert: filename=keybert-0.7.0-py3-none-any.whl size=23800 sha256=a0e155b4b5cb955fc4a301136d530aa89ac08f8f44acd9d557c92e0e61d87fec\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/bc/8b/a51bee77aec33895e6c8c236144b4cc10875659c4d2c80f070\n",
            "  Building wheel for sentence-transformers (setup.py): started\n",
            "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=42ef02e837a8c0cee20ebf926e7f1795671cfabbb6b43f80e411a88ac25f0505\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built keybert sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, mdurl, markdown-it-py, huggingface-hub, transformers, rich, sentence-transformers, keybert\n",
            "Successfully installed huggingface-hub-0.11.1 keybert-0.7.0 markdown-it-py-2.1.0 mdurl-0.1.2 rich-13.2.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-rp6qv5gv\n",
            "  Resolved https://github.com/LIAAD/yake to commit 8d71d94ded93fb77f1361f62e5264f19b9c91cd7\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (0.8.10)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (1.21.6)\n",
            "Collecting segtok\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (3.0)\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.6/132.6 KB 12.4 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from segtok->yake==0.4.8) (2022.6.2)\n",
            "Building wheels for collected packages: yake, jellyfish\n",
            "  Building wheel for yake (setup.py): started\n",
            "  Building wheel for yake (setup.py): finished with status 'done'\n",
            "  Created wheel for yake: filename=yake-0.4.8-py2.py3-none-any.whl size=62600 sha256=c8d64812a0d8f781866ee27184612bed9725dc92f690048410bee0898e7367f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vvbocyyx/wheels/91/6a/18/ab7d2fc1e58b6ed936dbe90881425aea909bae3f98dae32f66\n",
            "  Building wheel for jellyfish (setup.py): started\n",
            "  Building wheel for jellyfish (setup.py): finished with status 'done'\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp38-cp38-linux_x86_64.whl size=77905 sha256=cfcea08aea84fb83990d40c7cb682e2beace5b5308e7c42850c6d7ccc14d5d11\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/c7/3c/4c83132de76359e3a429fd09c08995945ca96c5290a41651d3\n",
            "Successfully built yake jellyfish\n",
            "Installing collected packages: segtok, jellyfish, yake\n",
            "Successfully installed jellyfish-0.9.0 segtok-1.5.11 yake-0.4.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 452.9/452.9 KB 19.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.0/132.0 KB 16.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.0/213.0 KB 23.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 KB 16.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.11)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/LIAAD/yake /tmp/pip-req-build-rp6qv5gv\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        ". ~/.bashrc\n",
        "python3 -m pip install keybert\n",
        "python3 -m pip install git+https://github.com/LIAAD/yake\n",
        "python3 -m pip install transformers\n",
        "python3 -m pip install datasets\n",
        "python3 -m pip install nltk\n",
        "python3 -m pip install spacy\n",
        "# Install necessary packages for all questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "315kqIiW2Zhr"
      },
      "source": [
        "## Task 1: Keyphrase Extraction (5 + 3 + 3 + 5) = 16 Points\n",
        "\n",
        "In this task, we will implement our own unsupervised keyphrase extraction (KPE) module utilizing a simple grammatical ruling system, which we apply to a Sherlock Holmes novel.\n",
        "To generate TF-IDF-weighted phrases, we will be using the entire collection of Sir Arthur Donan Coyle novels to calculate document frequencies.\n",
        "\n",
        "Finally, we compare the results to general-purpose KPE libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joME4y0pC-bq"
      },
      "source": [
        "### Sub Task 1: Unsupervised Keyphrase Extraction System (5 Points)\n",
        "\n",
        "#### 1. Candidate Generation\n",
        "We will need to generate a set of suitable candidate phrases first, which can then be ranked as keyphrases later on. To do this, we will again be using spaCy's, this time its rule-based [`Matcher` class](https://spacy.io/api/matcher).\n",
        "\n",
        "The syntactic pattern of a keyphrase candidate should satisfy the following rules:\n",
        "\n",
        "1. An optional adjective, noun, proper noun\n",
        "2. An optional adjective, noun, proper noun\n",
        "3. A mandatory noun or proper noun.\n",
        "\n",
        "Add a second pattern, which recognizes the pattern\n",
        "\n",
        "1. A noun or proper noun\n",
        "2. An adposition\n",
        "3. Another noun or proper noun\n",
        "\n",
        "Note that the first condition will match any phrase of length between 1-3 tokens, which is a suitable approximation for our task at hand, whereas the second pattern is slightly more specific, always matching exactly three tokens.\n",
        "An example of a valid matched phrases for the first pattern would be \"Sherlock Holmes\" ([PROPN, PROPN]), and \"Hounds of Baskervilles\" ([NOUN, ADP, PROPN]) for the second pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7yKhziiDkzJ"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cFJOPt3DlZu"
      },
      "outputs": [],
      "source": [
        "# load language model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define the above patterns\n",
        "pattern1 = [{'POS':'ADJ','OP':'?'},{'POS':'NOUN','OP':'*'},{'POS':'PROPN','OP':'*'},{'POS':'ADJ','OP':'?'},{'POS':'NOUN','OP':'*'},{'POS':'PROPN','OP':'*'},{'POS':'NOUN','OP':'+'},{'POS':'PROPN','OP':'+'}]\n",
        "pattern2 = [{'POS':'NOUN','OP':'*'},{'POS':'NOUN','OP':'*'},{'POS':'ADP','OP':'+'},{'POS':'NOUN','OP':'*'},{'POS':'NOUN','OP':'*'}]\n",
        "\n",
        "matcher.add('NOVEL',[pattern1,pattern2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuWXyX6UXPQ6"
      },
      "source": [
        "To verify whether your pattern is correct, use the below example.\n",
        "If you have done everything correctly, your matcher will identify **13 phrases**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71JZGbIlHfxx"
      },
      "outputs": [],
      "source": [
        "doc = nlp(\"This is a simple test. It should return 'simple', and 'test', among other phrases. Maybe we can also see if it can recognize the art of war. Would it recognize integer linear programming, too?\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "print(len(matches))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x2TRsI-GeLq"
      },
      "source": [
        "#### 2. Applying Your System\n",
        "\n",
        "Once you have matched the correct number of keyphrase candidates on the above example, apply your rule-based matcher to an actual data sample. We are going to use the Sherlock Holmes novel \"Hounds of Baskervilles\". You can find the raw text file at the following URL:\n",
        "\n",
        "https://sherlock-holm.es/stories/plain-text/houn.txt\n",
        "\n",
        "Download the text from this URL and apply your spaCy model and matcher on it.  \n",
        "**Hint:** Make sure you properly decode your input, since some libraries return binary strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbDFj7DSLQ5v"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "def load_txt_from_url(url: str = \"https://sherlock-holm.es/stories/plain-text/houn.txt\") -> str:\n",
        "  ## YOUR CODE\n",
        "\n",
        "text = load_txt_from_url()\n",
        "\n",
        "# Apply the spacy model to the loaded text and extract the phrases with the Matcher\n",
        "doc = ## YOUR CODE\n",
        "matches = ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4U96ELPKtIA"
      },
      "source": [
        "We will now investigate which phrase candidates are the most frequently appearing in this novel, simply based on the phrase frequency. Therefore, convert your abstract match objects into actual strings, lowercase them, and return the 20 most frequently occurring phrase candidates and their respective frequencies.  \n",
        "**Hint:** For counting occurrences, you may look at `collections.Counter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tGof05SIXES"
      },
      "outputs": [],
      "source": [
        "candidates = []\n",
        "# Lowercase and add the extracted candidate matches to `candidates`\n",
        "## YOUR CODE\n",
        "\n",
        "# Count the number of occurrences of different candidate phrases\n",
        "candidate_phrases = ## YOUR CODE\n",
        "# Print the most frequently occurring phrases, together with the respective frequencies\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQt8278NA7v"
      },
      "source": [
        "#### 3. Briefly summarize the quality of your top 20 candidates:\n",
        "\n",
        "YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aucrN3TPJqw1"
      },
      "source": [
        "### Sub Task 2: Generating Document Frequency Values (3 Points)\n",
        "\n",
        "To compare the previously generated terms with a more refined model, we are going to extract document frequencies from the collection of all Sherlock Holmes works. Since the books are relatively long documents, we are instead going to split based on a simple heuristic in the input document, which should allow a decent approximation by taking into account individual chapters of each novel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfp7rUN9N218"
      },
      "source": [
        "1. Start by loading the Sherlock Holmes canon from https://sherlock-holm.es/stories/plain-text/cnus.txt  \n",
        "Afterwards, split the full document into individual chapters. For this, use three consecutive line breaks `\\n\\n\\n` as a splitting condition to approximate the chapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJYRCDJyP-Xp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Load the document\n",
        "url = \"https://sherlock-holm.es/stories/plain-text/cnus.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Split the document into chapters\n",
        "chapters = text.split(\"\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRcMyiKkRU7d"
      },
      "source": [
        "After splitting, you should have 353 individual \"documents\" to work with.\n",
        "\n",
        "2. Now, create a dictionary containing each phrase encountered in the larger corpus, and its associated document frequency. Again, ensure that phrase strings are lowercased for consistency with the previous transformation.  \n",
        "**Hint:** Since the processing of 353 documents might take a while, incorporate [`tqdm.tqdm`](https://tqdm.github.io/) to visualize progress on the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtXpYkWQQJi2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "def return_occurring_phrases(doc_text: str) -> List[str]:\n",
        "  # process text with spaCy and apply the Matcher\n",
        "  doc = ## YOUR CODE\n",
        "  matches = ## YOUR CODE\n",
        "\n",
        "  # Candidates can be a set, since we only care about the occurrence *once* for IDF values.\n",
        "  # Again, extract the lower-cased text of a matched span.\n",
        "  candidates = set()\n",
        "  for match_id, start, end in matches:\n",
        "    ## YOUR CODE\n",
        "\n",
        "  return list(candidates)\n",
        "\n",
        "all_document_phrases = []\n",
        "# Iterate through the individual documents and extract phrases for them. Use `tqdm` to visualize progress\n",
        "## YOUR CODE\n",
        "\n",
        "# Once again, count the frequency of term occurrences across all documents\n",
        "## YOUR CODE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZjyowpXPQ9"
      },
      "source": [
        "3. Output the 20 most frequently appearing document phrases that your system detected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vDTmrTLSY2q"
      },
      "outputs": [],
      "source": [
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR9FmhTpS_TB"
      },
      "source": [
        "### Sub Task 3: Generating Weighted Keyphrases (3 Points)\n",
        "\n",
        "We can now incorporate the extracted keyphrases to calculate `tf-idf` scores, and return a hopefully improved version of our keyphrases for the original \"Hounds of Baskervilles\" document. \n",
        "\n",
        "1. Iterate over all phrases occurring in the novel \"Hounds of Baskervilles\", and re-score phrases according to the definition of TF-IDF. Use the smoothed definition of idf:\n",
        "\n",
        "$ idf(t, D) = \\log \\frac{|D|}{|\\{d \\in D : t \\in d\\}| + 1} + 1 $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDDoPiAATWat"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Dict\n",
        "\n",
        "def tf_idf(tf: int, df_count: int) -> float:\n",
        "  \"\"\"\n",
        "  Computes the TF-IDF scores according to the above-mentioned formula.\n",
        "  Note that you may use a constant for the number of documents (|D|).\n",
        "  \"\"\"\n",
        "  return ## YOUR CODE\n",
        "\n",
        "tf_idf_weighted_candidates = []\n",
        "\n",
        "# Iterate through all candidate phrase/frequency pairs and compute the TF-IDF scores for each phrase\n",
        "# Store the phrase together with its TF-IDF score in `tf_idf_weighted_candidates`\n",
        "for candidate, tf in candidate_phrases.items():\n",
        "  tf_idf_weighted_candidates.append( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSfTPsukVoEF"
      },
      "source": [
        "2. Now print the top 20 candidate phrases by TF-IDF weight, and compare the results to your previous output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8ulSamTUsGP"
      },
      "outputs": [],
      "source": [
        "print(## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdmGgOThVmyz"
      },
      "source": [
        "3. Write your insights on the comparison of the results below. Try to theorize why some of the phrases still appear, or why other phrases are no longer present:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7prjNuouXPRA"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y8eaRPNWHut"
      },
      "source": [
        "4. Give two examples of how you could further improve the list of keyphrase values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yfMZ0etWNj_"
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gceycuSdXPRA"
      },
      "source": [
        "### Sub Task 4: Apply off-the-shelf Keyphrase Extraction Tools (5 Points)\n",
        "\n",
        "To put the findings of your system into context, compare them with two popular open-source libraries, namely [YAKE!](https://github.com/LIAAD/yake) and [KeyBERT](https://github.com/MaartenGr/KeyBERT)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2MGiQjQXPRA"
      },
      "source": [
        "1. First, start by running the document with YAKE!; you may use the default parameters. Print the resulting keyphrases, which by default returns 20 phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mesNMFROXPRA"
      },
      "outputs": [],
      "source": [
        "from yake import KeywordExtractor\n",
        "\n",
        "extractor = ## YOUR CODE\n",
        "keywords = ## YOUR CODE\n",
        "\n",
        "# Print the top 20 keywords\n",
        "print(keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01UDXLakXPRA"
      },
      "source": [
        "2. Compare both runtime efficiency and the extracted phrases with your own system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIrMGHBkXPRA"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwTmlq10XPRB"
      },
      "source": [
        "3. Now use the KeyBERT library to extract keyphrases. Importantly, you will need to split the document into separate paragraphs, as the underlying neural model will be unable to handle the complete document as input.  \n",
        "Use the pattern of `\\n\\n` to separate the text into smaller paragraphs, and filter out any empty lines after. An \"empty line\" also constitutes all inputs that only contain newline (`\\n`) or whitespace ` ` characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icf29xfcXPRB"
      },
      "outputs": [],
      "source": [
        "# Split the input text according to the specified criteria and filter empty lines out.\n",
        "split_text = ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9STj-XbnXPRB"
      },
      "source": [
        "4. To ensure consistency between the tools when extracting keyphrases, set the *n*-gram range to `(1,3)`.\n",
        "Otherwise, leave all parameters at the default value, and extract the keyphrases from each paragraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeGRSplLXPRB"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "# This might take a while to install\n",
        "model = KeyBERT(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Extract the keyphrases from each split, using the adjusted keyphrase ngram range\n",
        "# Hint: You may pass a list to the extraction function and KeyBERT will automatically handle iteration.\n",
        "extracted_phrases = ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz385Lt6XPRB"
      },
      "source": [
        "5. Combine the predictions of all individual splits into a single list. For this, sum up the prediction scores across all splits.  \n",
        "**Hint:** `collections.defaultdict` makes aggregations like this much easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ddO4hc4XPRB"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "def merge_predictions(list_of_predictions: List[List[Tuple]]) -> List[Tuple]:\n",
        "    \"\"\"\n",
        "    Combines lists of predictions into a single list with added scores.\n",
        "    \"\"\"\n",
        "    phrase_dict = ## YOUR CODE\n",
        "\n",
        "    # Iterate through all the lists of predictions and add the scores to the correct dict entry\n",
        "    ## YOUR CODE\n",
        "\n",
        "    # Extract the 20 keyphrases with the highest weithgts from `phrase_dict`\n",
        "    phrase_list = ## YOUR CODE\n",
        "\n",
        "    return phrase_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xx2rGEhXPRB"
      },
      "outputs": [],
      "source": [
        "print(merge_predictions(extracted_phrases))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfqbdWTjXPRB"
      },
      "source": [
        "6. Again, evaluate the result and compare it to the other two approaches in terms of extraction quality and extraction speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlEfW1tSXPRB"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKeOggUYXPRB"
      },
      "source": [
        "## 2. Named Entity Recognition (4 + 5 + 5 = 14 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDGae9_YXPRC"
      },
      "source": [
        "Slightly different, but still operating on the sequence level, is the task of Named Entity Recognition (NER).\n",
        "In this task, we will evaluate the NER capabilities of some more open-source libraries.\n",
        "Particularly, we will also evaluate the utility of NER as a stand-in for Keyphrase Extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNA_f-2IXPRC"
      },
      "source": [
        "### Sub Task 1: Using spaCy NER (4 Points)\n",
        "\n",
        "So far, when using spaCy models, we have primarily disabled the NER component, as it requires significant extra compute.\n",
        "In this task, we will explicitly leave the component enabled, to see what results it can produce on the text from the previous question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "X643upo2XPRC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model, but with NER enabled.\n",
        "nlp = spacy.load(\"en_core_web_sm\",enable='ner')\n",
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPrpnYATXPRC"
      },
      "source": [
        "1. Re-load the text for the \"Hounds of Baskervilles\" novel, and run it with the spacy model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OjclAW66XPRC"
      },
      "outputs": [],
      "source": [
        "# Re-use the function from the previous exercise.\n",
        "import requests\n",
        "url = \"https://sherlock-holm.es/stories/plain-text/houn.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Run the text through the model\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9eiel5xXPRC"
      },
      "source": [
        "2. Similar to the previous exercise, count the number of occurrences, however, this time for the extracted entities instead of phrases. Print the top 20 most frequently occurring entities.  \n",
        "Make sure to lowercase the text again during your aggregation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IOyDhlhIXPRC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "holmes --> 151\n",
            "henry --> 117\n",
            "watson --> 104\n",
            "one --> 96\n",
            "charles --> 74\n",
            "stapleton --> 73\n",
            "mortimer --> 72\n",
            "two --> 61\n",
            "first --> 51\n",
            "london --> 49\n",
            "barrymore --> 43\n",
            "baskerville --> 34\n",
            "baskerville hall --> 28\n",
            "sherlock holmes --> 24\n",
            "half --> 21\n",
            "henry baskerville --> 20\n",
            "night --> 17\n",
            "coombe tracey --> 16\n",
            "second --> 15\n",
            "three --> 15\n"
          ]
        }
      ],
      "source": [
        "# Count the number of occurrences of particular entities\n",
        "from collections import defaultdict\n",
        "# Create a defaultdict to store the entity counts\n",
        "entity_counts = defaultdict(int)\n",
        "\n",
        "# Iterate over the entities in the doc\n",
        "for ent in doc.ents:\n",
        "    # Lowercase the entity text\n",
        "    ent_text = ent.text.lower()\n",
        "    # Increment the count for the entity\n",
        "    entity_counts[ent_text] += 1\n",
        "\n",
        "# Print the top 20 most frequently occurring entities.\n",
        "for ent_text,count in sorted(entity_counts.items(),key=lambda x:x[1],reverse=True)[:20]:\n",
        "    print(f'{ent_text} --> {count}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBJ2RAEeXPRC"
      },
      "source": [
        "You might have noticed some unwanted results in the list, such as \"night\". Upon closer inspection, it turns out that the NER module further differentiates between different entity *categories*, such as PERSON (referencing, as expected, a physical person) or ORG (organizations, such as companies, NGOs, etc.), but also TIME (under which \"night\" falls). For reference, you can find the full list of supported NER labels by this particular model [here](https://spacy.io/models/en#en_core_web_sm-labels).\n",
        "\n",
        "3. Refine the list of most common entities by printing out the top three occurring entities in the category `PERSON`, `ORG` and `GPE` (physical locations) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BEOOxZcBXPRC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 3 entities in category PERSON: [('holmes', 141), ('henry', 117), ('watson', 104)]\n",
            "Top 3 entities in category ORG: [('baker street', 10), ('times', 10), ('i.', 10)]\n",
            "Top 3 entities in category GPE: [('london', 49), ('baskerville', 16), ('devonshire', 14)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter \n",
        "from typing import List,Tuple\n",
        "def get_top_entities_by_class(doc: spacy.tokens.Doc, class_name: str, n: int = 3) -> List[Tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Returns the three most frequent entities (and their frequencies)\n",
        "    of entity type `class_name` from `doc`.\n",
        "    \"\"\"\n",
        "    # Extract phrase and frequency of a particular entity class\n",
        "    counter = Counter([ent.text.lower() for ent in doc.ents if ent.label_ == class_name])\n",
        "    # Return the top 3 entities and frequencies\n",
        "    return counter.most_common(n)\n",
        "\n",
        "# Print the results for \"PERSON\", \"ORG\" and \"GPE\"\n",
        "print(\"Top 3 entities in category PERSON:\", get_top_entities_by_class(doc, \"PERSON\"))\n",
        "print(\"Top 3 entities in category ORG:\", get_top_entities_by_class(doc, \"ORG\"))\n",
        "print(\"Top 3 entities in category GPE:\", get_top_entities_by_class(doc, \"GPE\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfLK2eYAXPRD"
      },
      "source": [
        "### Sub Task 2: Financial Bank Statements of Deutsche Bank (5 Points)\n",
        "\n",
        "Instead of using the Sherlock Holmes Novels, we will now compare the functionality of spaCy and NLTK's NER modules on the financial statements of Deutsche Bank from 2021. For this, see the file available on Moodle.\n",
        "\n",
        "1. Download it and convert the PDF document into text, by using the `pdftotext` command-line utility. In particular, run with the `-layout` option enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmMOfSKtXPRD"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        ". ~/.bashrc\n",
        "## YOUR SHELL COMMAND HERE\n",
        "# If you have to execute this command through your shell, still paste the command you ran in here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pdftotext', '-layout', '/Users/anureddy/Desktop/Sem01/DataScience_for_text_analytics/Assignments/Assignment04/DB_annual_report.pdf', '/Users/anureddy/Desktop/Sem01/DataScience_for_text_analytics/Assignments/Assignment04/DB_annual_report.txt'], returncode=0)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import subprocess\n",
        "pdf_file = '/Users/anureddy/Desktop/Sem01/DataScience_for_text_analytics/Assignments/Assignment04/DB_annual_report.pdf'\n",
        "text_file = '/Users/anureddy/Desktop/Sem01/DataScience_for_text_analytics/Assignments/Assignment04/DB_annual_report.txt'\n",
        "subprocess.run(['pdftotext','-layout',pdf_file,text_file])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "202WskkAXPRD"
      },
      "source": [
        "2. Given that the document is extremely long, split the inputs into chunks of 500.000 characters and process them separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NjSk7_TWXPRD"
      },
      "outputs": [],
      "source": [
        "def load_long_text_in_chunks(fp: str, chunk_size: int = 500_000):\n",
        "    \"\"\"\n",
        "    Loads a text file (located at `fp`) and chunks it into chunks fo at most `chunk_size` characters.\n",
        "    Note that the last chunk might be significantly shorter.\n",
        "    \"\"\"\n",
        "    # Load the text file\n",
        "    with open(fp,'r') as f1:\n",
        "        text = f1.read()\n",
        "\n",
        "    # Split the text into segments of at most `chunk_size` characters\n",
        "    chunks = [text[i:i+chunk_size]for i in range(0,len(text),chunk_size)]\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JNgRpI-WXPRD"
      },
      "outputs": [],
      "source": [
        "db_chunks = load_long_text_in_chunks(text_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD3YOWb3XPRD"
      },
      "source": [
        "3. Print the top 5 occurring `ORG` entities that are not referencing Deutsche Bank itself, both by using spaCy's NER module and the NER function of NLTK.  \n",
        "To exclude \"Deutsche Bank\" entities, filter out all entities that contain both \"deutsche\" and \"bank\" in their name, irrespective of the actual upper-/lowercasing.\n",
        "**Hint:** For more information on how to run NER with NLTK, see [here](https://nanonets.com/blog/named-entity-recognition-with-nltk-and-spacy/#performing-ner-with-nltk-and-spacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jOTEGmd2XPRD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/anureddy/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/anureddy/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /Users/anureddy/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /Users/anureddy/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "org_entities_spacy = []\n",
        "org_entities_nltk = []\n",
        "\n",
        "def is_deutsche_bank_entity(name: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if the entity name contains \"deutsche\" and \"bank\" in some upper-/lowercased version.\n",
        "    This means both \"Deutsche Bank\" and \"deutsche bank's\" should be recognized.\n",
        "    \"\"\"\n",
        "    ## YOUR CODE\n",
        "    names = name.lower()\n",
        "    if \"deutsche\" in names and \"bank\" in names:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "for chunk in db_chunks:\n",
        "    # Process the chunk with spaCy\n",
        "    doc = nlp(chunk)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # And also with NLTK\n",
        "    words = nltk.word_tokenize(chunk)\n",
        "    tags = nltk.pos_tag(words)\n",
        "    chunks = nltk.ne_chunk(tags)\n",
        "    \n",
        "\n",
        "    # Add all the extracted \"ORG\" entities to `org_entities`, except those referencing Deutsche Bank\n",
        "    org_entities_spacy.extend([ent[0] for ent in entities if ent[1] == \"ORG\" and not is_deutsche_bank_entity(ent[0])])\n",
        "    org_entities_nltk.extend([c[0][0] for c in chunks if hasattr(c, 'label') and c.label() == 'ORG' and not is_deutsche_bank_entity(c[0][0])])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4TjN8xdiXPRD"
      },
      "outputs": [],
      "source": [
        "for chunk in db_chunks:\n",
        "    # Process the chunk with spaCy\n",
        "    doc = nlp(chunk)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 ORG entities wrt spaCy:  [('Group', 891), ('the Management Board', 262), ('the Supervisory Board', 189), ('Bank', 136), ('Management Board', 132)]\n",
            "Top 5 ORG entities wrt NLTK :  []\n"
          ]
        }
      ],
      "source": [
        "# Return the top 5 entities by frequency\n",
        "\n",
        "entity_counts_spacy = Counter(org_entities_spacy)\n",
        "entity_counts_nltk = Counter(org_entities_nltk)\n",
        "print(\"Top 5 ORG entities wrt spaCy: \", entity_counts_spacy.most_common(5))\n",
        "print(\"Top 5 ORG entities wrt NLTK : \", entity_counts_nltk.most_common(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNTK_3rvXPRE"
      },
      "source": [
        "4. Compare and analyze the different results between the two methods."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eNcmA-xPXPRE"
      },
      "source": [
        "Running the same dataset with same functionalities, only spacy was able produce top 5 org entities,but nltk game none since spacy is also trained on larger corpus of data and therefore be able to recognize a wider range of entities.\n",
        "By comparing the results of spaCy and NLTK's NER, it can be seen that spaCy is better at recognizing entities with more context-specific features and is able to recognize a wider range of entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Most frequent from 'PERSON' entities by raw counts \n",
        "entity_counts = {}\n",
        "for chunk in db_chunks:\n",
        "    doc = nlp(chunk)\n",
        "    \n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'PERSON':\n",
        "            if ent.text not in entity_counts:\n",
        "                entity_counts[ent.text]=1\n",
        "            else:\n",
        "                entity_counts[ent.text]+=1\n",
        "\n",
        "most_freq_ent = max(entity_counts,key = entity_counts.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most frequent 'PERSON' entity is: MREL\n",
            "Count: 38\n"
          ]
        }
      ],
      "source": [
        "# Print the most frequent 'PERSON' entity and its count\n",
        "print(\"The most frequent 'PERSON' entity is:\", most_freq_ent)\n",
        "print(\"Count:\", entity_counts[most_freq_ent])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqJ3Er3tXPRE"
      },
      "source": [
        "### Sub Task 3: Co-Occurrence Counts of Entities (5 Points)\n",
        "\n",
        "As is becoming apparent, the *raw* occurrence counts of entities might not be meaningful on its own, especially if we are interested in less frequently occurring entities.\n",
        "\n",
        "Instead, we will \"investigate\" the entities that are most frequently mentioned in association with \"Deutsche Bank\". For this purpose, we will look at the textual co-occurrences of two named entities. The basic idea is that entities that frequently appear together are likely related.\n",
        "\n",
        "1. For each text chunk, extract all mentions of the entity `('Deutsche Bank', 'ORG')`, as well as all `PERSON` entity mentions in the text using spaCy. Store the respective entity name and the text position. Unlike the previous question, you do *not* need to check for different spelllings of the \"Deutsche Bank\" entity.  \n",
        "**Hint:** Entities are represented as a [`Span`](https://spacy.io/api/span) element in spaCy, which has access to text position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bgVqAJWRXPRE"
      },
      "outputs": [],
      "source": [
        "entity_mentions_with_start_position = []\n",
        "\n",
        "for chunk in db_chunks:\n",
        "    chunk_mentions = []\n",
        "    # Process the doc with spaCy\n",
        "    doc = nlp(chunk)\n",
        "    \n",
        "    # Extract only entity mentions of \"Deutsche Bank\" (ORG) or any PERSON mention.\n",
        "    # Append each mention, including the text and its starting position, to `chunk_mentions`\n",
        "    for e in doc.ents:\n",
        "        if e.label_ == 'ORG' and e.text == 'Deutsche Bank':\n",
        "            chunk_mentions.append((e.label_,e.text,e.start_char))\n",
        "        elif e.label_ == 'PERSON':\n",
        "            chunk_mentions.append((e.label_,e.text,e.start_char))\n",
        "    \n",
        "    # Append the chunk's entities to the aggregate list\n",
        "    entity_mentions_with_start_position.append(chunk_mentions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKH-yYN4XPRE"
      },
      "source": [
        "2. Within each chunk, for each mention of `Deutsche Bank`, search for `PERSON` entities that have a starting position within 200 characters before/after the starting position of the `Deutsche Bank` mention. Count for each `PERSON` entity how many times it occurs nearby a mention of `Deutsche Bank`.  \n",
        "Aggregate the co-occurrences across all chunks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1McG50lJXPRE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'Frank Kuhnke': 2, 'Rebecca Short': 2, 'Frank Werneke': 1, 'Frank Bsirske': 1, 'Olivier Vigneron': 3, 'Paul Achleitner': 1, 'Dagmar Valcárcel': 1, 'Paul Achl': 1, 'PB GY': 3, 'Main': 10, 'Zurich Italy': 1, '2021                                               Key': 1, '2021                              Risk': 1, 'Risk Type': 1, 'Risk Types': 1, 'Significant Increase': 1, '‚': 1, 'Leverage Ratio': 6, 'Eurosystem': 1, 'MREL': 15, 'Consent Order': 2, 'Jeffrey Epstein': 8, 'DB': 1, 'KGaA': 4, 'Datenträgerverfahren': 2, 'Nichtzulassungsbeschwerde': 1, 'Warburg Invest\\nKapitalanlagegesellschaft': 1, 'Warburg Invest': 5, 'Schwab': 3, 'Epstein': 2, 'George Town            Other Enterprise                   ': 2, 'Spólka Akcyjna                                 ': 1, 'Governance': 1, 'Fabrizio Campelli': 1, 'Alexander von zur Mühlen': 2, 'Stefan Simon': 1, 'Durin': 1, 'Sewing': 5, 'Generalbevollmächtigter': 1, 'Paul': 1, 'Achleitner': 2, 'Wirtschaftsprüfer': 1, 'Wirtschaftsprüfungsgesellschaften': 1, 'Significant Institutions': 2, 'Fixed Pay': 1, 'Risk Appetite\\nFramework': 1, 'Global Head': 1, 'Bankakademie Bielefeld': 2, 'Karl von Rohr\\nYear': 1, 'Karl von Rohr': 1, 'von Rohr': 1, 'Kiel': 1, 'Cloud': 1, 'Compliance': 2, 'Lewis': 2, 'von Moltke': 1, 'J.P. Morgan': 1, 'von zur Mühlen': 1, 'Riley': 1, 'European Head': 1, 'Simon': 1, 'Corporate Governance': 2, 'Polska Spółka Akcyjna': 1, 'São Paulo\\n                                                                     Deutsche': 1})\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "co_occurrences = collections.defaultdict(int)\n",
        "for chunk_mentions in entity_mentions_with_start_position:\n",
        "    for mention in chunk_mentions:\n",
        "        if mention[1]=='Deutsche Bank':\n",
        "            db_start = mention[2]\n",
        "            for other_mention in chunk_mentions:\n",
        "                if other_mention[0]=='PERSON' and abs(other_mention[2]-db_start)<=200:\n",
        "                    co_occurrences[other_mention[1]] += 1\n",
        "                    \n",
        "print(co_occurrences)\n",
        "aggregated_co_occurrences = dict(co_occurrences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-5yLuxXPRE"
      },
      "source": [
        "\n",
        "3. Return the number of co-occurrences and the name of the top 5 frequently occurring `PERSON` entities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VYaGMVquXPRE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of co-occurrences:  129\n",
            "Name:  MREL\n",
            "Name:  Main\n",
            "Name:  Jeffrey Epstein\n",
            "Name:  Leverage Ratio\n",
            "Name:  Warburg Invest\n"
          ]
        }
      ],
      "source": [
        "top_5_persons = sorted(aggregated_co_occurrences.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "# Print the number of co-occurrences and the name of the top 5 frequently occurring persons\n",
        "print(\"Number of co-occurrences: \", sum(aggregated_co_occurrences.values()))\n",
        "for person in top_5_persons:\n",
        "    print(\"Name: \", person[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E8o7LlwXPRE"
      },
      "source": [
        "4. Look back at the results of your previous task. Are the `PERSON` entities returned by your co-occurrence method the same ones that appear most frequently by raw counts?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7tn2CZcXPRE"
      },
      "source": [
        "Yes, the most frequent 'PERSON' entities is by name 'MREL' by both the methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OPFK74LXPRF"
      },
      "source": [
        "## 3. Neural Models with Huggingface (3 + 5 + 2 = 10 Points)\n",
        "\n",
        "For state-of-the-art performance, most text-related tasks nowadays use some variation of the Transformer architecture. The particular advantage is especiall the readily available weights for models that have been pre-trained on large general-purpose datasets, which reduces the amount of domain-specific labeled training data.\n",
        "\n",
        "In this task, we will explore the [Huggingface](https://hf.co/) ecosystem to see in which way Transformer models can be used.\n",
        "One of the central aspects of the Huggingface platform is the so-called [Model Hub](https://huggingface.co/models), where you can find many different models uploaded by community members for a variety of tasks.\n",
        "\n",
        "Because the neural models are generally very expensive to run, this exercise will be limited to  less data than in previous questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKs0GoliXPRF"
      },
      "source": [
        "### Sub Task 1: Loading Transformer Models (3 Points)\n",
        "\n",
        "1. Install the `transformers` library and load the model `cardiffnlp/twitter-roberta-base-sentiment-latest` to classify a sequence.\n",
        "2. Report the result of the prediction on the test sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb27KKeoXPRF"
      },
      "outputs": [],
      "source": [
        "from transformers import ## YOUR IMPORTS\n",
        "\n",
        "model = ## YOUR CODE\n",
        "tokenizer = ## YOUR CODE\n",
        "\n",
        "input_text = \"Das ist ein Test.\"\n",
        "\n",
        "prediction = ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD6r4BtWXPRF"
      },
      "source": [
        "### Sub Task 2: Using Pipelines (5 Points)\n",
        "\n",
        "The most succinct way of using a Transformer model is the [`transformers.pipeline`](https://huggingface.co/docs/transformers/pipeline_tutorial). You can check out the linked tutorial for more information on the topic, but essentially, `pipeline` provides a light-weight wrapper around a number of different popular NLP tasks\n",
        "\n",
        "1. Instead of manually defining a pipeline, now load a model through a `\"text-classification\"` pipeline. Look up the neural model that is loaded by default, and post the link to its [model card](https://huggingface.co/docs/hub/model-cards) below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "185wlN3wXPRF"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Gyv_LNXPRF"
      },
      "source": [
        "2. Now, instead, load a pipeline for `\"text-classification\"`, but with a custom model and tokenizer. Use the Model Hub platform to find the most popular model for the German language (by number of downloads) and manually specify the usage of another model (and tokenizer) to the pipeline. Re-run the previous example, and report the prediction result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnXPYk__XPRF"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = ## YOUR CODE\n",
        "tokenizer = ## YOUR CODE\n",
        "\n",
        "# Instantiate the pipeline with custom components\n",
        "pipe = ## YOUR CODE\n",
        "\n",
        "# Output the prediction by your pipe on the test sample.\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0kv0z4_XPRF"
      },
      "source": [
        "3. Keeping in line with the previous exercises, let us now try and actually predict something with the model. Re-load a pipeline, this time for Named Entity Recognition, using the default model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gUJGbWxXPRF"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVEzh84PXPRF"
      },
      "source": [
        "4. Run the pipeline with the text from the Deutsche Bank report from Question 2 and output the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gu5SqSTXPRG"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE\n",
        "\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUygSaMSXPRG"
      },
      "source": [
        "5. Look at the results. Something looks strange here; why is it not working properly? Elaborate your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYlLmqwtXPRG"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63eIg1QGXPRG"
      },
      "source": [
        "### Sub Task 3: Using Datasets through Huggingface (2 Points)\n",
        "\n",
        "Instead of using the `transformers` library for model training and inference, it is also possible to use other libraries by Huggingface without neural models.\n",
        "In particular, the `datasets` library provides a centralized and streamlined way of accessing a variety of different datasets.\n",
        "\n",
        "1. Using the `datasets` library, load the `imdb` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0Q8A1etXPRG"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDjxp8YGXPRG"
      },
      "source": [
        "2. Report the mean length of `text` column for the training, validation and test split, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVunrlyWXPRG"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "LSTM_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "c2042d1922a07c71a3ec2c9c9cbb77e812df8ca713873bc0ea973593272381e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
