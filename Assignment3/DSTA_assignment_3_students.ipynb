{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Text Classification and Word Embeddings\n",
    "\n",
    "Due: Monday, January 23, 2023, at 2pm via Moodle\n",
    "\n",
    "**Team Members** `<Fill out>`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Guidelines\n",
    "\n",
    "- Solutions need to be uploaded as a single Jupyter notebook. You will see that this notebook contains some pre-filled cells that you should complete for the individual tasks.\n",
    "- For answers requiring written solutions, use Markdown cells (in combination with Jupyter LaTeX support) **inside this notebook**. Do *not* hand in any separate files, simply re-upload the `.ipynb` file.\n",
    "- Download the .zip file containing the dataset but do *not* upload it with your solution.\n",
    "- Make sure that the names of all team members are present in the solution (see cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "%%python \"--version\"\n",
    "# check requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in e:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: spacy in e:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: gensim in e:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 3)) (4.1.2)\n",
      "Requirement already satisfied: numpy in e:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 4)) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in e:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 5)) (3.5.2)\n",
      "Requirement already satisfied: seaborn in e:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 6)) (0.11.2)\n",
      "Requirement already satisfied: scikit-learn in e:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 7)) (1.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in e:\\anaconda\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2022.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (3.0.10)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (63.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (1.10.2)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (8.1.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (2.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (1.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\anaconda\\lib\\site-packages (from spacy->-r requirements.txt (line 2)) (2.0.8)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in e:\\anaconda\\lib\\site-packages (from gensim->-r requirements.txt (line 3)) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in e:\\anaconda\\lib\\site-packages (from gensim->-r requirements.txt (line 3)) (1.9.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in e:\\anaconda\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\anaconda\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\anaconda\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\anaconda\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in e:\\anaconda\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (9.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\anaconda\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\anaconda\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in e:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy->-r requirements.txt (line 2)) (4.3.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 2)) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 2)) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 2)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 2)) (0.0.3)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy->-r requirements.txt (line 2)) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\anaconda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy->-r requirements.txt (line 2)) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\anaconda\\lib\\site-packages (from jinja2->spacy->-r requirements.txt (line 2)) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "%pip install \"-r\" requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 13.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in e:\\anaconda\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in e:\\anaconda\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in e:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\anaconda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\anaconda\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%%python \"-m\" spacy download en_core_web_sm\n",
    "# Install required language model for spaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: F.R.I.E.N.D.S and  Word2Vec (6 + 8 + 4 = 18 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Friends](https://en.wikipedia.org/wiki/Friends) is an American television sitcom, created by David Crane and Marta Kauffman. In this problem set we will use the transcripts from the show to train a Word2Vec model using the [Gensim](https://radimrehurek.com/gensim/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd  \n",
    "from collections import defaultdict, Counter  \n",
    "import spacy \n",
    "import logging\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.manifold import TSNE\n",
    "from typing import Type\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-processing (6 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading and cleaning the data. The dataset for this problem set can be found in the attached `data` folder. Load the `friends_quotes.csv` file using pandas. The dataset is from [Kaggle](https://www.kaggle.com/ryanstonebraker/friends-transcript) and is created for building a classifier that can determine which friend from the Friend's TV Show would be most likely to say a quote. The column `quote` contains the line from the movie and `author` is the one who spoke it. Since these are the only two columns we need, we remove the rest and only keep these two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= \"%H:%M:%S\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_title</th>\n",
       "      <th>quote</th>\n",
       "      <th>quote_order</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>All right Joey, be nice. So does he have a hum...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>Just, 'cause, I don't want her to go through w...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monica</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>Okay, everybody relax. This is not even a date...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>Sounds like a date to me.</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>Alright, so I'm back in high school, I'm stand...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>All</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>Oh, yeah. Had that dream.</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monica Gets A Roommate</td>\n",
       "      <td>Then I look down, and I realize there's a phon...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author  episode_number           episode_title  \\\n",
       "0    Monica             1.0  Monica Gets A Roommate   \n",
       "1      Joey             1.0  Monica Gets A Roommate   \n",
       "2  Chandler             1.0  Monica Gets A Roommate   \n",
       "3    Phoebe             1.0  Monica Gets A Roommate   \n",
       "4    Phoebe             1.0  Monica Gets A Roommate   \n",
       "5    Monica             1.0  Monica Gets A Roommate   \n",
       "6  Chandler             1.0  Monica Gets A Roommate   \n",
       "7  Chandler             1.0  Monica Gets A Roommate   \n",
       "8       All             1.0  Monica Gets A Roommate   \n",
       "9  Chandler             1.0  Monica Gets A Roommate   \n",
       "\n",
       "                                               quote  quote_order  season  \n",
       "0  There's nothing to tell! He's just some guy I ...          0.0     1.0  \n",
       "1  C'mon, you're going out with the guy! There's ...          1.0     1.0  \n",
       "2  All right Joey, be nice. So does he have a hum...          2.0     1.0  \n",
       "3                           Wait, does he eat chalk?          3.0     1.0  \n",
       "4  Just, 'cause, I don't want her to go through w...          4.0     1.0  \n",
       "5  Okay, everybody relax. This is not even a date...          5.0     1.0  \n",
       "6                          Sounds like a date to me.          6.0     1.0  \n",
       "7  Alright, so I'm back in high school, I'm stand...          7.0     1.0  \n",
       "8                          Oh, yeah. Had that dream.          8.0     1.0  \n",
       "9  Then I look down, and I realize there's a phon...          9.0     1.0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/friends_quotes.csv\")\n",
    "### filter out columns ###\n",
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there is no missing data, so we do not need to worry about that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author            0\n",
       "episode_number    0\n",
       "episode_title     0\n",
       "quote             0\n",
       "quote_order       0\n",
       "season            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() # check for missing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SpaCy similar to assignment 2 to pre-process the text and perform the following steps: \n",
    "- lowercase the words \n",
    "- remove stopwords and single characters\n",
    "- use regex to remove non-alphabetic characters; in other words: only keep \"a\" to \"z\" and digits. \n",
    "- remove lines that have less than 3 words, since they cannot contribute much to the training process.\n",
    "\n",
    "Please do not add additional steps on your own or additional cleaning as we want to create comparable results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "c'mon, you're going out with the guy! there's gotta be something wrong with him!\n",
      "c'mon, you're going guy! there's gotta wrong him!\n",
      "c mon  you re going guy  there s gotta wrong him\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS # only use these stop words, do not add your own!\n",
    "print(df[\"quote\"][1])\n",
    "df[\"quote\"] = df[\"quote\"].str.casefold()\n",
    "print(df[\"quote\"][1])\n",
    "df[\"quote\"] = df[\"quote\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "print(df[\"quote\"][1])\n",
    "df[\"quote\"] = df[\"quote\"].str.replace('[^a-zA-Z0-9]', ' ', regex=True).str.strip()\n",
    "print(df[\"quote\"][1])\n",
    "df[\"quote\"] = df[\"quote\"].str.replace('\\\\b\\\\w{1,2}\\\\s', ' ', regex=True).str.strip()\n",
    "df = df[df[\"quote\"].str.split().str.len().gt(3)]\n",
    "# NOTE: idk if the above line would count sentences with empty \" \" in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          there  tell    guy work with\n",
       "1           mon  you  going guy  there  gotta wrong him\n",
       "2                right joey  nice  hump  hump hairpiece\n",
       "4                 just   cause  don  want went carl  oh\n",
       "5     okay  everybody relax  date    people going di...\n",
       "6                                   sounds like date me\n",
       "7     alright    high school    standing middle cafe...\n",
       "9              look down  realize there  phone    there\n",
       "14    sudden  phone starts ring  don  know   everybo...\n",
       "16    finally  figure   better answer   turns   moth...\n",
       "Name: quote, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"quote\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes =df[\"quote\"].tolist() # to save all the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_temp_list = []\n",
    "for item in quotes:\n",
    "    lst_temp = item.split(\" \")\n",
    "    lst_temp = [x.strip() for x in lst_temp if x.strip()]\n",
    "    quotes_temp_list.append(lst_temp)\n",
    "quotes = quotes_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['there', 'tell', 'guy', 'work', 'with'], ['mon', 'you', 'going', 'guy', 'there', 'gotta', 'wrong', 'him'], ['right', 'joey', 'nice', 'hump', 'hump', 'hairpiece'], ['just', 'cause', 'don', 'want', 'went', 'carl', 'oh'], ['okay', 'everybody', 'relax', 'date', 'people', 'going', 'dinner', 'and', 'having', 'sex'], ['sounds', 'like', 'date', 'me'], ['alright', 'high', 'school', 'standing', 'middle', 'cafeteria', 'realize', 'totally', 'naked'], ['look', 'down', 'realize', 'there', 'phone', 'there'], ['sudden', 'phone', 'starts', 'ring', 'don', 'know', 'everybody', 'starts', 'looking', 'me'], ['finally', 'figure', 'better', 'answer', 'turns', 'mother', 'very', 'very', 'weird', 'because', 'calls', 'me']]\n"
     ]
    }
   ],
   "source": [
    "print(quotes[:10])\n",
    "# preserve the original version of quotes\n",
    "# NOTE: I only added the part since the original code mentioned \"quotes\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the vocabulary of the words and word combinations we want to learn representations from. We choose a subset of the most frequent words and bigrams to represent our corpus.\n",
    "- Use the Gensim Phrases package to automatically detect common phrases (bigrams) from a list of lines from the previous step (`min_count=10`). Now words like New_York will be considered as one entity and character names like joey_tribbiani will be recognized.\n",
    "- Create a list of words/bigrams with their frequencies and choose the top 15.000 words for the vocabulary, to keep the computation time-limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 00:42:11: collecting all words and their counts\n",
      "INFO - 00:42:11: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 00:42:12: PROGRESS: at sentence #10000, processed 86118 words and 64214 word types\n",
      "INFO - 00:42:12: PROGRESS: at sentence #20000, processed 174387 words and 111649 word types\n",
      "INFO - 00:42:13: PROGRESS: at sentence #30000, processed 261704 words and 155469 word types\n",
      "INFO - 00:42:13: collected 170125 token types (unigram + bigrams) from a corpus of 291447 words and 33546 sentences\n",
      "INFO - 00:42:13: merged Phrases<170125 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 00:42:13: Phrases lifecycle event {'msg': 'built Phrases<170125 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000> in 1.68s', 'datetime': '2023-01-16T00:42:13.651191', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "INFO - 00:42:13: exporting phrases from Phrases<170125 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 00:42:14: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<240 phrases, min_count=10, threshold=10.0> from Phrases<170125 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000> in 0.80s', 'datetime': '2023-01-16T00:42:14.528761', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "words =  Phrases(quotes, min_count=10) \n",
    "bigram =  Phraser(words)\n",
    "#new_lines = # transform the lines #\n",
    "#new_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sounds_like', 'date', 'me']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lines =  bigram[quotes]\n",
    "new_lines[5]\n",
    "# shows that it detects phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('know', 6302), ('you', 5857), ('okay', 3784), ('don', 3602), ('that', 3602)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### find the top words for the vocabulary###\n",
    "top_words = Counter(word for new_lines in new_lines for word in new_lines)\n",
    "vocab = sorted(top_words.items(), key=lambda x: x[1], reverse=True)[:15000] \n",
    "vocab[:5]### top words ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('squints', 1)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q: I don't get why we should filter the front 15,000 words, since if we train the\n",
    "gensim model with min_count = 2, the words not in vocab would not be present either?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train only for the top words\n",
    "# NOTE: I know this is dumb!!! Please don't laugh\n",
    "vocab_lst = [x[0] for x in vocab]\n",
    "lines = [] # to store the new lines\n",
    "for i in range(len(new_lines)):\n",
    "    element = new_lines[i]\n",
    "    lines.append([x for x in element if x in vocab_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['there', 'tell', 'guy', 'work', 'with'], ['mon', 'you', 'going', 'guy', 'there', 'gotta', 'wrong', 'him'], ['right', 'joey', 'nice', 'hump', 'hump', 'hairpiece'], ['just', 'cause', 'don', 'want', 'went', 'carl', 'oh'], ['okay', 'everybody', 'relax', 'date', 'people', 'going', 'dinner', 'and', 'having_sex']]\n"
     ]
    }
   ],
   "source": [
    "print(lines[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training (8 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Gensim implementation of Word2Vec to train a model on the quotes. The training can be divided into 3 stages:\n",
    "\n",
    "1. Set up and configure your model. Define the parameters in such a way that the following conditions are satisfied:\n",
    "    - Ignores all words that have a total absolute frequency less than 2\n",
    "    - Dimensions of the embeddings: 100 \n",
    "    - Initial learning rate of 0.03 \n",
    "    - 20 negative samples \n",
    "    - Window size 3 \n",
    "    - The learning rate in the training will decrease as you apply more and more updates. Most of the time when starting with gradient descent the initial steps can be larger, and as we get close to the local minima it is best to use smaller steps. This adjustment is done internally using a learning rate scheduler. Make sure that the smallest learning rate does not go below 0.0001.\n",
    "    - Set the threshold for configuring which higher-frequency words are randomly down-sampled to 6e-5. This parameter forces the sampling to choose the very frequent words less often in the sampling.\n",
    "    - Set the hashfunction of the word2vec to the given function.\n",
    "    - Train on a single worker to make sure you get the same result as ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash(astring):\n",
    "    return ord(astring[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 01:37:49: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.03)', 'datetime': '2023-01-16T01:37:49.636880', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec(min_count = 2, vector_size = 100, alpha = 0.03, \\\n",
    "    negative = 20, window = 3, min_alpha = 0.0001, sample = 6e-5, \\\n",
    "        hashfxn = hash,  workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = lines # lines\n",
    "# NOTE: I did not want to use the 15,000 word filtered version. I don't think it makes \n",
    "# that much difference since the word count values are very close.\n",
    "# NOTE: feel free to switch to \"lines\" if you want the 15,000-word version."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Before training, Word2Vec requires us to build the vocabulary table by filtering out the unique words and doing some basic counts on them. If you look at the logs you can see the effect of `min_count` and `sample` on the word corpus. Use the `build_vocab` function to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 01:38:10: collecting all words and their counts\n",
      "INFO - 01:38:10: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 01:38:10: PROGRESS: at sentence #10000, processed 83852 words, keeping 9048 word types\n",
      "INFO - 01:38:10: PROGRESS: at sentence #20000, processed 169452 words, keeping 12091 word types\n",
      "INFO - 01:38:10: PROGRESS: at sentence #30000, processed 254065 words, keeping 14738 word types\n",
      "INFO - 01:38:10: collected 15000 word types from a corpus of 282408 raw words and 33546 sentences\n",
      "INFO - 01:38:10: Creating a fresh vocabulary\n",
      "INFO - 01:38:10: Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 9158 unique words (61.053333333333335%% of original 15000, drops 5842)', 'datetime': '2023-01-16T01:38:10.559826', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 01:38:10: Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 276566 word corpus (97.93136171779837%% of original 282408, drops 5842)', 'datetime': '2023-01-16T01:38:10.562130', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 01:38:10: deleting the raw counts dictionary of 15000 items\n",
      "INFO - 01:38:10: sample=6e-05 downsamples 966 most-common words\n",
      "INFO - 01:38:10: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 127843.574023053 word corpus (46.2%% of prior 276566)', 'datetime': '2023-01-16T01:38:10.671242', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 01:38:11: estimated required memory for 9158 words and 100 dimensions: 11905400 bytes\n",
      "INFO - 01:38:11: resetting layer weights\n",
      "INFO - 01:38:11: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-01-16T01:38:11.094567', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "w2v.build_vocab(corpus_iterable=training_data)\n",
    "# w2v.build_vocab(corpus_iterable=new_lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finally, we get to train the model. Train the model for 100 epochs. This will take a while. As we do not plan to train the model any further, we call `init_sims()`, which will make the model much more memory-efficient by precomputing L2-norms of word weight vectors for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 01:38:16: Word2Vec lifecycle event {'msg': 'training model with 1 workers on 9158 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=3 shrink_windows=True', 'datetime': '2023-01-16T01:38:16.844421', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "INFO - 01:38:17: EPOCH 1 - PROGRESS: at 45.95% examples, 55988 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:18: EPOCH 1 - PROGRESS: at 87.95% examples, 53156 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:19: EPOCH - 1 : training on 282408 raw words (127416 effective words) took 2.4s, 53587 effective words/s\n",
      "INFO - 01:38:20: EPOCH 2 - PROGRESS: at 31.94% examples, 40405 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:21: EPOCH 2 - PROGRESS: at 63.36% examples, 39896 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:38:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:22: EPOCH - 2 : training on 282408 raw words (127649 effective words) took 3.0s, 42884 effective words/s\n",
      "INFO - 01:38:23: EPOCH 3 - PROGRESS: at 38.95% examples, 48450 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:24: EPOCH 3 - PROGRESS: at 81.02% examples, 49956 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:24: EPOCH - 3 : training on 282408 raw words (128004 effective words) took 2.4s, 53526 effective words/s\n",
      "INFO - 01:38:25: EPOCH 4 - PROGRESS: at 70.19% examples, 88323 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:26: EPOCH - 4 : training on 282408 raw words (128016 effective words) took 1.7s, 74045 effective words/s\n",
      "INFO - 01:38:27: EPOCH 5 - PROGRESS: at 38.95% examples, 48061 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:28: EPOCH 5 - PROGRESS: at 81.02% examples, 49340 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:28: EPOCH - 5 : training on 282408 raw words (127579 effective words) took 2.5s, 51472 effective words/s\n",
      "INFO - 01:38:29: EPOCH 6 - PROGRESS: at 49.44% examples, 61975 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:38:30: EPOCH 6 - PROGRESS: at 98.96% examples, 62017 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:30: EPOCH - 6 : training on 282408 raw words (127880 effective words) took 2.1s, 62208 effective words/s\n",
      "INFO - 01:38:32: EPOCH 7 - PROGRESS: at 59.79% examples, 73976 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:32: EPOCH - 7 : training on 282408 raw words (127645 effective words) took 1.4s, 89168 effective words/s\n",
      "INFO - 01:38:33: EPOCH 8 - PROGRESS: at 52.74% examples, 63579 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:34: EPOCH - 8 : training on 282408 raw words (128016 effective words) took 1.9s, 68188 effective words/s\n",
      "INFO - 01:38:35: EPOCH 9 - PROGRESS: at 66.72% examples, 84326 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:35: EPOCH - 9 : training on 282408 raw words (127625 effective words) took 1.6s, 78985 effective words/s\n",
      "INFO - 01:38:37: EPOCH 10 - PROGRESS: at 56.34% examples, 68415 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:37: EPOCH - 10 : training on 282408 raw words (127852 effective words) took 1.8s, 70769 effective words/s\n",
      "INFO - 01:38:38: EPOCH 11 - PROGRESS: at 56.34% examples, 68290 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:39: EPOCH - 11 : training on 282408 raw words (127527 effective words) took 1.8s, 71095 effective words/s\n",
      "INFO - 01:38:40: EPOCH 12 - PROGRESS: at 59.79% examples, 70891 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:41: EPOCH - 12 : training on 282408 raw words (127585 effective words) took 2.0s, 64321 effective words/s\n",
      "INFO - 01:38:42: EPOCH 13 - PROGRESS: at 70.19% examples, 87067 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:43: EPOCH - 13 : training on 282408 raw words (127579 effective words) took 1.5s, 85843 effective words/s\n",
      "INFO - 01:38:44: EPOCH 14 - PROGRESS: at 56.34% examples, 71585 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:38:45: EPOCH 14 - PROGRESS: at 95.27% examples, 59520 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:45: EPOCH - 14 : training on 282408 raw words (127925 effective words) took 2.2s, 58611 effective words/s\n",
      "INFO - 01:38:46: EPOCH 15 - PROGRESS: at 49.44% examples, 60026 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:47: EPOCH 15 - PROGRESS: at 98.96% examples, 61662 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:47: EPOCH - 15 : training on 282408 raw words (127687 effective words) took 2.1s, 61815 effective words/s\n",
      "INFO - 01:38:48: EPOCH 16 - PROGRESS: at 49.44% examples, 62082 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:49: EPOCH - 16 : training on 282408 raw words (127712 effective words) took 1.8s, 71032 effective words/s\n",
      "INFO - 01:38:50: EPOCH 17 - PROGRESS: at 56.34% examples, 71503 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:38:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:50: EPOCH - 17 : training on 282408 raw words (128024 effective words) took 1.6s, 79154 effective words/s\n",
      "INFO - 01:38:51: EPOCH 18 - PROGRESS: at 59.79% examples, 70462 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:52: EPOCH - 18 : training on 282408 raw words (127934 effective words) took 1.9s, 68881 effective words/s\n",
      "INFO - 01:38:53: EPOCH 19 - PROGRESS: at 70.19% examples, 89497 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:38:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:54: EPOCH - 19 : training on 282408 raw words (127576 effective words) took 1.3s, 98686 effective words/s\n",
      "INFO - 01:38:55: EPOCH 20 - PROGRESS: at 73.91% examples, 91856 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:55: EPOCH - 20 : training on 282408 raw words (127697 effective words) took 1.3s, 95684 effective words/s\n",
      "INFO - 01:38:56: EPOCH 21 - PROGRESS: at 77.37% examples, 95551 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:56: EPOCH - 21 : training on 282408 raw words (127952 effective words) took 1.3s, 98392 effective words/s\n",
      "INFO - 01:38:57: EPOCH 22 - PROGRESS: at 56.34% examples, 69706 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:38:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:58: EPOCH - 22 : training on 282408 raw words (127955 effective words) took 1.6s, 80756 effective words/s\n",
      "INFO - 01:38:59: EPOCH 23 - PROGRESS: at 70.19% examples, 85390 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:38:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:38:59: EPOCH - 23 : training on 282408 raw words (127583 effective words) took 1.6s, 77325 effective words/s\n",
      "INFO - 01:39:00: EPOCH 24 - PROGRESS: at 77.37% examples, 97339 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:01: EPOCH - 24 : training on 282408 raw words (127545 effective words) took 1.3s, 99634 effective words/s\n",
      "INFO - 01:39:02: EPOCH 25 - PROGRESS: at 81.02% examples, 101785 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:02: EPOCH - 25 : training on 282408 raw words (127895 effective words) took 1.2s, 102894 effective words/s\n",
      "INFO - 01:39:03: EPOCH 26 - PROGRESS: at 66.72% examples, 85314 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:03: EPOCH - 26 : training on 282408 raw words (128054 effective words) took 1.5s, 87761 effective words/s\n",
      "INFO - 01:39:04: EPOCH 27 - PROGRESS: at 95.27% examples, 119960 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:05: EPOCH - 27 : training on 282408 raw words (127844 effective words) took 1.1s, 118446 effective words/s\n",
      "INFO - 01:39:06: EPOCH 28 - PROGRESS: at 84.29% examples, 106622 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:06: EPOCH - 28 : training on 282408 raw words (127408 effective words) took 1.2s, 104603 effective words/s\n",
      "INFO - 01:39:07: EPOCH 29 - PROGRESS: at 81.02% examples, 99495 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:07: EPOCH - 29 : training on 282408 raw words (128179 effective words) took 1.3s, 98173 effective words/s\n",
      "INFO - 01:39:08: EPOCH 30 - PROGRESS: at 81.02% examples, 99862 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:08: EPOCH - 30 : training on 282408 raw words (127562 effective words) took 1.3s, 99226 effective words/s\n",
      "INFO - 01:39:09: EPOCH 31 - PROGRESS: at 73.91% examples, 94036 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:10: EPOCH - 31 : training on 282408 raw words (127361 effective words) took 1.3s, 98366 effective words/s\n",
      "INFO - 01:39:11: EPOCH 32 - PROGRESS: at 70.19% examples, 88736 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:11: EPOCH - 32 : training on 282408 raw words (127684 effective words) took 1.4s, 90038 effective words/s\n",
      "INFO - 01:39:12: EPOCH 33 - PROGRESS: at 73.91% examples, 94009 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:12: EPOCH - 33 : training on 282408 raw words (127799 effective words) took 1.3s, 95771 effective words/s\n",
      "INFO - 01:39:14: EPOCH 34 - PROGRESS: at 70.19% examples, 84934 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:14: EPOCH - 34 : training on 282408 raw words (127988 effective words) took 1.8s, 70181 effective words/s\n",
      "INFO - 01:39:15: EPOCH 35 - PROGRESS: at 56.34% examples, 70906 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:16: EPOCH - 35 : training on 282408 raw words (127772 effective words) took 1.6s, 80172 effective words/s\n",
      "INFO - 01:39:17: EPOCH 36 - PROGRESS: at 59.79% examples, 75826 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:17: EPOCH - 36 : training on 282408 raw words (128155 effective words) took 1.6s, 81894 effective words/s\n",
      "INFO - 01:39:19: EPOCH 37 - PROGRESS: at 52.74% examples, 63671 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:19: EPOCH - 37 : training on 282408 raw words (127857 effective words) took 1.7s, 75481 effective words/s\n",
      "INFO - 01:39:20: EPOCH 38 - PROGRESS: at 73.91% examples, 92466 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:21: EPOCH - 38 : training on 282408 raw words (127899 effective words) took 1.3s, 98835 effective words/s\n",
      "INFO - 01:39:22: EPOCH 39 - PROGRESS: at 87.95% examples, 110038 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:22: EPOCH - 39 : training on 282408 raw words (127661 effective words) took 1.1s, 112450 effective words/s\n",
      "INFO - 01:39:23: EPOCH 40 - PROGRESS: at 73.91% examples, 92855 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:23: EPOCH - 40 : training on 282408 raw words (127808 effective words) took 1.3s, 96934 effective words/s\n",
      "INFO - 01:39:24: EPOCH 41 - PROGRESS: at 81.02% examples, 99325 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:24: EPOCH - 41 : training on 282408 raw words (127974 effective words) took 1.3s, 99339 effective words/s\n",
      "INFO - 01:39:25: EPOCH 42 - PROGRESS: at 49.44% examples, 59562 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:26: EPOCH 42 - PROGRESS: at 87.95% examples, 54412 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:27: EPOCH - 42 : training on 282408 raw words (127902 effective words) took 2.3s, 56625 effective words/s\n",
      "INFO - 01:39:28: EPOCH 43 - PROGRESS: at 63.36% examples, 78214 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:28: EPOCH - 43 : training on 282408 raw words (127745 effective words) took 1.5s, 83827 effective words/s\n",
      "INFO - 01:39:29: EPOCH 44 - PROGRESS: at 70.19% examples, 89476 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:30: EPOCH - 44 : training on 282408 raw words (127723 effective words) took 1.8s, 70102 effective words/s\n",
      "INFO - 01:39:31: EPOCH 45 - PROGRESS: at 66.72% examples, 85310 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:32: EPOCH - 45 : training on 282408 raw words (127879 effective words) took 1.8s, 72408 effective words/s\n",
      "INFO - 01:39:33: EPOCH 46 - PROGRESS: at 45.95% examples, 56730 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:34: EPOCH - 46 : training on 282408 raw words (127916 effective words) took 2.0s, 63624 effective words/s\n",
      "INFO - 01:39:35: EPOCH 47 - PROGRESS: at 84.29% examples, 105292 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:35: EPOCH - 47 : training on 282408 raw words (127812 effective words) took 1.2s, 104828 effective words/s\n",
      "INFO - 01:39:36: EPOCH 48 - PROGRESS: at 77.37% examples, 95952 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:37: EPOCH - 48 : training on 282408 raw words (127810 effective words) took 1.5s, 82789 effective words/s\n",
      "INFO - 01:39:38: EPOCH 49 - PROGRESS: at 42.46% examples, 48166 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:39: EPOCH 49 - PROGRESS: at 91.61% examples, 53690 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:39: EPOCH - 49 : training on 282408 raw words (127901 effective words) took 2.4s, 52538 effective words/s\n",
      "INFO - 01:39:40: EPOCH 50 - PROGRESS: at 42.46% examples, 49485 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:41: EPOCH 50 - PROGRESS: at 84.29% examples, 49620 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:42: EPOCH - 50 : training on 282408 raw words (128187 effective words) took 2.5s, 51371 effective words/s\n",
      "INFO - 01:39:43: EPOCH 51 - PROGRESS: at 52.74% examples, 65843 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:43: EPOCH - 51 : training on 282408 raw words (127675 effective words) took 1.8s, 69849 effective words/s\n",
      "INFO - 01:39:44: EPOCH 52 - PROGRESS: at 70.19% examples, 87997 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:45: EPOCH - 52 : training on 282408 raw words (127810 effective words) took 1.4s, 88290 effective words/s\n",
      "INFO - 01:39:46: EPOCH 53 - PROGRESS: at 70.19% examples, 88154 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:46: EPOCH - 53 : training on 282408 raw words (127638 effective words) took 1.5s, 87853 effective words/s\n",
      "INFO - 01:39:47: EPOCH 54 - PROGRESS: at 49.44% examples, 62381 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:48: EPOCH - 54 : training on 282408 raw words (127688 effective words) took 1.8s, 70281 effective words/s\n",
      "INFO - 01:39:49: EPOCH 55 - PROGRESS: at 84.29% examples, 105919 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:49: EPOCH - 55 : training on 282408 raw words (127839 effective words) took 1.2s, 105494 effective words/s\n",
      "INFO - 01:39:50: EPOCH 56 - PROGRESS: at 49.44% examples, 62835 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:51: EPOCH - 56 : training on 282408 raw words (127890 effective words) took 1.9s, 69093 effective words/s\n",
      "INFO - 01:39:52: EPOCH 57 - PROGRESS: at 73.91% examples, 91991 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:53: EPOCH - 57 : training on 282408 raw words (127697 effective words) took 1.4s, 92913 effective words/s\n",
      "INFO - 01:39:54: EPOCH 58 - PROGRESS: at 63.36% examples, 79158 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:54: EPOCH - 58 : training on 282408 raw words (127968 effective words) took 1.5s, 85767 effective words/s\n",
      "INFO - 01:39:55: EPOCH 59 - PROGRESS: at 70.19% examples, 87693 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:56: EPOCH - 59 : training on 282408 raw words (127760 effective words) took 1.4s, 88146 effective words/s\n",
      "INFO - 01:39:57: EPOCH 60 - PROGRESS: at 52.74% examples, 67186 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:39:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:57: EPOCH - 60 : training on 282408 raw words (127649 effective words) took 1.6s, 81733 effective words/s\n",
      "INFO - 01:39:58: EPOCH 61 - PROGRESS: at 52.74% examples, 64689 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:39:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:39:59: EPOCH - 61 : training on 282408 raw words (127744 effective words) took 1.8s, 72421 effective words/s\n",
      "INFO - 01:40:00: EPOCH 62 - PROGRESS: at 52.74% examples, 64181 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:01: EPOCH - 62 : training on 282408 raw words (127944 effective words) took 2.0s, 64147 effective words/s\n",
      "INFO - 01:40:02: EPOCH 63 - PROGRESS: at 59.79% examples, 72934 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:03: EPOCH - 63 : training on 282408 raw words (127837 effective words) took 1.7s, 75160 effective words/s\n",
      "INFO - 01:40:04: EPOCH 64 - PROGRESS: at 49.44% examples, 62099 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:04: EPOCH - 64 : training on 282408 raw words (127932 effective words) took 1.8s, 72235 effective words/s\n",
      "INFO - 01:40:05: EPOCH 65 - PROGRESS: at 81.02% examples, 102666 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:06: EPOCH - 65 : training on 282408 raw words (128029 effective words) took 1.2s, 103924 effective words/s\n",
      "INFO - 01:40:07: EPOCH 66 - PROGRESS: at 70.19% examples, 87305 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:07: EPOCH - 66 : training on 282408 raw words (127575 effective words) took 1.4s, 87995 effective words/s\n",
      "INFO - 01:40:08: EPOCH 67 - PROGRESS: at 38.95% examples, 48062 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:09: EPOCH 67 - PROGRESS: at 87.95% examples, 53428 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:09: EPOCH - 67 : training on 282408 raw words (127736 effective words) took 2.3s, 55692 effective words/s\n",
      "INFO - 01:40:10: EPOCH 68 - PROGRESS: at 52.74% examples, 66643 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:12: EPOCH 68 - PROGRESS: at 98.96% examples, 61293 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:12: EPOCH - 68 : training on 282408 raw words (127547 effective words) took 2.1s, 61353 effective words/s\n",
      "INFO - 01:40:13: EPOCH 69 - PROGRESS: at 49.44% examples, 60881 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:13: EPOCH - 69 : training on 282408 raw words (127923 effective words) took 1.8s, 72201 effective words/s\n",
      "INFO - 01:40:14: EPOCH 70 - PROGRESS: at 45.95% examples, 54264 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:15: EPOCH 70 - PROGRESS: at 81.02% examples, 49003 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:16: EPOCH - 70 : training on 282408 raw words (127829 effective words) took 2.6s, 48540 effective words/s\n",
      "INFO - 01:40:17: EPOCH 71 - PROGRESS: at 38.95% examples, 49565 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:18: EPOCH 71 - PROGRESS: at 95.27% examples, 58482 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:18: EPOCH - 71 : training on 282408 raw words (127800 effective words) took 2.2s, 59099 effective words/s\n",
      "INFO - 01:40:19: EPOCH 72 - PROGRESS: at 52.74% examples, 66674 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:20: EPOCH - 72 : training on 282408 raw words (127924 effective words) took 1.9s, 68983 effective words/s\n",
      "INFO - 01:40:21: EPOCH 73 - PROGRESS: at 56.34% examples, 69261 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:22: EPOCH - 73 : training on 282408 raw words (127850 effective words) took 1.6s, 78468 effective words/s\n",
      "INFO - 01:40:23: EPOCH 74 - PROGRESS: at 56.34% examples, 69046 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:23: EPOCH - 74 : training on 282408 raw words (127952 effective words) took 1.7s, 77242 effective words/s\n",
      "INFO - 01:40:24: EPOCH 75 - PROGRESS: at 63.36% examples, 77542 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:25: EPOCH - 75 : training on 282408 raw words (127970 effective words) took 1.6s, 80217 effective words/s\n",
      "INFO - 01:40:26: EPOCH 76 - PROGRESS: at 84.29% examples, 102927 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:26: EPOCH - 76 : training on 282408 raw words (127866 effective words) took 1.2s, 104292 effective words/s\n",
      "INFO - 01:40:27: EPOCH 77 - PROGRESS: at 81.02% examples, 100082 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:28: EPOCH - 77 : training on 282408 raw words (127911 effective words) took 1.3s, 99247 effective words/s\n",
      "INFO - 01:40:29: EPOCH 78 - PROGRESS: at 77.37% examples, 96295 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:29: EPOCH - 78 : training on 282408 raw words (127886 effective words) took 1.3s, 99894 effective words/s\n",
      "INFO - 01:40:30: EPOCH 79 - PROGRESS: at 84.29% examples, 105192 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:30: EPOCH - 79 : training on 282408 raw words (127828 effective words) took 1.2s, 108666 effective words/s\n",
      "INFO - 01:40:31: EPOCH 80 - PROGRESS: at 70.19% examples, 89381 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:31: EPOCH - 80 : training on 282408 raw words (127870 effective words) took 1.4s, 92298 effective words/s\n",
      "INFO - 01:40:32: EPOCH 81 - PROGRESS: at 87.95% examples, 111983 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:33: EPOCH - 81 : training on 282408 raw words (128038 effective words) took 1.1s, 111832 effective words/s\n",
      "INFO - 01:40:34: EPOCH 82 - PROGRESS: at 84.29% examples, 107105 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:34: EPOCH - 82 : training on 282408 raw words (127644 effective words) took 1.2s, 107134 effective words/s\n",
      "INFO - 01:40:35: EPOCH 83 - PROGRESS: at 91.61% examples, 114438 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:35: EPOCH - 83 : training on 282408 raw words (127644 effective words) took 1.1s, 115853 effective words/s\n",
      "INFO - 01:40:36: EPOCH 84 - PROGRESS: at 91.61% examples, 113430 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:36: EPOCH - 84 : training on 282408 raw words (128003 effective words) took 1.1s, 111734 effective words/s\n",
      "INFO - 01:40:37: EPOCH 85 - PROGRESS: at 73.91% examples, 91338 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:37: EPOCH - 85 : training on 282408 raw words (127676 effective words) took 1.4s, 92199 effective words/s\n",
      "INFO - 01:40:38: EPOCH 86 - PROGRESS: at 81.02% examples, 102630 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:39: EPOCH - 86 : training on 282408 raw words (127918 effective words) took 1.2s, 103107 effective words/s\n",
      "INFO - 01:40:40: EPOCH 87 - PROGRESS: at 73.91% examples, 90699 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:40: EPOCH - 87 : training on 282408 raw words (127522 effective words) took 1.5s, 85075 effective words/s\n",
      "INFO - 01:40:41: EPOCH 88 - PROGRESS: at 52.74% examples, 63762 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:42: EPOCH - 88 : training on 282408 raw words (127751 effective words) took 2.0s, 64206 effective words/s\n",
      "INFO - 01:40:43: EPOCH 89 - PROGRESS: at 81.02% examples, 101874 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:43: EPOCH - 89 : training on 282408 raw words (128265 effective words) took 1.2s, 103789 effective words/s\n",
      "INFO - 01:40:44: EPOCH 90 - PROGRESS: at 70.19% examples, 88737 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:45: EPOCH - 90 : training on 282408 raw words (127920 effective words) took 1.6s, 82176 effective words/s\n",
      "INFO - 01:40:46: EPOCH 91 - PROGRESS: at 56.34% examples, 71215 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:47: EPOCH 91 - PROGRESS: at 95.27% examples, 60196 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:47: EPOCH - 91 : training on 282408 raw words (127767 effective words) took 2.1s, 59702 effective words/s\n",
      "INFO - 01:40:48: EPOCH 92 - PROGRESS: at 42.46% examples, 52040 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:49: EPOCH - 92 : training on 282408 raw words (128014 effective words) took 1.9s, 65709 effective words/s\n",
      "INFO - 01:40:50: EPOCH 93 - PROGRESS: at 66.72% examples, 80917 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:51: EPOCH - 93 : training on 282408 raw words (127769 effective words) took 1.5s, 87210 effective words/s\n",
      "INFO - 01:40:52: EPOCH 94 - PROGRESS: at 66.72% examples, 81629 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:52: EPOCH - 94 : training on 282408 raw words (127674 effective words) took 1.6s, 79207 effective words/s\n",
      "INFO - 01:40:53: EPOCH 95 - PROGRESS: at 73.91% examples, 91019 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:54: EPOCH - 95 : training on 282408 raw words (127823 effective words) took 1.4s, 91524 effective words/s\n",
      "INFO - 01:40:55: EPOCH 96 - PROGRESS: at 63.36% examples, 77374 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:40:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:56: EPOCH - 96 : training on 282408 raw words (128259 effective words) took 1.9s, 69018 effective words/s\n",
      "INFO - 01:40:57: EPOCH 97 - PROGRESS: at 59.79% examples, 74774 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:57: EPOCH - 97 : training on 282408 raw words (127765 effective words) took 1.6s, 78812 effective words/s\n",
      "INFO - 01:40:58: EPOCH 98 - PROGRESS: at 63.36% examples, 80358 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 01:40:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:40:59: EPOCH - 98 : training on 282408 raw words (127539 effective words) took 1.7s, 75431 effective words/s\n",
      "INFO - 01:41:00: EPOCH 99 - PROGRESS: at 63.36% examples, 78483 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:41:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:41:00: EPOCH - 99 : training on 282408 raw words (127681 effective words) took 1.5s, 82854 effective words/s\n",
      "INFO - 01:41:02: EPOCH 100 - PROGRESS: at 45.95% examples, 55156 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:41:03: EPOCH 100 - PROGRESS: at 95.27% examples, 56478 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 01:41:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:41:03: EPOCH - 100 : training on 282408 raw words (127785 effective words) took 2.3s, 55646 effective words/s\n",
      "INFO - 01:41:03: Word2Vec lifecycle event {'msg': 'training on 28240800 raw words (12780762 effective words) took 166.4s, 76785 effective words/s', 'datetime': '2023-01-16T01:41:03.301973', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12780762, 28240800)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.train(training_data, total_examples=w2v.corpus_count, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 01:41:03: Word2Vec lifecycle event {'fname_or_handle': './word2vec_15000.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-01-16T01:41:03.841735', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "INFO - 01:41:03: not storing attribute cum_table\n",
      "INFO - 01:41:03: saved ./word2vec_15000.model\n"
     ]
    }
   ],
   "source": [
    "w2v.save(\"./word2vec_15000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17660\\3544636113.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v.init_sims()\n"
     ]
    }
   ],
   "source": [
    "w2v.init_sims()\n",
    "# NOTE:according to documentation, fill_norms() is perferred for the\n",
    "# same purpose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploration (4 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lecture, word embeddings are suited for similarity and analogy tasks. Let's explore some of that with our dataset: \n",
    "\n",
    "We look for the most similar words to the famous coffee shop where most of the episodes took place, namely `central_perk` and also for the ones similar to the character `joey`. If you have followed the exercise correctly until now, you should see that words like `laying` are similar to `central_perk` and the other main characters are also considered similar to `joey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chandler', 0.82831871509552),\n",
       " ('monica', 0.7637197971343994),\n",
       " ('ross', 0.7634231448173523),\n",
       " ('rachel', 0.747829020023346),\n",
       " ('phoebe', 0.702581524848938),\n",
       " ('him', 0.691574215888977),\n",
       " ('she', 0.6334285736083984),\n",
       " ('her', 0.6287880539894104),\n",
       " ('hey', 0.6270869970321655),\n",
       " ('again', 0.6186127066612244)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('joey', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conan', 0.5807178020477295),\n",
       " ('sitting_couch', 0.5191601514816284),\n",
       " ('laying', 0.4634176790714264),\n",
       " ('hippity', 0.44837668538093567),\n",
       " ('mitzi', 0.43669331073760986),\n",
       " ('bonnie', 0.4285639226436615),\n",
       " ('village', 0.409267395734787),\n",
       " ('russ', 0.40759292244911194),\n",
       " ('recliner', 0.40472713112831116),\n",
       " ('cheryl', 0.4009304642677307)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('central_perk', topn=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the similarity of `mrs_green` to `rachel` (her mom) and `ross`  to `spaceship` (unrelated). The first one should have a high score whereas the second should have a low score. Finally look at the similarity of `smelly_cat` (a song from pheobe) and `song`, which should have a high value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39799932"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity(\"mrs_green\", \"rachel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21720359"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity(\"ross\", \"spaceship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4968858"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity(\"song\", \"smelly_cat\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask our model to give us the word that does not belong to a list of words. Let's see from the list of all 5 characters which one is the most dissimilar? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phoebe'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_names= [\"joey\", \"rachel\", \"phoebe\", \"monica\", \"chandler\"]\n",
    "w2v.wv.doesnt_match(character_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on analogies: Which word is to `rachel` as `man` is to `women`? (print the top 3); you should get `chandler` and `monica` among the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chandler', 0.6585094928741455),\n",
       " ('joey', 0.648540198802948),\n",
       " ('monica', 0.633843719959259)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive = [\"man\", \"rachel\"], negative = [\"women\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chandler', 1.1776899099349976),\n",
       " ('joey', 1.155627965927124),\n",
       " ('monica', 1.1257286071777344)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar_cosmul(positive = [\"man\", \"rachel\"], negative = [\"women\"], topn=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use t-SNE to look at the distribution of our embeddings in the vector space for the character `joey`. Follow the instructions and fill in the blank in the `tsneplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsneplot(model: Type[Word2Vec], word: str):\n",
    "    \"\"\" \n",
    "    Uses seaborn to plot the results from the t-SNE dimensionality reduction for the top 10 most similar and dissimiliar words. \n",
    "    \"\"\"\n",
    "    embs = np.empty((1,100), dtype=\"f\")    # to save all the embeddings\n",
    "    word_labels = [word]\n",
    "    color_list  = [\"green\"]\n",
    "    print(model.wv.get_vector(word, norm = True).shape)\n",
    "    embs = np.append(embs,np.array(model.wv.get_vector(word, norm = True)).reshape(1,100),axis = 0) # adds the vector of the query word\n",
    "    print(embs.shape)\n",
    "    close_words = model.wv.most_similar(word, topn = 10)# gets list of most similar words\n",
    "    all_sims = model.wv.similar_by_word(word)# gets list of most dissimilar words (get the sorted list of all the words and their similarity and choose the bottom 10)\n",
    "    far_words = all_sims[-10:]\n",
    "\n",
    "\n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.get_vector(wrd_score[0], norm = True)\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append(\"blue\")\n",
    "        embs = np.append(embs,np.array(wrd_vector).reshape(1,100), axis=0)\n",
    "        \n",
    "    # adds the vector for each of the furthest words to the array\n",
    "    for wrd_score in far_words:\n",
    "        wrd_vector = model.wv.get_vector(wrd_score[0], norm = True)\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append(\"red\")\n",
    "        embs = np.append(embs,np.array(wrd_vector).reshape(1,100), axis=0)\n",
    "    print(embs.shape)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = TSNE(learning_rate=200, \\\n",
    "        random_state=42, perplexity=15, init=\"random\").fit_transform(embs)\n",
    "    print(Y)\n",
    "    # NOTE: n_components = 2 is default value\n",
    "    # with  n_components=2, learning_rate=200, random_state=42, perplexity=15, init=\"random\"\n",
    "    # sets everything up to plot\n",
    "    df = pd.DataFrame({\"x\": [x[0] for x in Y],\n",
    "                       \"y\": [y[1] for y in Y],\n",
    "                       \"words\": word_labels,\n",
    "                       \"color\": color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    \n",
    "    # basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={\"s\": 40, \"facecolors\": df[\"color\"]}\n",
    "                    )\n",
    "    \n",
    "    # adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df[\"y\"][line],\n",
    "                 \"  \" + df[\"words\"][line].title(),\n",
    "                 horizontalalignment=\"left\",\n",
    "                 verticalalignment=\"bottom\", size=\"medium\",\n",
    "                 color=df[\"color\"][line],\n",
    "                 weight=\"normal\"\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title(\"t-SNE visualization for {}\".format(word.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(2, 100)\n",
      "(22, 100)\n",
      "[[-119.963745 -299.9579  ]\n",
      " [ 339.58957    46.281036]\n",
      " [ 239.23302   203.67735 ]\n",
      " [-198.94586   -52.608574]\n",
      " [  32.189373   75.182465]\n",
      " [  93.60478   -67.18788 ]\n",
      " [ -93.732796  245.17892 ]\n",
      " [ 315.54282  -291.37247 ]\n",
      " [-109.842606  460.22244 ]\n",
      " [ 129.35933  -393.63495 ]\n",
      " [-458.94324    -8.537677]\n",
      " [-182.95161  -422.1633  ]\n",
      " [ 199.76479    85.21013 ]\n",
      " [-126.93411    44.865524]\n",
      " [  64.48842   188.38046 ]\n",
      " [ -24.155415  -91.220215]\n",
      " [-207.72432   194.92973 ]\n",
      " [ 291.00888  -172.824   ]\n",
      " [-228.33252   426.60193 ]\n",
      " [ 107.72941  -276.16956 ]\n",
      " [-454.38156   113.69144 ]\n",
      " [-260.69345  -298.7806  ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17660\\2179808942.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtsneplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"joey\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17660\\4085680059.py\u001b[0m in \u001b[0;36mtsneplot\u001b[1;34m(model, word)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# with  n_components=2, learning_rate=200, random_state=42, perplexity=15, init=\"random\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# sets everything up to plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     df = pd.DataFrame({\"x\": [x[0] for x in Y],\n\u001b[0m\u001b[0;32m     38\u001b[0m                        \u001b[1;34m\"y\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                        \u001b[1;34m\"words\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mword_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All arrays must be of the same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "tsneplot(w2v, \"joey\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Multi-class Classification (1 + 3 + 2 = 6 points)\n",
    "In this task, we aim to classify consumer finance complaints into 12 pre-defined classes. Note that this is not a multi-label task, and we assume that each new complaint is assigned to one and only one category. The data comes from https://www.data.gov/ (US government’s open data) and contains complaints that are published after the company responds, confirming a commercial relationship with the consumer, or after 15 days, whatever comes first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 1: Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data to a pandas dataframe from `complaints.csv` (also located in the `data` folder); this is a rather large file of 206MB. Keep only the `Consumer complaint narrative` (input text) and `product` (labels). Remove the missing values, rename `Consumer complaint narrative` to `Narrative` for ease of use, and add a column encoding the product as an integer. This will represent your labels for classification and the mapping will be used later on. Create two dictionaries: one mapping the ids to products and one mapping products to their ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df = pd.read_csv(\"data/complaints.csv\")\n",
    "df =  df[[\"Consumer complaint narrative\",\"product\"]]\n",
    "###keep the columns you need ####\n",
    "df.columns =  ###rename columns ####\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ### remove missing #### \n",
    "df['product_id'] = ### integer ids for product ### \n",
    "### create the dictionary #### \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the class distribution of the products. It is always a good idea to look at the relative number of instances for each class before performing any classification task. Use the `plot` function from the dataframe to show the number of instances in each class in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "df ### your code ###\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done the exercise correctly you should observe a class imbalance with `credit reporting` having the most complaints. This can result in some difficulties for standard algorithms, making them biased towards the majority class and treating the minority classes as outliers and unimportant. One way to overcome this problem is by using **undersampling** or **oversampling**. However, this is beyond the scope of this assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: Text Representation and Training the Classifier \n",
    "Before performing any classification we need to split our data into train and test sets. Use `sklearn` to save 20 percent of the data for the test and the rest for training. Make sure to input the index of the data frame to retrieve the indices of the test and train. To ensure reproducibility, use `random_state=42`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = ### your code ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform any sort of classification task, we first need to convert our raw text into some vector representation. Let's use the `TfidfVectorizer` from `sklearn` to convert the `narrative` column into TF-IDF vectors. When transforming the text keep the following in mind:\n",
    "- use the logarithmic form for frequency\n",
    "- remove accents (ASCII) \n",
    "- lowercase all characters \n",
    "- remove `English` stop words \n",
    "- ignore terms that have a document frequency strictly less than 10\n",
    "- smooth IDF weights by adding one to document frequencies \n",
    "- output row should have unit L2 norm\n",
    "- set the encoding to `Latin-1`\n",
    "- extract both uni-grams and bi-grams \n",
    "- build a vocabulary that only considers the top 10.000 features\n",
    "Keep in mind that the vectorizer should be trained **only** on the training data, and the test data should be transformed using the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(###initlize the model ### \n",
    "X_train = ###transform text  ### \n",
    "X_train.shape # should be (132647, 10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data transformation, we attain the features and labels, to train the classifier. In our case, we use **Naive Bayes Classifier**. \n",
    "- use `MultinomialNB` from sklearn to classify the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = ### fit the model to the data ### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Model Evaluation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our model using the held-out test data. We are going to look at the confusion matrix to show the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "y_pred = ### predict on the test set ### \n",
    "conf = ### create the confusion matrix ### \n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(conf, annot=True, fmt='d',\n",
    "            xticklabels=###products names from the dictionary ###,\n",
    "            yticklabels=### products names from the dictionary###)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the predictions end up on the diagonal (predicted label = actual label). The diagonal shows the correct classified classes. However, there are several misclassifications, specifically `Checking or savings account` is often confused with `Bank account or service`. Let's take a look at why this happens. For this, we look at 5 misclassified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = 'Bank account or service'\n",
    "predicted = 'Checking or savings account'\n",
    "### print only the top 5 \n",
    "df###choose the ones that have an actual label of Bank account or service and the predicted label of Checking or savings account ###)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of the misclassified complaints are complaints that are not easy to distinguish. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Auto-Complete (2 + 5 + 4 = 11 points)\n",
    "Let's get even more practical! In this problem set, you will build your own auto-completion system that you see every day while using search engines.\n",
    "\n",
    "[google]: https://www.thedad.com/wp-content/uploads/2018/05/screen-shot-2018-05-12-at-2-01-56-pm.png \"google auto complete\"\n",
    "\n",
    "![google]\n",
    "\n",
    "By the end of this assignment, you will develop a simple prototype of such a system using n-gram language models. At the heart of the system is a language model that assigns the probability to a sequence of words. We take advantage of this probability calculation to predict the next word. \n",
    "\n",
    "The problem set contains 3 main parts:\n",
    "\n",
    "1. Load and preprocess data (tokenize and split into train and test)\n",
    "2. Develop n-gram based language models by estimating the conditional probability of the next word.\n",
    "3. Evaluate the model by computing the perplexity score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 1: Load and Preprocess Data \n",
    "We use a subset of English tweets to train our model. Run the cell below to load the data and observe a few lines of it. Notice that tweets are saved in a text file, where tweets are separated by `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "with open(\"data/twitter.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "print(\"First 500 characters of the data:\")\n",
    "display(data[0:500])\n",
    "print(\"-------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to separate the tweets and split them into train and test set. Apply the following pre-processing steps:\n",
    "\n",
    "1. Split data into sentences using \"\\n\" as the delimiter and remove the leading and trailing spaces (drop empty sentences)\n",
    "2. Tokenize the sentences into words using SpaCy and lowercase them. (notice that we do not remove stop words or punctuations.) \n",
    "3. Divide the sentences into 80 percent training and 20 percent test set. No validation set is required, although in a real-world application it is best to set aside part of the data for hyperparameter tuning.\n",
    "4. To limit the vocabulary and remove potential spelling mistakes, make a vocabulary of the words that appear at least 2 times. The rest of the words will be replaced by the `<unk>` symbol. This is a crucial step since if your model encounters a word that it never saw during training, it won't have an input word that helps determining the next word for suggestion. We use the `<unk>` word for **out of Vocabulary (OOV)** words. Keep in mind that we built the vocabulary on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = #split\n",
    "sentences = #remove spaces and drop empty sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [] # list of list of the tokens in a sentence \n",
    "##Your Code###   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "Random(4).shuffle(tokenized_corpus)\n",
    "\n",
    "train = ##Your Code###\n",
    "test = ##Your Code###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "flatten_corpus = ### Flatten the train corpus ### \n",
    "word_counts = ### count the number of each token ### \n",
    "vocab = []\n",
    "\n",
    "### keep only the ones with frequency bigger than 2 ### \n",
    "print(len(vocab)) ### should be 14930 ### \n",
    "train_replaced = []\n",
    "test_replaced = []\n",
    "for sentence in train:\n",
    "    ### adjust the sentence to contain the word in the vocabulary and <unk> for the rest #### \n",
    "for sentence in test:\n",
    "    ### adjust the sentence to contain the word in the vocabulary and <unk> for the rest #### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: N-gram Based Language Model: \n",
    "In this section, you will develop the n-grams language model. We assume that the probability of the next word depends only on the previous n-gram or previous n words. We compute this probability by counting the occurrences in the corpus.\n",
    "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ can be estimated as follows:\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})}  $$\n",
    "\n",
    "The numerator is the number of times word 't' appears after the n-gram, and the denominator is the number of times the n-gram occurs in the corpus, where $C(\\cdots)$ is a count function. Later, we add k-smoothing to avoid errors when any counts are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the problem of probability estimation we divide the problem into 3 parts. In the following you will: \n",
    "1. Implement a function that computes the counts of n-grams for an arbitrary number n.\n",
    "2. Estimate the probability of a word given the prior n-words using the n-gram counts.\n",
    "3. Calculate probabilities for all possible words.\n",
    "The steps are detailed below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by implementing a function that computes the counts of n-grams for an arbitrary number n.\n",
    "- Prepend necessary starting markers `<s>` to indicate the beginning of the sentence. In the case of a bi-gram model, you need to prepend two start tokens `<s><s>` to be able to predict the first word. \"hello world\"-> \"`<s><s>`hello world\".\n",
    "- Append an end token `<e>` so that the model can predict when to finish a sentence.\n",
    "- Create a dictionary to store all the n-gram counts (called n_gram in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def n_grams_counts(corpus, n):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the corpus given the parameter n \n",
    "    \n",
    "    data: List of lists of words (your tokenized corpus)\n",
    "    n: n in the n-gram\n",
    "    \n",
    "    Returns: A dictionary that maps a tuple of n words to its frequency\n",
    "    \"\"\"\n",
    "    start_token='<s>'\n",
    "    end_token = '<e>'\n",
    "    n_grams = defaultdict(int)\n",
    "    for sentence in corpus: \n",
    "        sentence = ### add start and end token ###\n",
    "        # convert list to tuple so it can be used a the key in the dictionary \n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        ###iterate over the n-grams in the sentence, you can use the range() function, and increament the counts in the\n",
    "        ## n_grams dictionary, where the key is the n_gram and the value is count \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to estimate the probability of a word given the prior n words using the n-gram counts, based on the formula given at the beginning of this task. To deal with the problem of zero division we add k-smoothing. K-smoothing adds a positive constant $k$ to each numerator and $k \\times |vocabulary size|$ in the denominator. Below we will define a function that takes in a dictionary `n_gram_cnt`, where the key is the n-gram, and the value is the count of that n-gram, plus a dictionary for `plus_current_gram_cnt`, which you'll use to find the count for the previous n-gram plus the current word. Notice that these dictionaries are computed using the previous function `n_grams_counts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(word, prev_n_gram, \n",
    "                         n_gram_cnts, n_plus1_gram_cnts, vocab_size):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    word: next word\n",
    "    prev_n_gram: previous n gram\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab_size: number of words in the vocabulary\n",
    "    \n",
    "    Returns: A probability\n",
    "    \"\"\"\n",
    "    k=1.0\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "    \n",
    "    prev_n_gram_cnt =  # get the previous n-gram count from the dictionary \n",
    "    denominator = # denominator with the previous n-gram count and k-smoothing\n",
    "    n_plus1_gram =  # add the current word to the n-gram \n",
    "    n_plus1_gram_cnt =  # get the current n-gram count using the dictionary\n",
    "    numerator = #calculate the numerator with k-smoothing\n",
    "    prob =\n",
    "    \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the functions we have defined to calculate probabilities for all possible words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities(prev_n_gram, n_gram_cnts, plus_current_gram_cnts, vocab):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities for all the words in the vocabulary given the previous n-gram \n",
    "    prev_n_gram: previous n-gram\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cntsplus_current_gram_cnt: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab: List of words\n",
    "    \n",
    "    Returns: A dictionary mapping from next words to the probability.\n",
    "    \"\"\"\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "    \n",
    "    vocab =  # add <e> <unk> to the vocabulary\n",
    "    vocabulary_size = #compute the size \n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocab:\n",
    "        ### compute the probability \n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Predict the probability of the all possible words after the unigram \"the\"\n",
    "sentences = [['the', 'moon', 'and', 'stars', 'are','shining','bright'],\n",
    "             ['the', 'moon', 'is', 'shinnig','tonight'],\n",
    "             ['mars','and' ,'moon', 'are', 'plants'],\n",
    "             ['the' ,'moon', 'is','a', 'plant']]\n",
    "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]+ sentences[3]))\n",
    "unigram_counts = n_grams_counts(sentences, 1)\n",
    "bigram_counts = n_grams_counts(sentences, 2)\n",
    "print(\"The word 'moon' should have the highest probability, if it is not the case, re-visit your previous functions.\")\n",
    "probabilities([\"the\"], unigram_counts, bigram_counts, unique_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Evaluation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we use the perplexity score to evaluate your model on the test set.\n",
    "The perplexity score of the test set on an n-gram model is defined as follows: \n",
    "\n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } $$\n",
    "- where $N$ is the length of the sentence. ($N-1$ is used because in the code we start from the index 0).\n",
    "- $n$ is the number of words in the n-gram.\n",
    "\n",
    "Notice that we have already computed this probability. \n",
    "\n",
    "The higher the probabilities are, the lower the perplexity will be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(sentence, n_gram_cnts, plus_current_gram_cnts, vocab_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    sentence: List of strings\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab_size: number of unique words in the vocabulary\n",
    "    k: positive smoothing constant\n",
    "    \n",
    "    Returns: Perplexity score for a single sentence \n",
    "    \"\"\"\n",
    "    \n",
    "    n =  # get the number 'n' in  n-gram  from n_gram_cnts  \n",
    "    \n",
    "    sentence =  # prepend <s> and append <e>\n",
    "    sentence = tuple(sentence)\n",
    "    N =# length of sentence \n",
    "    \n",
    "   \n",
    "    product_pi = 1.0 \n",
    "    \n",
    "    ### Compute the product of probabilites ###\n",
    "    \n",
    "    for t in range(n, N): \n",
    "        n_gram =# get the n-gram before the predicted word (n-gram before t )\n",
    "        word =  # get the word to be predicted (position t)\n",
    "        prob = probability(\n",
    "        product_pi *= # Update the product of the probabilities\n",
    "    \n",
    "    perplexity = product_pi**(1/float(N)) # Take the Nth root of the product\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to find the perplexity of a bi-gram model on the first instance of training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = ### your code ###\n",
    "trigram_counts = ### your code ###\n",
    "\n",
    "perplexity_train = perplexity(train_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
    "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
    "\n",
    "perplexity_test = perplexity(test_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")# the preprexity for the train sample should be much lower\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use the model we created to generate an auto-complete system that makes suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(up_to_here, n_gram_cnts, plus_current_gram_cnts, vocab , start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    up_to_here: the sentence so far, must have length > n \n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab: List of words\n",
    "    start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns: (most likely next word,  probability) \n",
    "    \"\"\"    \n",
    "    n = len(list(n_gram_cnts.keys())[0]) # get the number 'n' in  n-gram  from n_gram_cnts     \n",
    "    previous_n_gram = previous_tokens[-n:] # get the last 'n' words as the previous n-gram from the input sentence\n",
    "\n",
    "    \n",
    "    probabs = # Estimate the probabilities for each word in the vocabulary\n",
    "    \n",
    "    probabs = \n",
    "    ### sort the probability for higher to lower and return the highest probability word,probability tuple\n",
    "    #if start_with is specified then return the highest probability word that starts with that specific character \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model based on the bi-gram model created on the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = ['i', 'like']\n",
    "start_with = 'g'\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,\n",
    "                            trigram_counts, list(vocab), start_with=start_with)\n",
    "suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = ['i', 'like', 'to']\n",
    "start_with = None\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,\n",
    "                            trigram_counts, list(vocab), start_with=start_with)\n",
    "suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hello\", \"my\", \"name\", \"is\"]\n",
    "start_with = None\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,\n",
    "                            trigram_counts, list(vocab), start_with=start_with)\n",
    "suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hello\", \"my\", \"name\", \"is\"]\n",
    "start_with = 'sa'\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,\n",
    "                            trigram_counts, list(vocab), start_with=start_with)\n",
    "suggestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying learning has the advantage that at the beginning of training where the model weights are usually\n",
    "chosen at random, the algorithm can fastly converge towards a minimum. After more and more steps, it can be\n",
    "assumed that the solution is near such a minimum, and only smaller steps are taken, such that the algorithm can \n",
    "now \"fine-tune\" model parameters, instead of altering weights massively, potentially stepping over the optimal\n",
    "the solution again and again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Model Evaluation & Comparison (1 + 2 + 2 + 2 = 7 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we want to evaluate and compare the performance of three different imaginary spam mail classifiers. The file `spam_ham_dataset_predictions.csv` consists of a dataset of e-mails with the labels `ham (0)` and `spam (1)` which was taken over from [Kaggle](https://www.kaggle.com/code/syamkakarla/spam-mail-classifier/data). Additionally, predictions from three different models (A, B and C) were added to the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 1: Class Distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data and looking at it first. Since we want to evaluate the performance of the given classifiers, one of the important aspects to know is how the classes are distributed within the dataset. Therefore, we extract the true distribution of classes from the gold labels as well as the predicted distributions of classes from the predicted labels of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "      <th>prediction_model_A</th>\n",
       "      <th>prediction_model_B</th>\n",
       "      <th>prediction_model_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: tenaska iv 12 / 00 , 2 / 01 and 3 / 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: revised : eastrans nomination change ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon lesson # 5\\r\\nhere is the lesson...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: cornhusker\\r\\ni have entered deals in...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: prior month misnomination\\r\\ndaren - ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: 23 rd noms\\r\\n- - - - - - - - - - - -...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: tenaska iv outage update\\r\\nfyi .\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: me again\\r\\ndid you survive bid week ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: 6 / 4 / 99 and 6 / 9 / 99 ( 98 - 0439...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: 8 th noms\\r\\n- - - - - - - - - - - - ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3912 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  label_num  \\\n",
       "3063   ham  Subject: tenaska iv 12 / 00 , 2 / 01 and 3 / 0...          0   \n",
       "1661   ham  Subject: revised : eastrans nomination change ...          0   \n",
       "2816   ham  Subject: neon lesson # 5\\r\\nhere is the lesson...          0   \n",
       "1507   ham  Subject: cornhusker\\r\\ni have entered deals in...          0   \n",
       "955    ham  Subject: prior month misnomination\\r\\ndaren - ...          0   \n",
       "...    ...                                                ...        ...   \n",
       "503    ham  Subject: 23 rd noms\\r\\n- - - - - - - - - - - -...          0   \n",
       "1764   ham  Subject: tenaska iv outage update\\r\\nfyi .\\r\\n...          0   \n",
       "2941   ham  Subject: me again\\r\\ndid you survive bid week ...          0   \n",
       "1958   ham  Subject: 6 / 4 / 99 and 6 / 9 / 99 ( 98 - 0439...          0   \n",
       "269    ham  Subject: 8 th noms\\r\\n- - - - - - - - - - - - ...          0   \n",
       "\n",
       "      prediction_model_A  prediction_model_B  prediction_model_C  \n",
       "3063                   1                   1                   0  \n",
       "1661                   1                   1                   0  \n",
       "2816                   1                   1                   0  \n",
       "1507                   1                   1                   0  \n",
       "955                    1                   1                   0  \n",
       "...                  ...                 ...                 ...  \n",
       "503                    0                   0                   0  \n",
       "1764                   0                   0                   0  \n",
       "2941                   0                   0                   0  \n",
       "1958                   0                   0                   0  \n",
       "269                    0                   0                   0  \n",
       "\n",
       "[3912 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/spam_ham_dataset_predictions.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Labels</th>\n",
       "      <th>Model A</th>\n",
       "      <th>Model B</th>\n",
       "      <th>Model C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3912 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Gold Labels  Model A  Model B  Model C\n",
       "3063            0        1        1        0\n",
       "1661            0        1        1        0\n",
       "2816            0        1        1        0\n",
       "1507            0        1        1        0\n",
       "955             0        1        1        0\n",
       "...           ...      ...      ...      ...\n",
       "503             0        0        0        0\n",
       "1764            0        0        0        0\n",
       "2941            0        0        0        0\n",
       "1958            0        0        0        0\n",
       "269             0        0        0        0\n",
       "\n",
       "[3912 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distribution = df[[\"label_num\", \"prediction_model_A\", \"prediction_model_B\", \"prediction_model_C\"]]### your code ###\n",
    "\n",
    "# renaming the rows for a nicer table and plot\n",
    "class_distribution = class_distribution.rename(columns={\n",
    "    \"label_num\": \"Gold Labels\",\n",
    "    \"prediction_model_A\": \"Model A\",\n",
    "    \"prediction_model_B\": \"Model B\",\n",
    "    \"prediction_model_C\": \"Model C\"\n",
    "})\n",
    "\n",
    "# display the class distribution table\n",
    "class_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gold Labels', 0), ('Gold Labels', 1), ('Model A', 0), ('Model A', 1), ('Model B', 0), ('Model B', 1), ('Model C', 0)]\n",
      "Gold Labels  0    3672.0\n",
      "             1     240.0\n",
      "Model A      0    3681.0\n",
      "             1     231.0\n",
      "Model B      0    3809.0\n",
      "             1     103.0\n",
      "Model C      0    3912.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAALzCAYAAAArnSfZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLLklEQVR4nO3dfZjVBZ03/vfwNDwEo4A8TI6mt+hiqOuiAm67aioPibTp3rZS/LQlrCxdQrLU3Y2eoLVb0YVyXTKfwKhtxbrSRnFT0gBFtkklMytdIRkwHQYhGgzP74/727kbeZBR8XD09bquc12c7/czh8/pHcrbc8731JRKpVIAAABIp0ovAAAAsLdQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAIBCl0ovsKe89NJLeeaZZ9K7d+/U1NRUeh0AAKBCSqVSXnjhhdTX16dTp12/RvSmLUjPPPNMGhoaKr0GAACwl1i9enX233//Xc68aQtS7969k/zf/xH69OlT4W0AAIBK2bhxYxoaGsodYVfetAXpj2+r69Onj4IEAADs1kdvXKQBAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACg8JoK0qxZs1JTU5OpU6eWj5VKpcyYMSP19fXp0aNHTjzxxKxatardz7W1teWCCy5I//7906tXr0yYMCFr1qxpN9PS0pJJkyalrq4udXV1mTRpUjZs2PBa1gUAANilV12QVqxYkX//93/PkUce2e745ZdfniuvvDJz587NihUrMmjQoJx66ql54YUXyjNTp07NokWLsnDhwtx///3ZtGlTxo8fn23btpVnJk6cmKampjQ2NqaxsTFNTU2ZNGnSq10XAADgFb2qgrRp06Z84AMfyLx587LvvvuWj5dKpVx11VW57LLLcsYZZ2TYsGG58cYb87vf/S633HJLkqS1tTXXXXddrrjiipxyyik5+uijM3/+/DzyyCO5++67kySPPfZYGhsb8/Wvfz2jRo3KqFGjMm/evHz/+9/P448//jo8bQAAgO29qoL08Y9/PKeddlpOOeWUdseffPLJNDc3Z/To0eVjtbW1OeGEE7J06dIkycqVK/Piiy+2m6mvr8+wYcPKM8uWLUtdXV1GjBhRnhk5cmTq6urKMy/X1taWjRs3trsBAAB0RJeO/sDChQuzcuXKPPTQQ9uda25uTpIMHDiw3fGBAwfmf/7nf8oz3bp1a/fK0x9n/vjzzc3NGTBgwHaPP2DAgPLMy82aNSuf+9znOvp0AAAAyjr0CtLq1avzD//wD1mwYEG6d+++07mampp290ul0nbHXu7lMzua39XjXHLJJWltbS3fVq9evcvfDwAA4OU6VJBWrlyZ9evXZ/jw4enSpUu6dOmSJUuW5F//9V/TpUuX8itHL3+VZ/369eVzgwYNytatW9PS0rLLmXXr1m33+z/77LPbvTr1R7W1tenTp0+7GwAAQEd0qCCdfPLJeeSRR9LU1FS+HXPMMfnABz6QpqamHHzwwRk0aFAWL15c/pmtW7dmyZIlOf7445Mkw4cPT9euXdvNrF27No8++mh5ZtSoUWltbc2DDz5YnnnggQfS2tpangEAAHi9degzSL17986wYcPaHevVq1f69etXPj516tTMnDkzQ4YMyZAhQzJz5sz07NkzEydOTJLU1dVl8uTJueiii9KvX7/07ds306dPzxFHHFG+6MPQoUMzduzYTJkyJddee22S5Lzzzsv48eNz2GGHveYnDQAAsCMdvkjDK7n44ouzZcuWnH/++WlpacmIESNy1113pXfv3uWZ2bNnp0uXLjnrrLOyZcuWnHzyybnhhhvSuXPn8syCBQty4YUXlq92N2HChMydO/f1XhcAAKCsplQqlSq9xJ6wcePG1NXVpbW11eeRAADgLawj3eBVfQ8SAADAm5GCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAACF1/17kAAAgNfHOz5ze6VX2KOe+vJplV5hO15BAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFDoUukFAIDq8Y7P3F7pFfaYp758WqVXAPYCXkECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKDQpdILALxa7/jM7ZVeYY966sunVXoFAHjLUZBeR2/mv6z5ixoAAG8F3mIHAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAIUOFaRrrrkmRx55ZPr06ZM+ffpk1KhR+cEPflA+f+6556ampqbdbeTIke0eo62tLRdccEH69++fXr16ZcKECVmzZk27mZaWlkyaNCl1dXWpq6vLpEmTsmHDhlf/LAEAAHZDhwrS/vvvny9/+ct56KGH8tBDD+Xd73533vve92bVqlXlmbFjx2bt2rXl2x133NHuMaZOnZpFixZl4cKFuf/++7Np06aMHz8+27ZtK89MnDgxTU1NaWxsTGNjY5qamjJp0qTX+FQBAAB2rUtHhk8//fR297/0pS/lmmuuyfLly/POd74zSVJbW5tBgwbt8OdbW1tz3XXX5eabb84pp5ySJJk/f34aGhpy9913Z8yYMXnsscfS2NiY5cuXZ8SIEUmSefPmZdSoUXn88cdz2GGHdfhJAgAA7I5X/Rmkbdu2ZeHChdm8eXNGjRpVPn7vvfdmwIABOfTQQzNlypSsX7++fG7lypV58cUXM3r06PKx+vr6DBs2LEuXLk2SLFu2LHV1deVylCQjR45MXV1deWZH2trasnHjxnY3AACAjuhwQXrkkUfytre9LbW1tfnoRz+aRYsW5fDDD0+SjBs3LgsWLMgPf/jDXHHFFVmxYkXe/e53p62tLUnS3Nycbt26Zd999233mAMHDkxzc3N5ZsCAAdv9vgMGDCjP7MisWbPKn1mqq6tLQ0NDR58aAADwFteht9glyWGHHZampqZs2LAh//mf/5lzzjknS5YsyeGHH573v//95blhw4blmGOOyYEHHpjbb789Z5xxxk4fs1Qqpaampnz/T3+9s5mXu+SSSzJt2rTy/Y0bNypJAABAh3S4IHXr1i2HHHJIkuSYY47JihUrcvXVV+faa6/dbnbw4ME58MAD88QTTyRJBg0alK1bt6alpaXdq0jr16/P8ccfX55Zt27ddo/17LPPZuDAgTvdq7a2NrW1tR19OgAAAGWv+XuQSqVS+S10L/fcc89l9erVGTx4cJJk+PDh6dq1axYvXlyeWbt2bR599NFyQRo1alRaW1vz4IMPlmceeOCBtLa2lmcAAAD2hA69gnTppZdm3LhxaWhoyAsvvJCFCxfm3nvvTWNjYzZt2pQZM2bkzDPPzODBg/PUU0/l0ksvTf/+/fO+970vSVJXV5fJkyfnoosuSr9+/dK3b99Mnz49RxxxRPmqdkOHDs3YsWMzZcqU8qtS5513XsaPH+8KdgAAwB7VoYK0bt26TJo0KWvXrk1dXV2OPPLINDY25tRTT82WLVvyyCOP5KabbsqGDRsyePDgnHTSSfnWt76V3r17lx9j9uzZ6dKlS84666xs2bIlJ598cm644YZ07ty5PLNgwYJceOGF5avdTZgwIXPnzn2dnjIAAMCOdaggXXfddTs916NHj9x5552v+Bjdu3fPnDlzMmfOnJ3O9O3bN/Pnz+/IagAAAK/Za/4MEgAAwJuFggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUOlSQrrnmmhx55JHp06dP+vTpk1GjRuUHP/hB+XypVMqMGTNSX1+fHj165MQTT8yqVavaPUZbW1suuOCC9O/fP7169cqECROyZs2adjMtLS2ZNGlS6urqUldXl0mTJmXDhg2v/lkCAADshg4VpP333z9f/vKX89BDD+Whhx7Ku9/97rz3ve8tl6DLL788V155ZebOnZsVK1Zk0KBBOfXUU/PCCy+UH2Pq1KlZtGhRFi5cmPvvvz+bNm3K+PHjs23btvLMxIkT09TUlMbGxjQ2NqapqSmTJk16nZ4yAADAjnXpyPDpp5/e7v6XvvSlXHPNNVm+fHkOP/zwXHXVVbnssstyxhlnJEluvPHGDBw4MLfccks+8pGPpLW1Ndddd11uvvnmnHLKKUmS+fPnp6GhIXfffXfGjBmTxx57LI2NjVm+fHlGjBiRJJk3b15GjRqVxx9/PIcddtjr8bwBAAC286o/g7Rt27YsXLgwmzdvzqhRo/Lkk0+mubk5o0ePLs/U1tbmhBNOyNKlS5MkK1euzIsvvthupr6+PsOGDSvPLFu2LHV1deVylCQjR45MXV1deWZH2trasnHjxnY3AACAjuhwQXrkkUfytre9LbW1tfnoRz+aRYsW5fDDD09zc3OSZODAge3mBw4cWD7X3Nycbt26Zd99993lzIABA7b7fQcMGFCe2ZFZs2aVP7NUV1eXhoaGjj41AADgLa7DBemwww5LU1NTli9fno997GM555xz8rOf/ax8vqampt18qVTa7tjLvXxmR/Ov9DiXXHJJWltby7fVq1fv7lMCAABI8ioKUrdu3XLIIYfkmGOOyaxZs3LUUUfl6quvzqBBg5Jku1d51q9fX35VadCgQdm6dWtaWlp2ObNu3brtft9nn312u1en/lRtbW356np/vAEAAHTEa/4epFKplLa2thx00EEZNGhQFi9eXD63devWLFmyJMcff3ySZPjw4enatWu7mbVr1+bRRx8tz4waNSqtra158MEHyzMPPPBAWltbyzMAAAB7QoeuYnfppZdm3LhxaWhoyAsvvJCFCxfm3nvvTWNjY2pqajJ16tTMnDkzQ4YMyZAhQzJz5sz07NkzEydOTJLU1dVl8uTJueiii9KvX7/07ds306dPzxFHHFG+qt3QoUMzduzYTJkyJddee22S5Lzzzsv48eNdwQ4AANijOlSQ1q1bl0mTJmXt2rWpq6vLkUcemcbGxpx66qlJkosvvjhbtmzJ+eefn5aWlowYMSJ33XVXevfuXX6M2bNnp0uXLjnrrLOyZcuWnHzyybnhhhvSuXPn8syCBQty4YUXlq92N2HChMydO/f1eL4AAAA71aGCdN111+3yfE1NTWbMmJEZM2bsdKZ79+6ZM2dO5syZs9OZvn37Zv78+R1ZDQAA4DV7zZ9BAgAAeLNQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAIBChwrSrFmzcuyxx6Z3794ZMGBA/uZv/iaPP/54u5lzzz03NTU17W4jR45sN9PW1pYLLrgg/fv3T69evTJhwoSsWbOm3UxLS0smTZqUurq61NXVZdKkSdmwYcOre5YAAAC7oUMFacmSJfn4xz+e5cuXZ/HixfnDH/6Q0aNHZ/Pmze3mxo4dm7Vr15Zvd9xxR7vzU6dOzaJFi7Jw4cLcf//92bRpU8aPH59t27aVZyZOnJimpqY0NjamsbExTU1NmTRp0mt4qgAAALvWpSPDjY2N7e5ff/31GTBgQFauXJm//uu/Lh+vra3NoEGDdvgYra2tue6663LzzTfnlFNOSZLMnz8/DQ0NufvuuzNmzJg89thjaWxszPLlyzNixIgkybx58zJq1Kg8/vjjOeywwzr0JAEAAHbHa/oMUmtra5Kkb9++7Y7fe++9GTBgQA499NBMmTIl69evL59buXJlXnzxxYwePbp8rL6+PsOGDcvSpUuTJMuWLUtdXV25HCXJyJEjU1dXV555uba2tmzcuLHdDQAAoCNedUEqlUqZNm1a3vWud2XYsGHl4+PGjcuCBQvywx/+MFdccUVWrFiRd7/73Wlra0uSNDc3p1u3btl3333bPd7AgQPT3NxcnhkwYMB2v+eAAQPKMy83a9as8ueV6urq0tDQ8GqfGgAA8BbVobfY/alPfOITefjhh3P//fe3O/7+97+//Othw4blmGOOyYEHHpjbb789Z5xxxk4fr1Qqpaampnz/T3+9s5k/dckll2TatGnl+xs3blSSAACADnlVryBdcMEF+d73vpd77rkn+++//y5nBw8enAMPPDBPPPFEkmTQoEHZunVrWlpa2s2tX78+AwcOLM+sW7duu8d69tlnyzMvV1tbmz59+rS7AQAAdESHClKpVMonPvGJ3HrrrfnhD3+Ygw466BV/5rnnnsvq1aszePDgJMnw4cPTtWvXLF68uDyzdu3aPProozn++OOTJKNGjUpra2sefPDB8swDDzyQ1tbW8gwAAMDrrUNvsfv4xz+eW265Jd/97nfTu3fv8ueB6urq0qNHj2zatCkzZszImWeemcGDB+epp57KpZdemv79++d973tfeXby5Mm56KKL0q9fv/Tt2zfTp0/PEUccUb6q3dChQzN27NhMmTIl1157bZLkvPPOy/jx413BDgAA2GM6VJCuueaaJMmJJ57Y7vj111+fc889N507d84jjzySm266KRs2bMjgwYNz0kkn5Vvf+lZ69+5dnp89e3a6dOmSs846K1u2bMnJJ5+cG264IZ07dy7PLFiwIBdeeGH5ancTJkzI3LlzX+3zBAAAeEUdKkilUmmX53v06JE777zzFR+ne/fumTNnTubMmbPTmb59+2b+/PkdWQ8AAOA1eU3fgwQAAPBmoiABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAAhQ4VpFmzZuXYY49N7969M2DAgPzN3/xNHn/88XYzpVIpM2bMSH19fXr06JETTzwxq1atajfT1taWCy64IP3790+vXr0yYcKErFmzpt1MS0tLJk2alLq6utTV1WXSpEnZsGHDq3uWAAAAu6FDBWnJkiX5+Mc/nuXLl2fx4sX5wx/+kNGjR2fz5s3lmcsvvzxXXnll5s6dmxUrVmTQoEE59dRT88ILL5Rnpk6dmkWLFmXhwoW5//77s2nTpowfPz7btm0rz0ycODFNTU1pbGxMY2NjmpqaMmnSpNfhKQMAAOxYl44MNzY2trt//fXXZ8CAAVm5cmX++q//OqVSKVdddVUuu+yynHHGGUmSG2+8MQMHDswtt9ySj3zkI2ltbc11112Xm2++OaecckqSZP78+WloaMjdd9+dMWPG5LHHHktjY2OWL1+eESNGJEnmzZuXUaNG5fHHH89hhx32ejx3AACAdl7TZ5BaW1uTJH379k2SPPnkk2lubs7o0aPLM7W1tTnhhBOydOnSJMnKlSvz4osvtpupr6/PsGHDyjPLli1LXV1duRwlyciRI1NXV1eeebm2trZs3Lix3Q0AAKAjXnVBKpVKmTZtWt71rndl2LBhSZLm5uYkycCBA9vNDhw4sHyuubk53bp1y7777rvLmQEDBmz3ew4YMKA883KzZs0qf16prq4uDQ0Nr/apAQAAb1GvuiB94hOfyMMPP5xvfvOb252rqalpd79UKm137OVePrOj+V09ziWXXJLW1tbybfXq1bvzNAAAAMpeVUG64IIL8r3vfS/33HNP9t9///LxQYMGJcl2r/KsX7++/KrSoEGDsnXr1rS0tOxyZt26ddv9vs8+++x2r079UW1tbfr06dPuBgAA0BEdKkilUimf+MQncuutt+aHP/xhDjrooHbnDzrooAwaNCiLFy8uH9u6dWuWLFmS448/PkkyfPjwdO3atd3M2rVr8+ijj5ZnRo0aldbW1jz44IPlmQceeCCtra3lGQAAgNdbh65i9/GPfzy33HJLvvvd76Z3797lV4rq6urSo0eP1NTUZOrUqZk5c2aGDBmSIUOGZObMmenZs2cmTpxYnp08eXIuuuii9OvXL3379s306dNzxBFHlK9qN3To0IwdOzZTpkzJtddemyQ577zzMn78eFewAwAA9pgOFaRrrrkmSXLiiSe2O3799dfn3HPPTZJcfPHF2bJlS84///y0tLRkxIgRueuuu9K7d+/y/OzZs9OlS5ecddZZ2bJlS04++eTccMMN6dy5c3lmwYIFufDCC8tXu5swYULmzp37ap4jAADAbulQQSqVSq84U1NTkxkzZmTGjBk7nenevXvmzJmTOXPm7HSmb9++mT9/fkfWAwAAeE1e0/cgAQAAvJkoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEBBQQIAACgoSAAAAAUFCQAAoKAgAQAAFBQkAACAgoIEAABQUJAAAAAKChIAAEChwwXpRz/6UU4//fTU19enpqYmt912W7vz5557bmpqatrdRo4c2W6mra0tF1xwQfr3759evXplwoQJWbNmTbuZlpaWTJo0KXV1damrq8ukSZOyYcOGDj9BAACA3dXhgrR58+YcddRRmTt37k5nxo4dm7Vr15Zvd9xxR7vzU6dOzaJFi7Jw4cLcf//92bRpU8aPH59t27aVZyZOnJimpqY0NjamsbExTU1NmTRpUkfXBQAA2G1dOvoD48aNy7hx43Y5U1tbm0GDBu3wXGtra6677rrcfPPNOeWUU5Ik8+fPT0NDQ+6+++6MGTMmjz32WBobG7N8+fKMGDEiSTJv3ryMGjUqjz/+eA477LCOrg0AAPCK9shnkO69994MGDAghx56aKZMmZL169eXz61cuTIvvvhiRo8eXT5WX1+fYcOGZenSpUmSZcuWpa6urlyOkmTkyJGpq6srz7xcW1tbNm7c2O4GAADQEa97QRo3blwWLFiQH/7wh7niiiuyYsWKvPvd705bW1uSpLm5Od26dcu+++7b7ucGDhyY5ubm8syAAQO2e+wBAwaUZ15u1qxZ5c8r1dXVpaGh4XV+ZgAAwJtdh99i90re//73l389bNiwHHPMMTnwwANz++2354wzztjpz5VKpdTU1JTv/+mvdzbzpy655JJMmzatfH/jxo1KEgAA0CF7/DLfgwcPzoEHHpgnnngiSTJo0KBs3bo1LS0t7ebWr1+fgQMHlmfWrVu33WM9++yz5ZmXq62tTZ8+fdrdAAAAOmKPF6Tnnnsuq1evzuDBg5Mkw4cPT9euXbN48eLyzNq1a/Poo4/m+OOPT5KMGjUqra2tefDBB8szDzzwQFpbW8szAAAAr7cOv8Vu06ZN+eUvf1m+/+STT6apqSl9+/ZN3759M2PGjJx55pkZPHhwnnrqqVx66aXp379/3ve+9yVJ6urqMnny5Fx00UXp169f+vbtm+nTp+eII44oX9Vu6NChGTt2bKZMmZJrr702SXLeeedl/PjxrmAHAADsMR0uSA899FBOOumk8v0/fu7nnHPOyTXXXJNHHnkkN910UzZs2JDBgwfnpJNOyre+9a307t27/DOzZ89Oly5dctZZZ2XLli05+eSTc8MNN6Rz587lmQULFuTCCy8sX+1uwoQJu/zuJQAAgNeqwwXpxBNPTKlU2un5O++88xUfo3v37pkzZ07mzJmz05m+fftm/vz5HV0PAADgVdvjn0ECAACoFgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUOhwQfrRj36U008/PfX19ampqcltt93W7nypVMqMGTNSX1+fHj165MQTT8yqVavazbS1teWCCy5I//7906tXr0yYMCFr1qxpN9PS0pJJkyalrq4udXV1mTRpUjZs2NDhJwgAALC7OlyQNm/enKOOOipz587d4fnLL788V155ZebOnZsVK1Zk0KBBOfXUU/PCCy+UZ6ZOnZpFixZl4cKFuf/++7Np06aMHz8+27ZtK89MnDgxTU1NaWxsTGNjY5qamjJp0qRX8RQBAAB2T5eO/sC4ceMybty4HZ4rlUq56qqrctlll+WMM85Iktx4440ZOHBgbrnllnzkIx9Ja2trrrvuutx888055ZRTkiTz589PQ0ND7r777owZMyaPPfZYGhsbs3z58owYMSJJMm/evIwaNSqPP/54DjvssFf7fAEAAHbqdf0M0pNPPpnm5uaMHj26fKy2tjYnnHBCli5dmiRZuXJlXnzxxXYz9fX1GTZsWHlm2bJlqaurK5ejJBk5cmTq6urKMy/X1taWjRs3trsBAAB0xOtakJqbm5MkAwcObHd84MCB5XPNzc3p1q1b9t13313ODBgwYLvHHzBgQHnm5WbNmlX+vFJdXV0aGhpe8/MBAADeWvbIVexqamra3S+VStsde7mXz+xoflePc8kll6S1tbV8W7169avYHAAAeCt7XQvSoEGDkmS7V3nWr19fflVp0KBB2bp1a1paWnY5s27duu0e/9lnn93u1ak/qq2tTZ8+fdrdAAAAOuJ1LUgHHXRQBg0alMWLF5ePbd26NUuWLMnxxx+fJBk+fHi6du3abmbt2rV59NFHyzOjRo1Ka2trHnzwwfLMAw88kNbW1vIMAADA663DV7HbtGlTfvnLX5bvP/nkk2lqakrfvn1zwAEHZOrUqZk5c2aGDBmSIUOGZObMmenZs2cmTpyYJKmrq8vkyZNz0UUXpV+/funbt2+mT5+eI444onxVu6FDh2bs2LGZMmVKrr322iTJeeedl/Hjx7uCHQAAsMd0uCA99NBDOemkk8r3p02bliQ555xzcsMNN+Tiiy/Oli1bcv7556elpSUjRozIXXfdld69e5d/Zvbs2enSpUvOOuusbNmyJSeffHJuuOGGdO7cuTyzYMGCXHjhheWr3U2YMGGn370EAADweuhwQTrxxBNTKpV2er6mpiYzZszIjBkzdjrTvXv3zJkzJ3PmzNnpTN++fTN//vyOrgcAAPCq7ZGr2AEAAFQjBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAovO4FacaMGampqWl3GzRoUPl8qVTKjBkzUl9fnx49euTEE0/MqlWr2j1GW1tbLrjggvTv3z+9evXKhAkTsmbNmtd7VQAAgHb2yCtI73znO7N27dry7ZFHHimfu/zyy3PllVdm7ty5WbFiRQYNGpRTTz01L7zwQnlm6tSpWbRoURYuXJj7778/mzZtyvjx47Nt27Y9sS4AAECSpMseedAuXdq9avRHpVIpV111VS677LKcccYZSZIbb7wxAwcOzC233JKPfOQjaW1tzXXXXZebb745p5xySpJk/vz5aWhoyN13350xY8bsiZUBAAD2zCtITzzxROrr63PQQQfl7/7u7/LrX/86SfLkk0+mubk5o0ePLs/W1tbmhBNOyNKlS5MkK1euzIsvvthupr6+PsOGDSvP7EhbW1s2btzY7gYAANARr3tBGjFiRG666abceeedmTdvXpqbm3P88cfnueeeS3Nzc5Jk4MCB7X5m4MCB5XPNzc3p1q1b9t13353O7MisWbNSV1dXvjU0NLzOzwwAAHize90L0rhx43LmmWfmiCOOyCmnnJLbb789yf99K90f1dTUtPuZUqm03bGXe6WZSy65JK2treXb6tWrX8OzAAAA3or2+GW+e/XqlSOOOCJPPPFE+XNJL38laP369eVXlQYNGpStW7empaVlpzM7Ultbmz59+rS7AQAAdMQeL0htbW157LHHMnjw4Bx00EEZNGhQFi9eXD6/devWLFmyJMcff3ySZPjw4enatWu7mbVr1+bRRx8tzwAAAOwJr/tV7KZPn57TTz89BxxwQNavX58vfvGL2bhxY84555zU1NRk6tSpmTlzZoYMGZIhQ4Zk5syZ6dmzZyZOnJgkqaury+TJk3PRRRelX79+6du3b6ZPn15+yx4AAMCe8roXpDVr1uTss8/Ob3/72+y3334ZOXJkli9fngMPPDBJcvHFF2fLli05//zz09LSkhEjRuSuu+5K7969y48xe/bsdOnSJWeddVa2bNmSk08+OTfccEM6d+78eq8LAABQ9roXpIULF+7yfE1NTWbMmJEZM2bsdKZ79+6ZM2dO5syZ8zpvBwAAsHN7/DNIAAAA1UJBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAICCggQAAFBQkAAAAApdKr0A7A3e8ZnbK73CHvPUl0+r9AoAAFVDQQLgDec/SgCwt/IWOwAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgIKCBAAAUOhS6QUAANiz3vGZ2yu9wh711JdPq/QKvIl4BQkAAKCgIAEAABQUJAAAgIKCBAAAUFCQAAAACgoSAABAQUECAAAoKEgAAAAFBQkAAKCgIAEAABQUJAAAgMJeX5C+9rWv5aCDDkr37t0zfPjw3HfffZVeCQAAeJPaqwvSt771rUydOjWXXXZZfvKTn+Sv/uqvMm7cuDz99NOVXg0AAHgT2qsL0pVXXpnJkyfnwx/+cIYOHZqrrroqDQ0Nueaaayq9GgAA8CbUpdIL7MzWrVuzcuXKfOYzn2l3fPTo0Vm6dOl2821tbWlrayvfb21tTZJs3Lhxzy76J15q+90b9nu90d7I/x0rQXbV6c2cWyK7avVmzi2RXbV6M+eWyK6avVHZ/fH3KZVKrzi71xak3/72t9m2bVsGDhzY7vjAgQPT3Ny83fysWbPyuc99brvjDQ0Ne2zHt5K6qyq9Aa+W7KqX7KqT3KqX7KqX7KrXG53dCy+8kLq6ul3O7LUF6Y9qamra3S+VStsdS5JLLrkk06ZNK99/6aWX8vzzz6dfv347nK92GzduTENDQ1avXp0+ffpUeh12k9yql+yql+yqk9yql+yq15s5u1KplBdeeCH19fWvOLvXFqT+/func+fO271atH79+u1eVUqS2tra1NbWtju2zz777MkV9wp9+vR50/0f+K1AbtVLdtVLdtVJbtVLdtXrzZrdK71y9Ed77UUaunXrluHDh2fx4sXtji9evDjHH398hbYCAADezPbaV5CSZNq0aZk0aVKOOeaYjBo1Kv/+7/+ep59+Oh/96EcrvRoAAPAmtFcXpPe///157rnn8vnPfz5r167NsGHDcscdd+TAAw+s9GoVV1tbm89+9rPbva2QvZvcqpfsqpfsqpPcqpfsqpfs/q+a0u5c6w4AAOAtYK/9DBIAAMAbTUECAAAoKEgAAAAFBQkAAKCwV1/Fjv+nra0tDz74YJ566qn87ne/y3777Zejjz46Bx10UKVXYxfkBtBxbW1tb/mraFUr2VWfF198Mc3NzeW/p/Tt27fSK1WcgrSXW7p0aebMmZPbbrstW7duzT777JMePXrk+eefT1tbWw4++OCcd955+ehHP5revXtXel0Kcqtujz/+eL75zW/mvvvu267cjhkzJmeeeaa/AOylZFed7rzzznJuTz/9dF566aX07Nkzf/EXf5HRo0fnQx/6UOrr6yu9Jjsgu+q0adOmLFiwIN/85jfz4IMPpq2trXxu//33z+jRo3Peeefl2GOPreCWleMy33ux9773vVmxYkUmTpyYCRMm5JhjjknPnj3L53/961/nvvvuyze/+c389Kc/zU033ZRTTz21ghuTyK2a/eQnP8nFF1+c++67L8cff3yOO+64vP3tby+X20cffTT33XdfNm7cmIsvvjhTp071l+29hOyq02233ZZPf/rTaW1tzXve856d5rZs2bKce+65+cIXvpD99tuv0msT2VWz2bNn50tf+lLe8Y53ZMKECTvNbtGiRRk5cmTmzJmTIUOGVHrtN5SCtBf76le/milTpqRbt26vOLtq1ao888wz/qK9F5Bb9TrwwAPzqU99KhMnTtzlWwyWLVuW2bNn58///M9z6aWXvoEbsjOyq07HHXdc/umf/imnnXZaOnXa+ceif/Ob3+Tqq6/OwIEDc9FFF72BG7Izsqte//t//+/88z//c4444ohdzrW1teW6665Lt27d8uEPf/gN2m7voCABFLZu3bpbxfbVzrPnyA6A14uCVEX+53/+J83NzampqcnAgQNz4IEHVnolAAB4U3GZ7yowe/bsNDQ05OCDD86oUaMycuTIHHzwwWloaMhVV11V6fV4FX7605+mc+fOlV6Dndi8eXPmzZuXD33oQxk3blze85735EMf+lC+/vWvZ/PmzZVej1dp3bp1+fznP1/pNdiBNWvW5LLLLstJJ52UoUOH5vDDD89JJ52Uyy67LKtXr670erxKq1evzt///d9Xeg12YsWKFfnABz6Qgw46KD169EjPnj1z0EEH5QMf+EAeeuihSq9XUV5B2st94QtfyP/5P/8nl156acaMGZOBAwemVCpl/fr1ufPOOzNr1qxMnz49//iP/1jpVemAn/70pzn66KPz0ksvVXoVXuZnP/tZTj311Pzud7/LCSec0O7P3JIlS9KrV6/cddddOfzwwyu9Kh3005/+NH/xF3+Rbdu2VXoV/sT999+fcePGpaGhIaNHj273Z27x4sVZvXp1fvCDH+Qv//IvK70qHeTP3N7rtttuy1lnnZWTTz55u79f3nXXXfmv//qvfPvb38573/veSq9aEQrSXq6hoSFz5szJ3/zN3+zw/KJFi/KJT3wiv/nNb97YxdilM844Y5fnW1tbc++99/qXxl7opJNOyqBBg3LjjTdu9xmVrVu35txzz83atWtzzz33VGhDdubhhx/e5fmf//znOfvss/2528sce+yxede73pXZs2fv8PwnP/nJ3H///VmxYsUbvBmv5Hvf+94uz//617/ORRdd5M/cXmjYsGH54Ac/mM985jM7PP8v//Ivuemmm7Jq1ao3eLO9g4K0l+vZs2dWrlyZoUOH7vD8qlWrcuyxx+Z3v/vdG7wZu9K1a9eceuqpGThw4A7PP//88/n+97/vXxp7oZ49e+ahhx7a6StEjz76aI477jh/5vZCnTp1Sk1NTXb0r7U/Hq+pqfHnbi/To0ePNDU15bDDDtvh+Z///Oc5+uijs2XLljd4M17Jrv7M/ZE/c3un7t275+GHH86hhx66w/OPP/54jjrqqPz+979/gzfbO/ii2L3ccccdly996Uu54YYb0qVL+7j+8Ic/ZObMmTnuuOMqtB07M3To0Jx55pmZPHnyDs83NTXl+9///hu8Fbtj3333zRNPPLHTgvTLX/4y++677xu8FbujX79++Zd/+ZecfPLJOzy/atWqnH766W/wVrySwYMHZ+nSpTstSMuWLcvgwYPf4K3YHYMHD85Xv/rVnb7LpampKcOHD39jl2K3/K//9b9y22235eKLL97h+e9+97s5+OCD3+Ct9h4K0l5uzpw5GT16dAYMGFD+PERNTU2am5vzox/9KLW1tVm8eHGl1+Rlhg8fnv/+7//eaUGqra3NAQcc8AZvxe6YMmVKzjnnnPzjP/5j+VXAP/6ZW7x4cWbOnJmpU6dWek12YPjw4XnmmWd2eoXPDRs27PK/dFMZ06dPz0c/+tGsXLlyh3/mvv71r7sg0V7qj/+u21lBeqVXl6icz3/+8/m7v/u7LFmypPzZvz/9c3fXXXdl4cKFlV6zYrzFrgq88MILmT9/fpYvX57m5uYkyaBBgzJq1KhMnDgxffr0qfCGvFxbW1u2bduWnj17VnoVXoV/+Zd/ydVXX12+rH6SlEqlDBo0KFOnTt3pf3GjshYtWpTNmzfngx/84A7Pt7S05Hvf+17OOeecN3gzXsm3vvWtzJ49OytXriy/Hatz584ZPnx4pk2blrPOOqvCG7Ij9913XzZv3pyxY8fu8PzmzZvz0EMP5YQTTniDN2N3LFu2LFdffXWWLVu23d8v/+Ef/iGjRo2q8IaVoyAB7MSTTz7Z7l8aBx10UIU3gje3F198Mb/97W+TJP3790/Xrl0rvBHwVqQgAQAAFHxRLAAAQEFBAgAAKChIAAAABQUJAACgoCC9Cfz93/99br755kqvQQfJrXo9/fTTvhm+SsmuOv3oRz9Ka2trpdfgVZBd9brpppvyq1/9qtJrVISC9Cbw61//Ov/8z/+co446qtKr0AFyq17veMc7cvjhh+fWW2+t9Cp0kOyq04knnpiDDz44V1xxRaVXoYNkV73OPffcHH744bngggsqvcobrkulF+C1u/fee5Mkjz/+eGUXoUPkVr1++MMf5qmnnsp3vvOdnHHGGZVehw6QXXV68skn8+STT+bOO++s9Cp0kOyq10svvZSnnnrqLZmd70ECAAAoeItdlbjxxhtz++23l+9ffPHF2WeffXL88cfnf/7nfyq4GbsitzeXlpaWzJkzJ3/+539e6VXoINlVn1//+tdZtWpVXnrppUqvQgfJbu/3zDPPZPr06dm4ceN251pbW/OpT30q69atq8BmewcFqUrMnDkzPXr0SJIsW7Ysc+fOzeWXX57+/fvnk5/8ZIW3Y2fk9uZw99135+yzz059fX0uv/zynHDCCZVeid0ku73fiy++mM9+9rM5/fTT86UvfSnbtm3L2WefnSFDhuTII4/MsGHD8tRTT1V6TXZAdtXryiuvzMaNG9OnT5/tztXV1eWFF17IlVdeWYHN9g7eYlclevbsmZ///Oc54IAD8ulPfzpr167NTTfdlFWrVuXEE0/Ms88+W+kV2QG5Va+nn346119/fa6//vps2rQpLS0t+fa3v50zzzyz0qvxCmRXXS666KLcfPPNmTBhQu65554MGzYsjz/+eD73uc+lU6dO+cIXvpAjjjgiCxYsqPSqvIzsqtewYcPyb//2b3nXu961w/NLly7NlClTsmrVqjd4s72DizRUibe97W157rnncsABB+Suu+4qv/rQvXv3bNmypcLbsTNyqz7f/va38/Wvfz0//vGP8573vCdXX311xo0bl169emXo0KGVXo9dkF11+s53vpMbbrgh73nPe/KLX/wif/Znf5bbb78948aNS5IMGDAgH/jAByq8JTsiu+r15JNP5oADDtjp+f333/8t/eqfglQlTj311Hz4wx/O0UcfnV/84hc57bTTkiSrVq3KO97xjsoux07JrfpMnDgxF198cf7zP/8zvXv3rvQ6dIDsqtMzzzxT/rqDQw89NLW1tTnkkEPK5w899NA0NzdXaj12QXbVq0ePHnnqqad2WpKeeuqp8kcE3op8BqlKfPWrX82oUaPy7LPP5j//8z/Tr1+/JMnKlStz9tlnV3g7dkZu1efv//7v87WvfS1jx47Nv/3bv6WlpaXSK7GbZFedtm3blq5du5bvd+nSJZ07dy7f79SpU3waYO8ku+o1YsSIXX5Z/U033ZTjjjvuDdxo7+IzSAAvs2XLlnz729/ON77xjTzwwAMZM2ZMbr/99jQ1NWXYsGGVXo9dkF316dSpU2688cbU1dUlSc4+++xcddVVGThwYJJkw4YN+dCHPpRt27ZVck12QHbV65577smpp56aqVOn5lOf+lQ5s3Xr1uXyyy/P1Vdfnbvuuivvfve7K7xpZShIe7GHH354t2ePPPLIPbgJHSG3N5cnnngi3/jGN3LTTTdl06ZNOe200/K3f/u3vmS0CsiuOnTq9MpvZqmpqfGX7L2Q7Krbtddem3/4h3/Iiy++mD59+qSmpiatra3p2rVrZs+enY997GOVXrFiFKS9WKdOnVJTU7PTl6f/eM4/fPYucntzeumll3L77bfnuuuuyw9+8IO0tbVVeiV2k+wAduw3v/lNvv3tb+eXv/xlSqVSDj300Pzt3/5t9t9//0qvVlEK0l6sI18keuCBB+7BTegIub35rV+/PgMGDKj0GrwKsgPglShIAAAABVexqyI333xz/vIv/zL19fXlVymuuuqqfPe7363wZuyK3AAAqoeCVCWuueaaTJs2Le95z3uyYcOG8mdX9tlnn1x11VWVXY6dkhsAQHVRkKrEnDlzMm/evFx22WXtvmPgmGOOySOPPFLBzdgVuQEAVBcFqUo8+eSTOfroo7c7Xltbm82bN1dgI3aH3AAAqkuXSi/A7jnooIPS1NS03VXPfvCDH+Twww+v0Fa8ErlVl3333Tc1NTW7Nfv888/v4W3oCNlVJ7lVL9lVL9m9MgWpSnzqU5/Kxz/+8fz+979PqVTKgw8+mG9+85uZNWtWvv71r1d6PXZCbtXF58Kql+yqk9yql+yql+xemct8V5F58+bli1/8YlavXp0kefvb354ZM2Zk8uTJFd6MXZEbAED1UJCq0G9/+9u89NJLvuywysit+vzqV7/K9ddfn1/96le5+uqrM2DAgDQ2NqahoSHvfOc7K70euyC76iS36iW76iW77blIQ5VZv359HnvssfziF7/Is88+W+l12E1yqz5LlizJEUcckQceeCC33nprNm3alCR5+OGH89nPfrbC27ErsqtOcqtesqtestsxBalKbNy4MZMmTUp9fX1OOOGE/PVf/3Xq6+vzwQ9+MK2trZVej52QW/X6zGc+ky9+8YtZvHhxunXrVj5+0kknZdmyZRXcjFciu+okt+olu+olux1TkKrEhz/84TzwwAO5/fbbs2HDhrS2tub73/9+HnrooUyZMqXS67ETcqtejzzySN73vvdtd3y//fbLc889V4GN2F2yq05yq16yq16y2zEFqUrcfvvt+cY3vpExY8akT58+6d27d8aMGZN58+bl9ttvr/R67ITcqtc+++yTtWvXbnf8Jz/5Sd7+9rdXYCN2l+yqk9yql+yql+x2TEGqEv369UtdXd12x+vq6rLvvvtWYCN2h9yq18SJE/PpT386zc3NqampyUsvvZQf//jHmT59ev6//+//q/R67ILsqpPcqpfsqpfsdqJEVbj22mtLp5xySumZZ54pH1u7dm1p9OjRpX/7t3+r4Gbsityq19atW0sTJ04sderUqVRTU1Pq2rVrqVOnTqUPfvCDpT/84Q+VXo9dkF11klv1kl31kt2Oucz3Xuzoo49u903HTzzxRNra2nLAAQckSZ5++unU1tZmyJAh+e///u9KrcnLyO3N5Ve/+lV+8pOf5KWXXsrRRx+dIUOGVHoldpPsqpPcqpfsqpfs2lOQ9mKf+9zndnv2rXwpxr2N3AAAqpeCBFCYNm3abs9eeeWVe3ATOkp21Ulu1Ut21Ut2r6xLpRcA2Fv85Cc/aXd/5cqV2bZtWw477LAkyS9+8Yt07tw5w4cPr8R67ILsqpPcqpfsqpfsXpmCVCW2bduW2bNn59vf/naefvrpbN26td35559/vkKbsStyqy733HNP+ddXXnllevfunRtvvLF8xcGWlpZ86EMfyl/91V9VakV2QnbVSW7VS3bVS3a7oZJXiGD3/dM//VNp8ODBpa985Sul7t27l77whS+UJk+eXOrXr1/p6quvrvR67ITcqld9fX3p0Ucf3e74I488Uho8eHAFNmJ3ya46ya16ya56yW7HfA9SlViwYEHmzZuX6dOnp0uXLjn77LPz9a9/Pf/8z/+c5cuXV3o9dkJu1Wvjxo1Zt27ddsfXr1+fF154oQIbsbtkV53kVr1kV71kt2MKUpVobm7OEUcckSR529veltbW1iTJ+PHjc/vtt1dyNXZBbtXrfe97Xz70oQ/lO9/5TtasWZM1a9bkO9/5TiZPnpwzzjij0uuxC7KrTnKrXrKrXrLbiUq/hMXuOfTQQ0vLly8vlUql0rve9a7SrFmzSqVSqbRw4cLSfvvtV8nV2AW5Va/NmzeXPvaxj5Vqa2tLnTp1KnXq1KnUrVu30sc+9rHSpk2bKr0euyC76iS36iW76iW7HXOZ7yrxmc98Jn369Mmll16a73znOzn77LPzjne8I08//XQ++clP5stf/nKlV2QH5Fb9Nm/enF/96lcplUo55JBD0qtXr0qvxG6SXXWSW/WSXfWSXXsKUpV64IEH8uMf/ziHHHJIJkyYUOl12E1yq05r1qxJTU1N3v72t1d6FTpIdtVJbtVLdtVLdv+PzyBVqREjRmTatGkZMWJEPv/5z1d6HXaT3KrHSy+9lM9//vOpq6vLgQcemAMOOCD77LNPvvCFL+Sll16q9Hrsguyqk9yql+yql+x2omJv7uN10dTUVOrUqVOl16CD5Lb3+8xnPlPab7/9Sl/72tdKP/3pT0tNTU2lr371q6X99tuvdOmll1Z6PXZBdtVJbtVLdtVLdjumIFU5f9GuTnLb+w0ePLj03e9+d7vjt912W6m+vr4CG7G7ZFed5Fa9ZFe9ZLdj3mIHsAPPP/98/uzP/my743/2Z3+W559/vgIbsbtkV53kVr1kV71kt2MKEsAOHHXUUZk7d+52x+fOnZujjjqqAhuxu2RXneRWvWRXvWS3Y10qvQC7Nm3atF2ef/bZZ9+gTegIuVW/yy+/PKeddlruvvvujBo1KjU1NVm6dGlWr16dO+64o9LrsQuyq05yq16yq16y2zGX+d7LnXTSSbs1d8899+zhTegIub05PPPMM/nqV7+an//85ymVSjn88MNz/vnnp76+vtKr8QpkV53kVr1kV71ktz0FCQAAoOAtdgB/4umnn96tuQMOOGAPb0JHya46ya16ya56yW7XvIIE8Cc6d+5c/vUf//FYU1PT7lhNTU22bdv2hu/GrsmuOsmtesmueslu17yCBPAnampqsv/+++fcc8/N6aefni5d/GOyWsiuOsmtesmueslu17yCBPAnmpubc+ONN+aGG25IS0tLPvjBD2by5MkZOnRopVfjFciuOsmtesmueslu1xQkgJ24//77c/311+c//uM/cvjhh2fy5MmZPHlyOnXyFXJ7O9lVJ7lVL9lVL9ltT0Haiz388MO7PXvkkUfuwU3oCLm9+axbty5nn312lixZkmeffTZ9+/at9ErsJtlVJ7lVL9lVL9n9P95wuBf78z//89TU1JQ/KLcrb9UP0e2N5PbmsXTp0nzjG9/If/zHf+Swww7LV7/61eyzzz6VXovdILvqJLfqJbvqJbvtKUh7sSeffLL865/85CeZPn16PvWpT2XUqFFJkmXLluWKK67I5ZdfXqkV2QG5Vbe1a9fmpptuyvXXX5+WlpZ84AMfyNKlS/POd76z0qvxCmRXneRWvWRXvWS3a95iVyWOO+64zJgxI+95z3vaHb/jjjvyT//0T1m5cmWFNmNX5FZ9unXrlvr6+pxzzjmZMGFCunbtusM5b4/c+8iuOsmtesmueslu1xSkKtGjR4/893//93ZXF3nsscfyF3/xF9myZUuFNmNX5FZ9/vRDqX98i+TL/zH5Vv5uiL2Z7KqT3KqX7KqX7HbNW+yqxNChQ/PFL34x1113Xbp3754kaWtryxe/+EWXZNyLya36/OlbJKkusqtOcqtesqtests1ryBViQcffDCnn356XnrppRx11FFJkp/+9KepqanJ97///Rx33HEV3pAdkRsAQHVRkKrI7373u8yfPz8///nPUyqVcvjhh2fixInp1atXpVdjF+RWPZ5++ukccMABuz3/m9/8Jm9/+9v34EbsLtlVJ7lVL9lVL9m9srfuN0BVoZ49e+a8887LlVdemdmzZ2fKlCn+kl0F5FY9jj322EyZMiUPPvjgTmdaW1szb968DBs2LLfeeusbuB27IrvqJLfqJbvqJbtX5jNIe7Hvfe97uz07YcKEPbgJHSG36vXYY49l5syZGTt2bLp27Zpjjjkm9fX16d69e1paWvKzn/0sq1atyjHHHJOvfOUrGTduXKVXpiC76iS36iW76iW7V+YtdnuxP73CyK68la8ysjeSW/X7/e9/nzvuuCP33XdfnnrqqWzZsiX9+/fP0UcfnTFjxmTYsGGVXpGdkF11klv1kl31kt3OKUgAAAAFn0ECAAAoKEhVZMmSJTn99NNzyCGHZMiQIZkwYULuu+++Sq/FK5AbAED1UJCqxPz583PKKaekZ8+eufDCC/OJT3wiPXr0yMknn5xbbrml0uuxE3IDAKguPoNUJYYOHZrzzjsvn/zkJ9sdv/LKKzNv3rw89thjFdqMXZEbAEB1UZCqRG1tbVatWpVDDjmk3fFf/vKXGTZsWH7/+99XaDN2RW4AANXFW+yqRENDQ/7rv/5ru+P/9V//lYaGhgpsxO6QGwBAdfFFsVXioosuyoUXXpimpqYcf/zxqampyf33358bbrghV199daXXYyfkBgBQXbzFroosWrQoV1xxRflzK0OHDs2nPvWpvPe9763wZuyK3AAAqoeCBAAAUPAWuyrzwgsv5E87badOnfK2t72tghuxO+QGAFAdXKRhL9fU1JTTTjutfL++vj777rtv+bbPPvtkxYoVFdyQHZEbAEB18grSXm7OnDl517ve1e7YzTffnLe//e0plUr5xje+kX/913/NzTffXKEN2RG5AQBUJwVpL/fjH/845557brtjI0eOzMEHH5wk6dGjR84666wKbMauyA0AoDp5i91ebvXq1TnggAPK9z//+c+nf//+5fuDBw/OunXrKrEauyA3AIDqpCDt5Wpra7NmzZry/U9+8pPp06dP+f7q1avTs2fPSqzGLsgNAKA6KUh7uaOPPjq33XbbTs/feuutOfroo9+4hdgtcgMAqE4+g7SXO//88/N3f/d3ecc73pGPfexj6dTp/3babdu25Wtf+1rmzJmTW265pcJb8nJyAwCoTr4otgp8+tOfzle+8pX07t07Bx98cGpqavKrX/0qmzZtyrRp0/KVr3yl0iuyA3IDAKg+ClKVWL58eb75zW/miSeeSJIMGTIkZ599dkaOHFnhzdgVuQEAVBcFCQAAoOAiDXuxp59+ukPzv/nNb/bQJnSE3AAAqpeCtBc79thjM2XKlDz44IM7nWltbc28efMybNiw3HrrrW/gduyM3AAAqper2O3FHnvsscycOTNjx45N165dc8wxx6S+vj7du3dPS0tLfvazn2XVqlU55phj8pWvfCXjxo2r9MpEbgAA1cxnkKrA73//+9xxxx2577778tRTT2XLli3p379/jj766IwZMybDhg2r9IrsgNwAAKqPggQAAFDwGSQAAICCggQAAFBQkAAAAAoKEgAAQEFBAgAAKChIAAAABQUJAACgoCABAAAUFCQAAIDC/w9qhuP+VBZlgwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "count = class_distribution.apply(lambda x: x.value_counts()).T.stack()\n",
    "#count.iloc[1,0]\n",
    "print(count.index.tolist())\n",
    "print(count[:])\n",
    "count.plot.bar()\n",
    "#    count[\"\"]\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the distribution of classes in the dataset is very unbalanced. There are far more \"ham\" mails than spam mails in the dataset. Model A comes closest to the real distribution, while Model B predicts even fewer spam mails than actually exist. An extreme case is Model C, which classifies all mails as unproblematic \"ham\" mails and assumes no spam mails in the data set.\n",
    "\n",
    "Obviously, based on the predicted distributions, we cannot yet estimate how many labels were actually predicted correctly. Therefore, we calculate this in the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which model made the most correct predictions, we want to calculate the accuracy in the next step. In general, the metric of the accuracy is defined as follows:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{correct classifications}}{\\text{all classifications}}\\\\\n",
    "$$.\n",
    "\n",
    "In the case of binary classification, the accuracy can be calculated as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} &= \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\\\\\n",
    "\\text{Accuracy} &= \\frac{\\text{true positives} + \\text{true negatives}}{\\text{true positives} + \\text{false positives} + \\text{true negatives} + \\text{false negatives}}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define methods that return TP, TN, FP and FN for a given model from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\") -> int:\n",
    "    df[\"New\"] = df.apply(lambda x: 1 if x[prediction_column]==x[gold_label_column] and\\\n",
    "         x[prediction_column] > 0 else 0, axis = 1)\n",
    "    #print(df[\"New\"].tolist())\n",
    "    return df[\"New\"].sum()### your code ###\n",
    "\n",
    "def true_negatives(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\") -> int:\n",
    "    df[\"New\"] = df.apply(lambda x: 1 if x[prediction_column]==x[gold_label_column] and\\\n",
    "    x[prediction_column] == 0 else 0, axis = 1)\n",
    "    #print(df[\"New\"].tolist())\n",
    "    return df[\"New\"].sum()\n",
    "\n",
    "def false_positives(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\") -> int:\n",
    "    df[\"New\"] = df.apply(lambda x: 1 if x[prediction_column]>x[gold_label_column] else 0, axis = 1)\n",
    "    #print(df[\"New\"].tolist())\n",
    "    return df[\"New\"].sum()\n",
    "\n",
    "def false_negatives(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\") -> int:\n",
    "    df[\"New\"] = df.apply(lambda x: 1 if x[prediction_column]<x[gold_label_column] else 0, axis = 1)\n",
    "    #print(df[\"New\"].tolist())\n",
    "    return df[\"New\"].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "3489\n",
      "183\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "# testtrue_positives(class_distribution, \"Model A\", \"Gold Labels\")\n",
    "print(true_positives(class_distribution, \"Model A\", \"Gold Labels\"))\n",
    "print(true_negatives(class_distribution, \"Model A\", \"Gold Labels\"))\n",
    "print(false_positives(class_distribution, \"Model A\", \"Gold Labels\"))\n",
    "print(false_negatives(class_distribution, \"Model A\", \"Gold Labels\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the method for calculating the accuracy using parameters for the values for TP, TN, FP and FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\"):\n",
    "    t_p = true_positives(df, prediction_column, gold_label_column)\n",
    "    t_n = true_negatives(df, prediction_column, gold_label_column)\n",
    "    f_p = false_positives(df, prediction_column, gold_label_column)\n",
    "    f_n = false_negatives(df, prediction_column, gold_label_column)\n",
    "    accuracy_val = (t_p+t_n)/(t_p+t_n+f_p+f_n)\n",
    "    return accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9041411042944786"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "accuracy(class_distribution, \"Model A\", \"Gold Labels\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having defined the methods, we can now calculate the accuracy for each model. To be able to reuse the calculated values and additionally depict them in a DataFrame, we store the results in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>183.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>3489.000000</td>\n",
       "      <td>3599.000000</td>\n",
       "      <td>3672.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>192.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>240.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.904141</td>\n",
       "      <td>0.927658</td>\n",
       "      <td>0.93865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    A            B           C\n",
       "TP          48.000000    30.000000     0.00000\n",
       "FP         183.000000    73.000000     0.00000\n",
       "TN        3489.000000  3599.000000  3672.00000\n",
       "FN         192.000000   210.000000   240.00000\n",
       "accuracy     0.904141     0.927658     0.93865"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary for storing all results\n",
    "evaluation_results = {}\n",
    "\n",
    "for model in [\"A\", \"B\", \"C\"]:\n",
    "    # column with the predicted labels from the model in the df\n",
    "    prediction_column = f\"prediction_model_{model}\"\n",
    "    str_temp = \"Model \"+model\n",
    "    # save TP, FP, TN and FN for this model\n",
    "    evaluation_results[model] = {\n",
    "        \"TP\":int(true_positives(class_distribution, str_temp, \"Gold Labels\")),\n",
    "        \"FP\":false_positives(class_distribution, str_temp, \"Gold Labels\"),\n",
    "        \"TN\":true_negatives(class_distribution, str_temp, \"Gold Labels\"),\n",
    "        \"FN\":false_negatives(class_distribution, str_temp, \"Gold Labels\"),\n",
    "        ### your code ###\n",
    "    }\n",
    "\n",
    "    # save accuracy for this model\n",
    "    evaluation_results[model][\"accuracy\"] = \\\n",
    "        accuracy(class_distribution,str_temp , \"Gold Labels\")\n",
    "\n",
    "# create a temporary DataFrame for displaying the results in a table\n",
    "pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that all models have a very high accuracy. This can be explained by the uneven distribution of the data, which is roughly reflected by all models.\n",
    "\n",
    "We also see that Model C has the highest accuracy and is therefore the best classifier according to this metric. However, since Model C does not recognize spam mails at all, but classifies all mails as \"ham\", this model will not add any value in practice. The use of the model would have no effect.\n",
    "\n",
    "Therefore, in the next step we want to look at other metrics with which we can compare the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Precision, Recall, F-measure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two very commonly used metrics for evaluating classifiers are precision and recall. \n",
    "\n",
    "Precision measures the percentage of the items that the classifier detected as positive that are actually positive according to the gold labels. Precision is defined as follows:\n",
    "$$\n",
    "\\text{Precision (\\textit{P})} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "Recall measures the percentage of positive items that the classifier was able to detect as positive. Recall is defined as follows:\n",
    "$$\n",
    "\\text{Recall (\\textit{R})} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define methods to calculate precision and recall based on the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\"):\n",
    "    tp = true_positives(df, prediction_column, gold_label_column)\n",
    "    fp = false_positives(df, prediction_column, gold_label_column)\n",
    "    return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\"):\n",
    "    tp = true_positives(df, prediction_column, gold_label_column)\n",
    "    fn = false_negatives(df, prediction_column, gold_label_column)\n",
    "    return tp/(tp+fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate precision and recall for the classifiers to see which classifier performs better overall.\n",
    "\n",
    "*Note*: Since we know that Model C does not generate true positives, both recall and precision will be 0. Besides, we already determined that the model is unsuitable for application. Therefore, we can exclude the model from the following calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>183.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>3489.000000</td>\n",
       "      <td>3599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>192.000000</td>\n",
       "      <td>210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.904141</td>\n",
       "      <td>0.927658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.291262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     A            B\n",
       "TP           48.000000    30.000000\n",
       "FP          183.000000    73.000000\n",
       "TN         3489.000000  3599.000000\n",
       "FN          192.000000   210.000000\n",
       "accuracy      0.904141     0.927658\n",
       "precision     0.207792     0.291262\n",
       "recall        0.200000     0.125000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude Model C from the following calculations\n",
    "evaluation_results.pop(\"C\", None)\n",
    "\n",
    "for model in evaluation_results:\n",
    "    # save precision and recall for this model\n",
    "    str_temp = \"Model \"+model\n",
    "    evaluation_results[model][\"precision\"] =\\\n",
    "         precision(class_distribution, str_temp , \"Gold Labels\")\n",
    "    evaluation_results[model][\"recall\"] = \\\n",
    "        recall(class_distribution, str_temp, \"Gold Labels\")\n",
    "\n",
    "pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculations should show that Model B has a significantly higher precision than Model A. However, Model A has a higher recall. Thus, we cannot easily decide which model is better. Therefore, we want to combine the two metrics in order to be able to compare the classifiers on the basis of a single value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One frequently used combination of precision and recall is the F-score. The F-Score is defined as follows:\n",
    "$$\n",
    "F_\\beta = \\frac{(\\beta^2 + 1) P R}{\\beta^2 P + R}\n",
    "$$\n",
    "The $\\beta$ parameter in the formula can be used to weight the importance between precision and recall.\n",
    "\n",
    "The most commonly used value for $\\beta$ is $1$. The resulting metric is called $F_1$ score.\n",
    "$$\n",
    "F_1 = \\frac{2 P R}{P + R}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a method to calculate the F-score based on the required parameters at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_beta(df: pd.DataFrame,  beta:float, prediction_column: str, gold_label_column: str = \"label_num\"):\n",
    "    P = precision(df, prediction_column, gold_label_column)\n",
    "    R = recall(df, prediction_column, gold_label_column)\n",
    "    val_temp = (beta**2+1)*P*R/(beta**2*P+R)\n",
    "    return val_temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to calculate the $F_1$ score for the classifiers to see which one performs better overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>183.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>3489.000000</td>\n",
       "      <td>3599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>192.000000</td>\n",
       "      <td>210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.904141</td>\n",
       "      <td>0.927658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.291262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F_1</th>\n",
       "      <td>0.203822</td>\n",
       "      <td>0.174927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     A            B\n",
       "TP           48.000000    30.000000\n",
       "FP          183.000000    73.000000\n",
       "TN         3489.000000  3599.000000\n",
       "FN          192.000000   210.000000\n",
       "accuracy      0.904141     0.927658\n",
       "precision     0.207792     0.291262\n",
       "recall        0.200000     0.125000\n",
       "F_1           0.203822     0.174927"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model in evaluation_results:\n",
    "    # save the F_1 score for this model\n",
    "    str_temp = \"Model \"+model\n",
    "    evaluation_results[model][\"F_1\"] = f_beta(class_distribution,1, str_temp , \"Gold Labels\")\n",
    "\n",
    "pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After correct calculation you should see that the $F_1$ score of Model A is higher than that of Model B. Accordingly, Model A is the better classifier if we want to weight precision and recall equally."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 4: Adapting the metric to the use case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not stop at that point. Is Model A really better for spam detection than Model B?\n",
    "\n",
    "Consider why Model B might be better than Model A for the spam detection use case in practice. Consider how the metric could be easily adapted for the purpose of spam detection. \n",
    "\n",
    "Calculate an adjusted metric for the models and briefly explain the adjustment and the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in evaluation_results:\n",
    "    evaluation_results[model][### metric name ###] = ### your code ###\n",
    "\n",
    "pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your brief explanation:** \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
