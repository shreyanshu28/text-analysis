{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Text Classification and Word Embeddings\n",
    "\n",
    "Due: Monday, January 23, 2023, at 2pm via Moodle\n",
    "\n",
    "**Team Members** `<Fill out>`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Guidelines\n",
    "\n",
    "- Solutions need to be uploaded as a single Jupyter notebook. You will see that this notebook contains some pre-filled cells that you should complete for the individual tasks.\n",
    "- For answers requiring written solutions, use Markdown cells (in combination with Jupyter LaTeX support) **inside this notebook**. Do *not* hand in any separate files, simply re-upload the `.ipynb` file.\n",
    "- Download the .zip file containing the dataset but do *not* upload it with your solution.\n",
    "- Make sure that the names of all team members are present in the solution (see cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.8\n"
     ]
    }
   ],
   "source": [
    "%%python \"--version\"\n",
    "# check requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "%pip install \"-r\" requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python \"-m\" spacy download en_core_web_sm\n",
    "# Install required language model for spaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: F.R.I.E.N.D.S and  Word2Vec (6 + 8 + 4 = 18 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Friends](https://en.wikipedia.org/wiki/Friends) is an American television sitcom, created by David Crane and Marta Kauffman. In this problem set we will use the transcripts from the show to train a Word2Vec model using the [Gensim](https://radimrehurek.com/gensim/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd  \n",
    "from collections import defaultdict,Counter\n",
    "import spacy \n",
    "import logging\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.manifold import TSNE\n",
    "from typing import Type\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-processing (6 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading and cleaning the data. The dataset for this problem set can be found in the attached `data` folder. Load the `friends_quotes.csv` file using pandas. The dataset is from [Kaggle](https://www.kaggle.com/ryanstonebraker/friends-transcript) and is created for building a classifier that can determine which friend from the Friend's TV Show would be most likely to say a quote. The column `quote` contains the line from the movie and `author` is the one who spoke it. Since these are the only two columns we need, we remove the rest and only keep these two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= \"%H:%M:%S\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>All right Joey, be nice. So does he have a hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>Just, 'cause, I don't want her to go through w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monica</td>\n",
       "      <td>Okay, everybody relax. This is not even a date...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>Sounds like a date to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>Alright, so I'm back in high school, I'm stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>All</td>\n",
       "      <td>Oh, yeah. Had that dream.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>Then I look down, and I realize there's a phon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                              quote\n",
       "0    Monica  There's nothing to tell! He's just some guy I ...\n",
       "1      Joey  C'mon, you're going out with the guy! There's ...\n",
       "2  Chandler  All right Joey, be nice. So does he have a hum...\n",
       "3    Phoebe                           Wait, does he eat chalk?\n",
       "4    Phoebe  Just, 'cause, I don't want her to go through w...\n",
       "5    Monica  Okay, everybody relax. This is not even a date...\n",
       "6  Chandler                          Sounds like a date to me.\n",
       "7  Chandler  Alright, so I'm back in high school, I'm stand...\n",
       "8       All                          Oh, yeah. Had that dream.\n",
       "9  Chandler  Then I look down, and I realize there's a phon..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = ['author','quote']\n",
    "df = pd.read_csv(\"/Users/anureddy/Desktop/Sem01/DataScience_for_text_analytics/Assignments/Assignment03/Assignment_03/data/friends_quotes.csv\",usecols=fields)\n",
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author    0\n",
       "quote     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() # check for missing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fortunately, there is no missing data, so we do not need to worry about that. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SpaCy similar to assignment 2 to pre-process the text and perform the following steps: \n",
    "- lowercase the words \n",
    "- remove stopwords and single characters\n",
    "- use regex to remove non-alphabetic characters; in other words: only keep \"a\" to \"z\" and digits. \n",
    "- remove lines that have less than 3 words, since they cannot contribute much to the training process.\n",
    "\n",
    "Please do not add additional steps on your own or additional cleaning as we want to create comparable results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS # only use these stop words, do not add your own!\n",
    "#df[\"quote\"] = # lowercase and remove non-alphabetic characters\n",
    "#quotes =[]  # to save all the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b584ee8da3374a289f4ed47115bac028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=7537), Label(value='0 / 7537'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)    \n",
    "\n",
    "quotes = df['quote'].parallel_apply(lambda x: nlp(x))\n",
    "#print([token.text for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quote']=df['quote'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "      <th>tokenizing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>there's nothing to tell! he's just some guy i ...</td>\n",
       "      <td>[there, 's, nothing, to, tell, !, he, 's, just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>c'mon, you're going out with the guy! there's ...</td>\n",
       "      <td>[c'm, on, ,, you, 're, going, out, with, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>all right joey, be nice. so does he have a hum...</td>\n",
       "      <td>[all, right, joey, ,, be, nice, ., so, does, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>wait, does he eat chalk?</td>\n",
       "      <td>[wait, ,, does, he, eat, chalk, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>just, 'cause, i don't want her to go through w...</td>\n",
       "      <td>[just, ,, 'cause, ,, i, do, n't, want, her, to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                              quote  \\\n",
       "0    Monica  there's nothing to tell! he's just some guy i ...   \n",
       "1      Joey  c'mon, you're going out with the guy! there's ...   \n",
       "2  Chandler  all right joey, be nice. so does he have a hum...   \n",
       "3    Phoebe                           wait, does he eat chalk?   \n",
       "4    Phoebe  just, 'cause, i don't want her to go through w...   \n",
       "\n",
       "                                          tokenizing  \n",
       "0  [there, 's, nothing, to, tell, !, he, 's, just...  \n",
       "1  [c'm, on, ,, you, 're, going, out, with, the, ...  \n",
       "2  [all, right, joey, ,, be, nice, ., so, does, h...  \n",
       "3                 [wait, ,, does, he, eat, chalk, ?]  \n",
       "4  [just, ,, 'cause, ,, i, do, n't, want, her, to...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(word):\n",
    "  return [token.text for token in nlp(word)]\n",
    "\n",
    "df['tokenizing'] = df['quote'].apply(tokenize)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quote_without_single_chars']=df['tokenizing'].apply(lambda x: ' '.join([word for word in x if len(word)> 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "      <th>tokenizing</th>\n",
       "      <th>quote_without_single_chars</th>\n",
       "      <th>quote_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>there's nothing to tell! he's just some guy i ...</td>\n",
       "      <td>[there, 's, nothing, to, tell, !, he, 's, just...</td>\n",
       "      <td>there 's nothing to tell he 's just some guy w...</td>\n",
       "      <td>tell guy work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>c'mon, you're going out with the guy! there's ...</td>\n",
       "      <td>[c'm, on, ,, you, 're, going, out, with, the, ...</td>\n",
       "      <td>c'm on you 're going out with the guy there 's...</td>\n",
       "      <td>c'm going guy got ta wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>all right joey, be nice. so does he have a hum...</td>\n",
       "      <td>[all, right, joey, ,, be, nice, ., so, does, h...</td>\n",
       "      <td>all right joey be nice so does he have hump hu...</td>\n",
       "      <td>right joey nice hump hump hairpiece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>wait, does he eat chalk?</td>\n",
       "      <td>[wait, ,, does, he, eat, chalk, ?]</td>\n",
       "      <td>wait does he eat chalk</td>\n",
       "      <td>wait eat chalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>just, 'cause, i don't want her to go through w...</td>\n",
       "      <td>[just, ,, 'cause, ,, i, do, n't, want, her, to...</td>\n",
       "      <td>just 'cause do n't want her to go through what...</td>\n",
       "      <td>'cause want went carl- oh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                              quote  \\\n",
       "0    Monica  there's nothing to tell! he's just some guy i ...   \n",
       "1      Joey  c'mon, you're going out with the guy! there's ...   \n",
       "2  Chandler  all right joey, be nice. so does he have a hum...   \n",
       "3    Phoebe                           wait, does he eat chalk?   \n",
       "4    Phoebe  just, 'cause, i don't want her to go through w...   \n",
       "\n",
       "                                          tokenizing  \\\n",
       "0  [there, 's, nothing, to, tell, !, he, 's, just...   \n",
       "1  [c'm, on, ,, you, 're, going, out, with, the, ...   \n",
       "2  [all, right, joey, ,, be, nice, ., so, does, h...   \n",
       "3                 [wait, ,, does, he, eat, chalk, ?]   \n",
       "4  [just, ,, 'cause, ,, i, do, n't, want, her, to...   \n",
       "\n",
       "                          quote_without_single_chars  \\\n",
       "0  there 's nothing to tell he 's just some guy w...   \n",
       "1  c'm on you 're going out with the guy there 's...   \n",
       "2  all right joey be nice so does he have hump hu...   \n",
       "3                             wait does he eat chalk   \n",
       "4  just 'cause do n't want her to go through what...   \n",
       "\n",
       "               quote_without_stopwords  \n",
       "0                        tell guy work  \n",
       "1           c'm going guy got ta wrong  \n",
       "2  right joey nice hump hump hairpiece  \n",
       "3                       wait eat chalk  \n",
       "4            'cause want went carl- oh  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quote_without_stopwords'] = df['quote_without_single_chars'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/ksgfnxyn4jg66j5nhtx4v0mc0000gn/T/ipykernel_1381/10699197.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['quote_without_stopwords_non_alphanumeric'] = df['quote_without_stopwords'].str.replace('[^a-zA-Z0-9]', ' ').str.lower()\n"
     ]
    }
   ],
   "source": [
    "df['quote_without_stopwords_non_alphanumeric'] = df['quote_without_stopwords'].str.replace('[^a-zA-Z0-9]', ' ').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "      <th>tokenizing</th>\n",
       "      <th>quote_without_single_chars</th>\n",
       "      <th>quote_without_stopwords</th>\n",
       "      <th>quote_without_stopwords_non_alphanumeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>there's nothing to tell! he's just some guy i ...</td>\n",
       "      <td>[there, 's, nothing, to, tell, !, he, 's, just...</td>\n",
       "      <td>there 's nothing to tell he 's just some guy w...</td>\n",
       "      <td>tell guy work</td>\n",
       "      <td>tell guy work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>c'mon, you're going out with the guy! there's ...</td>\n",
       "      <td>[c'm, on, ,, you, 're, going, out, with, the, ...</td>\n",
       "      <td>c'm on you 're going out with the guy there 's...</td>\n",
       "      <td>c'm going guy got ta wrong</td>\n",
       "      <td>c m going guy got ta wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>all right joey, be nice. so does he have a hum...</td>\n",
       "      <td>[all, right, joey, ,, be, nice, ., so, does, h...</td>\n",
       "      <td>all right joey be nice so does he have hump hu...</td>\n",
       "      <td>right joey nice hump hump hairpiece</td>\n",
       "      <td>right joey nice hump hump hairpiece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>wait, does he eat chalk?</td>\n",
       "      <td>[wait, ,, does, he, eat, chalk, ?]</td>\n",
       "      <td>wait does he eat chalk</td>\n",
       "      <td>wait eat chalk</td>\n",
       "      <td>wait eat chalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>just, 'cause, i don't want her to go through w...</td>\n",
       "      <td>[just, ,, 'cause, ,, i, do, n't, want, her, to...</td>\n",
       "      <td>just 'cause do n't want her to go through what...</td>\n",
       "      <td>'cause want went carl- oh</td>\n",
       "      <td>cause want went carl  oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60286</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>oh, it's gonna be okay.</td>\n",
       "      <td>[oh, ,, it, 's, gon, na, be, okay, .]</td>\n",
       "      <td>oh it 's gon na be okay</td>\n",
       "      <td>oh gon na okay</td>\n",
       "      <td>oh gon na okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60287</th>\n",
       "      <td>Rachel</td>\n",
       "      <td>(crying) do you guys have to go to the new hou...</td>\n",
       "      <td>[(, crying, ), do, you, guys, have, to, go, to...</td>\n",
       "      <td>crying do you guys have to go to the new house...</td>\n",
       "      <td>crying guys new house right away time</td>\n",
       "      <td>crying guys new house right away time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60288</th>\n",
       "      <td>Monica</td>\n",
       "      <td>we got some time.</td>\n",
       "      <td>[we, got, some, time, .]</td>\n",
       "      <td>we got some time</td>\n",
       "      <td>got time</td>\n",
       "      <td>got time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60289</th>\n",
       "      <td>Rachel</td>\n",
       "      <td>okay, should we get some coffee?</td>\n",
       "      <td>[okay, ,, should, we, get, some, coffee, ?]</td>\n",
       "      <td>okay should we get some coffee</td>\n",
       "      <td>okay coffee</td>\n",
       "      <td>okay coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60290</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>sure. where?</td>\n",
       "      <td>[sure, ., where, ?]</td>\n",
       "      <td>sure where</td>\n",
       "      <td>sure</td>\n",
       "      <td>sure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60291 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                              quote  \\\n",
       "0        Monica  there's nothing to tell! he's just some guy i ...   \n",
       "1          Joey  c'mon, you're going out with the guy! there's ...   \n",
       "2      Chandler  all right joey, be nice. so does he have a hum...   \n",
       "3        Phoebe                           wait, does he eat chalk?   \n",
       "4        Phoebe  just, 'cause, i don't want her to go through w...   \n",
       "...         ...                                                ...   \n",
       "60286  Chandler                            oh, it's gonna be okay.   \n",
       "60287    Rachel  (crying) do you guys have to go to the new hou...   \n",
       "60288    Monica                                  we got some time.   \n",
       "60289    Rachel                   okay, should we get some coffee?   \n",
       "60290  Chandler                                       sure. where?   \n",
       "\n",
       "                                              tokenizing  \\\n",
       "0      [there, 's, nothing, to, tell, !, he, 's, just...   \n",
       "1      [c'm, on, ,, you, 're, going, out, with, the, ...   \n",
       "2      [all, right, joey, ,, be, nice, ., so, does, h...   \n",
       "3                     [wait, ,, does, he, eat, chalk, ?]   \n",
       "4      [just, ,, 'cause, ,, i, do, n't, want, her, to...   \n",
       "...                                                  ...   \n",
       "60286              [oh, ,, it, 's, gon, na, be, okay, .]   \n",
       "60287  [(, crying, ), do, you, guys, have, to, go, to...   \n",
       "60288                           [we, got, some, time, .]   \n",
       "60289        [okay, ,, should, we, get, some, coffee, ?]   \n",
       "60290                                [sure, ., where, ?]   \n",
       "\n",
       "                              quote_without_single_chars  \\\n",
       "0      there 's nothing to tell he 's just some guy w...   \n",
       "1      c'm on you 're going out with the guy there 's...   \n",
       "2      all right joey be nice so does he have hump hu...   \n",
       "3                                 wait does he eat chalk   \n",
       "4      just 'cause do n't want her to go through what...   \n",
       "...                                                  ...   \n",
       "60286                            oh it 's gon na be okay   \n",
       "60287  crying do you guys have to go to the new house...   \n",
       "60288                                   we got some time   \n",
       "60289                     okay should we get some coffee   \n",
       "60290                                         sure where   \n",
       "\n",
       "                     quote_without_stopwords  \\\n",
       "0                              tell guy work   \n",
       "1                 c'm going guy got ta wrong   \n",
       "2        right joey nice hump hump hairpiece   \n",
       "3                             wait eat chalk   \n",
       "4                  'cause want went carl- oh   \n",
       "...                                      ...   \n",
       "60286                         oh gon na okay   \n",
       "60287  crying guys new house right away time   \n",
       "60288                               got time   \n",
       "60289                            okay coffee   \n",
       "60290                                   sure   \n",
       "\n",
       "      quote_without_stopwords_non_alphanumeric  \n",
       "0                                tell guy work  \n",
       "1                   c m going guy got ta wrong  \n",
       "2          right joey nice hump hump hairpiece  \n",
       "3                               wait eat chalk  \n",
       "4                     cause want went carl  oh  \n",
       "...                                        ...  \n",
       "60286                           oh gon na okay  \n",
       "60287    crying guys new house right away time  \n",
       "60288                                 got time  \n",
       "60289                              okay coffee  \n",
       "60290                                     sure  \n",
       "\n",
       "[60291 rows x 6 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lines that have less than 3 words, since they cannot contribute much to the training process.\n",
    "\n",
    "df['quote_length']=df[\"quote\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['quote_length']>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe before the removal of lines of length 3 --> (60291, 7)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the dataframe before the removal of lines of length 3 -->', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe after the removal of lines of length 3 --> (59271, 7)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the dataframe after the removal of lines of length 3 -->', df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS # only use these stop words, do not add your own!\n",
    "df[\"quote\"] = df[\"quote\"].str.casefold()\n",
    "df[\"quote\"] = df[\"quote\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "df[\"quote\"] = df[\"quote\"].str.replace('[^a-zA-Z0-9]', ' ', regex=True).str.strip()\n",
    "df[\"quote\"] = df[\"quote\"].str.replace('\\\\b\\\\w{1,2}\\\\s', ' ', regex=True).str.strip()\n",
    "quotes =[]  # to save all the lines\n",
    "\n",
    "#remove lines having less than 3 words\n",
    "df = df[df[\"quote\"].str.split().str.len().gt(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>there  tell    guy work with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey</td>\n",
       "      <td>mon  you  going guy  there  gotta wrong him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler</td>\n",
       "      <td>right joey  nice  hump  hump hairpiece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoebe</td>\n",
       "      <td>just   cause  don  want went carl  oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monica</td>\n",
       "      <td>okay  everybody relax  date    people going di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60279</th>\n",
       "      <td>Monica</td>\n",
       "      <td>wait minute  summer college lived grandma  tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60280</th>\n",
       "      <td>Ross</td>\n",
       "      <td>realise years coming up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60281</th>\n",
       "      <td>Monica</td>\n",
       "      <td>honey  forgot  promised treeger   leave keys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60285</th>\n",
       "      <td>Monica</td>\n",
       "      <td>crying  harder thought be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60287</th>\n",
       "      <td>Rachel</td>\n",
       "      <td>crying  guys new house right away  time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                              quote\n",
       "0        Monica                       there  tell    guy work with\n",
       "1          Joey        mon  you  going guy  there  gotta wrong him\n",
       "2      Chandler             right joey  nice  hump  hump hairpiece\n",
       "4        Phoebe              just   cause  don  want went carl  oh\n",
       "5        Monica  okay  everybody relax  date    people going di...\n",
       "...         ...                                                ...\n",
       "60279    Monica  wait minute  summer college lived grandma  tri...\n",
       "60280      Ross                            realise years coming up\n",
       "60281    Monica       honey  forgot  promised treeger   leave keys\n",
       "60285    Monica                          crying  harder thought be\n",
       "60287    Rachel            crying  guys new house right away  time\n",
       "\n",
       "[33546 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the vocabulary of the words and word combinations we want to learn representations from. We choose a subset of the most frequent words and bigrams to represent our corpus.\n",
    "- Use the Gensim Phrases package to automatically detect common phrases (bigrams) from a list of lines from the previous step (`min_count=10`). Now words like New_York will be considered as one entity and character names like joey_tribbiani will be recognized.\n",
    "- Create a list of words/bigrams with their frequencies and choose the top 15.000 words for the vocabulary, to keep the computation time-limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:39:16: collecting all words and their counts\n",
      "INFO - 19:39:16: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 19:39:16: PROGRESS: at sentence #10000, processed 147150 words and 47552 word types\n",
      "INFO - 19:39:16: PROGRESS: at sentence #20000, processed 291678 words and 80248 word types\n",
      "INFO - 19:39:16: PROGRESS: at sentence #30000, processed 433708 words and 110790 word types\n",
      "INFO - 19:39:16: collected 120759 token types (unigram + bigrams) from a corpus of 483279 words and 33546 sentences\n",
      "INFO - 19:39:16: merged Phrases<120759 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 19:39:16: Phrases lifecycle event {'msg': 'built Phrases<120759 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000> in 0.31s', 'datetime': '2023-01-08T19:39:16.418338', 'gensim': '4.3.0', 'python': '3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]', 'platform': 'macOS-13.0.1-arm64-i386-64bit', 'event': 'created'}\n",
      "INFO - 19:39:16: exporting phrases from Phrases<120759 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 19:39:16: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<173 phrases, min_count=10, threshold=10.0> from Phrases<120759 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000> in 0.13s', 'datetime': '2023-01-08T19:39:16.561303', 'gensim': '4.3.0', 'python': '3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]', 'platform': 'macOS-13.0.1-arm64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['there', 'tell', 'guy', 'work', 'with']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words =  Phrases(sentences=df[\"quote\"].str.split(\" \"), min_count=10) \n",
    "bigram =  Phraser(words) # define the phraser for bi-gram creation #\n",
    "new_lines =  bigram[df[\"quote\"].str.split(\" \")]# transform the lines #\n",
    "new_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('know', 6302),\n",
       " ('you', 5857),\n",
       " ('don', 3806),\n",
       " ('okay', 3784),\n",
       " ('that', 3641),\n",
       " ('yeah', 3285),\n",
       " ('like', 2914),\n",
       " ('right', 2647),\n",
       " ('gonna', 2567),\n",
       " ('hey', 2537),\n",
       " ('well', 2417),\n",
       " ('ross', 2282),\n",
       " ('what', 2165),\n",
       " ('she', 2159),\n",
       " ('look', 2141),\n",
       " ('joey', 2039),\n",
       " ('think', 1995),\n",
       " ('mean', 1830),\n",
       " ('chandler', 1826),\n",
       " ('monica', 1684),\n",
       " ('rachel', 1672),\n",
       " ('got', 1600),\n",
       " ('want', 1514),\n",
       " ('can', 1512),\n",
       " ('there', 1417),\n",
       " ('come', 1407),\n",
       " ('here', 1298),\n",
       " ('phoebe', 1272),\n",
       " ('guys', 1245),\n",
       " ('good', 1195),\n",
       " ('going', 1156),\n",
       " ('god', 1150),\n",
       " ('let', 1115),\n",
       " ('little', 1086),\n",
       " ('time', 1072),\n",
       " ('sorry', 1069),\n",
       " ('tell', 1068),\n",
       " ('great', 1051),\n",
       " ('him', 1044),\n",
       " ('her', 1022),\n",
       " ('didn', 1008),\n",
       " ('they', 989),\n",
       " ('guy', 978),\n",
       " ('this', 964),\n",
       " ('thing', 893),\n",
       " ('love', 861),\n",
       " ('yes', 844),\n",
       " ('it', 843),\n",
       " ('maybe', 807),\n",
       " ('way', 805),\n",
       " ('umm', 782),\n",
       " ('said', 740),\n",
       " ('wait', 724),\n",
       " ('looks', 703),\n",
       " ('now', 702),\n",
       " ('out', 697),\n",
       " ('me', 681),\n",
       " ('again', 652),\n",
       " ('people', 645),\n",
       " ('wanna', 625),\n",
       " ('thought', 603),\n",
       " ('man', 599),\n",
       " ('huh', 587),\n",
       " ('need', 559),\n",
       " ('actually', 543),\n",
       " ('sure', 539),\n",
       " ('wow', 536),\n",
       " ('listen', 534),\n",
       " ('believe', 529),\n",
       " ('doesn', 528),\n",
       " ('starts', 527),\n",
       " ('work', 525),\n",
       " ('big', 523),\n",
       " ('baby', 510),\n",
       " ('talk', 503),\n",
       " ('fine', 496),\n",
       " ('one', 493),\n",
       " ('room', 483),\n",
       " ('phone', 479),\n",
       " ('stuff', 475),\n",
       " ('and', 471),\n",
       " ('goes', 471),\n",
       " ('gets', 465),\n",
       " ('gotta', 453),\n",
       " ('looking', 451),\n",
       " ('pause', 437),\n",
       " ('cause', 435),\n",
       " ('day', 435),\n",
       " ('back', 429),\n",
       " ('too', 427),\n",
       " ('night', 421),\n",
       " ('hands', 421),\n",
       " ('away', 418),\n",
       " ('ooh', 405),\n",
       " ('lot', 403),\n",
       " ('nice', 394),\n",
       " ('see', 392),\n",
       " ('stop', 382),\n",
       " ('door', 380),\n",
       " ('just', 377),\n",
       " ('please', 377),\n",
       " ('things', 376),\n",
       " ('told', 372),\n",
       " ('really', 370),\n",
       " ('guess', 369),\n",
       " ('alright', 367),\n",
       " ('head', 366),\n",
       " ('but', 366),\n",
       " ('place', 366),\n",
       " ('them', 365),\n",
       " ('honey', 362),\n",
       " ('getting', 361),\n",
       " ('new', 361),\n",
       " ('feel', 360),\n",
       " ('woman', 355),\n",
       " ('thanks', 354),\n",
       " ('wanted', 354),\n",
       " ('not', 352),\n",
       " ('the', 351),\n",
       " ('remember', 348),\n",
       " ('kinda', 348),\n",
       " ('who', 348),\n",
       " ('entering', 346),\n",
       " ('better', 345),\n",
       " ('points', 340),\n",
       " ('hand', 338),\n",
       " ('girl', 338),\n",
       " ('turns', 335),\n",
       " ('pretty', 323),\n",
       " ('find', 319),\n",
       " ('help', 315),\n",
       " ('takes', 312),\n",
       " ('thinking', 309),\n",
       " ('rach', 309),\n",
       " ('face', 309),\n",
       " ('play', 308),\n",
       " ('went', 307),\n",
       " ('left', 304),\n",
       " ('off', 304),\n",
       " ('thank_you', 303),\n",
       " ('isn', 303),\n",
       " ('up', 303),\n",
       " ('won', 301),\n",
       " ('something', 300),\n",
       " ('friend', 297),\n",
       " ('kind', 296),\n",
       " ('apartment', 295),\n",
       " ('leave', 288),\n",
       " ('trying', 288),\n",
       " ('ohh', 287),\n",
       " ('tonight', 285),\n",
       " ('coming', 284),\n",
       " ('say', 284),\n",
       " ('happy', 283),\n",
       " ('married', 282),\n",
       " ('talking', 281),\n",
       " ('why', 278),\n",
       " ('life', 278),\n",
       " ('start', 277),\n",
       " ('fun', 277),\n",
       " ('bad', 277),\n",
       " ('hear', 276),\n",
       " ('try', 274),\n",
       " ('long', 274),\n",
       " ('funny', 272),\n",
       " ('date', 271),\n",
       " ('money', 271),\n",
       " ('having', 270),\n",
       " ('together', 263),\n",
       " ('walks', 262),\n",
       " ('friends', 262),\n",
       " ('wrong', 261),\n",
       " ('down', 261),\n",
       " ('today', 261),\n",
       " ('hello', 258),\n",
       " ('home', 258),\n",
       " ('hard', 258),\n",
       " ('job', 252),\n",
       " ('pheebs', 250),\n",
       " ('grabs', 250),\n",
       " ('laughs', 250),\n",
       " ('mom', 249),\n",
       " ('care', 245),\n",
       " ('wouldn', 244),\n",
       " ('wedding', 241),\n",
       " ('probably', 241),\n",
       " ('table', 240),\n",
       " ('right_now', 238),\n",
       " ('dad', 238),\n",
       " ('whoa', 237),\n",
       " ('happened', 235),\n",
       " ('ask', 235),\n",
       " ('sees', 235),\n",
       " ('listens', 234),\n",
       " ('totally', 231),\n",
       " ('weird', 231),\n",
       " ('old', 230),\n",
       " ('wasn', 229),\n",
       " ('then', 228),\n",
       " ('ready', 226),\n",
       " ('stupid', 226),\n",
       " ('all', 224),\n",
       " ('years', 224),\n",
       " ('ben', 223),\n",
       " ('wants', 221),\n",
       " ('everybody', 220),\n",
       " ('did', 220),\n",
       " ('makes', 220),\n",
       " ('party', 220),\n",
       " ('stay', 219),\n",
       " ('emma', 219),\n",
       " ('best', 216),\n",
       " ('kiss', 216),\n",
       " ('crazy', 216),\n",
       " ('saying', 214),\n",
       " ('runs', 212),\n",
       " ('watch', 211),\n",
       " ('sex', 210),\n",
       " ('leaves', 210),\n",
       " ('pick', 208),\n",
       " ('mon', 207),\n",
       " ('comes', 205),\n",
       " ('called', 204),\n",
       " ('happen', 204),\n",
       " ('mike', 204),\n",
       " ('go', 203),\n",
       " ('came', 201),\n",
       " ('supposed', 200),\n",
       " ('course', 199),\n",
       " ('haven', 198),\n",
       " ('second', 198),\n",
       " ('saw', 196),\n",
       " ('says', 195),\n",
       " ('cool', 195),\n",
       " ('was', 194),\n",
       " ('over', 194),\n",
       " ('are', 188),\n",
       " ('real', 187),\n",
       " ('bathroom', 186),\n",
       " ('picture', 185),\n",
       " ('wait_minute', 184),\n",
       " ('game', 184),\n",
       " ('dinner', 183),\n",
       " ('couldn', 183),\n",
       " ('anyway', 181),\n",
       " ('eat', 181),\n",
       " ('use', 180),\n",
       " ('with', 179),\n",
       " ('puts', 179),\n",
       " ('anymore', 179),\n",
       " ('thank', 178),\n",
       " ('meet', 178),\n",
       " ('picks', 178),\n",
       " ('anything', 176),\n",
       " ('live', 176),\n",
       " ('do', 176),\n",
       " ('open', 176),\n",
       " ('making', 174),\n",
       " ('took', 173),\n",
       " ('taking', 172),\n",
       " ('person', 170),\n",
       " ('problem', 170),\n",
       " ('idea', 169),\n",
       " ('couch', 168),\n",
       " ('eyes', 167),\n",
       " ('wearing', 167),\n",
       " ('tomorrow', 166),\n",
       " ('somebody', 166),\n",
       " ('ring', 165),\n",
       " ('voice', 165),\n",
       " ('miss', 164),\n",
       " ('and_and', 164),\n",
       " ('found', 163),\n",
       " ('telling', 162),\n",
       " ('bed', 162),\n",
       " ('excuse', 162),\n",
       " ('box', 162),\n",
       " ('yet', 161),\n",
       " ('break', 161),\n",
       " ('forget', 161),\n",
       " ('understand', 160),\n",
       " ('kids', 160),\n",
       " ('seeing', 160),\n",
       " ('women', 159),\n",
       " ('stops', 158),\n",
       " ('holds', 158),\n",
       " ('how', 157),\n",
       " ('boy', 156),\n",
       " ('hate', 156),\n",
       " ('enters', 156),\n",
       " ('gives', 155),\n",
       " ('end', 155),\n",
       " ('food', 155),\n",
       " ('heard', 155),\n",
       " ('uhm', 154),\n",
       " ('hair', 153),\n",
       " ('year', 153),\n",
       " ('reading', 153),\n",
       " ('book', 153),\n",
       " ('couple', 152),\n",
       " ('about', 151),\n",
       " ('does', 151),\n",
       " ('story', 150),\n",
       " ('before', 150),\n",
       " ('emily', 150),\n",
       " ('hell', 149),\n",
       " ('gave', 149),\n",
       " ('doing', 149),\n",
       " ('hot', 149),\n",
       " ('sister', 149),\n",
       " ('parents', 148),\n",
       " ('father', 148),\n",
       " ('cute', 148),\n",
       " ('pants', 148),\n",
       " ('means', 146),\n",
       " ('bring', 146),\n",
       " ('wife', 146),\n",
       " ('seen', 146),\n",
       " ('later', 146),\n",
       " ('turn', 145),\n",
       " ('nothing', 145),\n",
       " ('sleep', 144),\n",
       " ('dog', 143),\n",
       " ('morning', 142),\n",
       " ('everything', 142),\n",
       " ('would', 142),\n",
       " ('knew', 141),\n",
       " ('window', 141),\n",
       " ('sweet', 140),\n",
       " ('name', 140),\n",
       " ('much', 140),\n",
       " ('house', 140),\n",
       " ('moving', 140),\n",
       " ('thinks', 139),\n",
       " ('late', 139),\n",
       " ('aren', 139),\n",
       " ('fact', 139),\n",
       " ('run', 139),\n",
       " ('chair', 139),\n",
       " ('world', 138),\n",
       " ('cut', 137),\n",
       " ('car', 137),\n",
       " ('laugh', 137),\n",
       " ('movie', 137),\n",
       " ('office', 137),\n",
       " ('mind', 136),\n",
       " ('us', 136),\n",
       " ('hold', 135),\n",
       " ('coffee', 134),\n",
       " ('matter', 134),\n",
       " ('holding', 134),\n",
       " ('doctor', 134),\n",
       " ('bedroom', 133),\n",
       " ('gone', 133),\n",
       " ('number', 132),\n",
       " ('in', 132),\n",
       " ('kid', 131),\n",
       " ('check', 131),\n",
       " ('uhh', 130),\n",
       " ('sits', 129),\n",
       " ('playing', 129),\n",
       " ('first', 129),\n",
       " ('walk', 129),\n",
       " ('sound', 128),\n",
       " ('hang', 128),\n",
       " ('singing', 128),\n",
       " ('geller', 128),\n",
       " ('worry', 128),\n",
       " ('different', 128),\n",
       " ('amazing', 127),\n",
       " ('sitting', 127),\n",
       " ('sit', 127),\n",
       " ('two', 127),\n",
       " ('bye', 125),\n",
       " ('hope', 124),\n",
       " ('read', 124),\n",
       " ('half', 124),\n",
       " ('around', 124),\n",
       " ('bag', 124),\n",
       " ('started', 123),\n",
       " ('throw', 123),\n",
       " ('chance', 122),\n",
       " ('girls', 122),\n",
       " ('lost', 121),\n",
       " ('throws', 121),\n",
       " ('wish', 120),\n",
       " ('minutes', 120),\n",
       " ('richard', 120),\n",
       " ('babies', 120),\n",
       " ('outside', 119),\n",
       " ('tries', 119),\n",
       " ('knows', 119),\n",
       " ('question', 119),\n",
       " ('where', 119),\n",
       " ('reason', 119),\n",
       " ('beautiful', 119),\n",
       " ('anybody', 118),\n",
       " ('big_deal', 118),\n",
       " ('finally', 117),\n",
       " ('exits', 117),\n",
       " ('opens', 117),\n",
       " ('easy', 117),\n",
       " ('huge', 117),\n",
       " ('times', 116),\n",
       " ('point', 115),\n",
       " ('close', 115),\n",
       " ('mother', 114),\n",
       " ('excited', 114),\n",
       " ('relationship', 114),\n",
       " ('janice', 114),\n",
       " ('met', 114),\n",
       " ('dance', 114),\n",
       " ('scene', 113),\n",
       " ('change', 113),\n",
       " ('line', 113),\n",
       " ('true', 113),\n",
       " ('else', 113),\n",
       " ('buy', 112),\n",
       " ('kitchen', 112),\n",
       " ('kidding', 112),\n",
       " ('mad', 112),\n",
       " ('wear', 112),\n",
       " ('for', 111),\n",
       " ('perfect', 111),\n",
       " ('hit', 110),\n",
       " ('other', 110),\n",
       " ('minute', 110),\n",
       " ('mouth', 109),\n",
       " ('is', 108),\n",
       " ('working', 108),\n",
       " ('important', 108),\n",
       " ('tape', 108),\n",
       " ('fire', 108),\n",
       " ('shouldn', 108),\n",
       " ('hours', 107),\n",
       " ('walks_away', 107),\n",
       " ('carol', 106),\n",
       " ('week', 106),\n",
       " ('glad', 106),\n",
       " ('asked', 106),\n",
       " ('card', 106),\n",
       " ('everyone', 105),\n",
       " ('watching', 105),\n",
       " ('answer', 104),\n",
       " ('dude', 104),\n",
       " ('naked', 103),\n",
       " ('giving', 103),\n",
       " ('yelling', 102),\n",
       " ('running', 102),\n",
       " ('standing', 101),\n",
       " ('word', 100),\n",
       " ('family', 100),\n",
       " ('set', 100),\n",
       " ('gunther', 100),\n",
       " ('broke', 99),\n",
       " ('ahh', 99),\n",
       " ('bet', 99),\n",
       " ('win', 99),\n",
       " ('myself', 98),\n",
       " ('moment', 98),\n",
       " ('floor', 98),\n",
       " ('rest', 98),\n",
       " ('street', 98),\n",
       " ('seriously', 98),\n",
       " ('boyfriend', 97),\n",
       " ('someone', 97),\n",
       " ('birthday', 97),\n",
       " ('they_hug', 97),\n",
       " ('cat', 97),\n",
       " ('more', 97),\n",
       " ('arm', 96),\n",
       " ('pregnant', 96),\n",
       " ('pay', 96),\n",
       " ('joe', 96),\n",
       " ('shocked', 96),\n",
       " ('talking_about', 95),\n",
       " ('dead', 95),\n",
       " ('hurt', 95),\n",
       " ('drink', 94),\n",
       " ('mrs', 94),\n",
       " ('joke', 94),\n",
       " ('waiting', 93),\n",
       " ('exactly', 93),\n",
       " ('leaving', 93),\n",
       " ('though', 93),\n",
       " ('ball', 93),\n",
       " ('hmm', 93),\n",
       " ('ass', 93),\n",
       " ('will', 92),\n",
       " ('dress', 92),\n",
       " ('s', 92),\n",
       " ('quickly', 92),\n",
       " ('kill', 91),\n",
       " ('laughing', 91),\n",
       " ('butt', 91),\n",
       " ('free', 91),\n",
       " ('check_out', 91),\n",
       " ('lady', 91),\n",
       " ('feeling', 90),\n",
       " ('hat', 90),\n",
       " ('ugh', 90),\n",
       " ('pulls', 90),\n",
       " ('believe_this', 90),\n",
       " ('moves', 90),\n",
       " ('little_bit', 90),\n",
       " ('water', 90),\n",
       " ('clothes', 89),\n",
       " ('crying', 89),\n",
       " ('shows', 88),\n",
       " ('living', 88),\n",
       " ('sounds', 88),\n",
       " ('have', 87),\n",
       " ('realizes', 87),\n",
       " ('ago', 87),\n",
       " ('girlfriend', 87),\n",
       " ('christmas', 87),\n",
       " ('coat', 87),\n",
       " ('tickets', 86),\n",
       " ('heads', 86),\n",
       " ('on', 86),\n",
       " ('soon', 86),\n",
       " ('starts_leave', 86),\n",
       " ('marriage', 86),\n",
       " ('audition', 86),\n",
       " ('because', 85),\n",
       " ('thanksgiving', 85),\n",
       " ('city', 84),\n",
       " ('machine', 84),\n",
       " ('bing', 84),\n",
       " ('able', 84),\n",
       " ('mine', 84),\n",
       " ('talked', 84),\n",
       " ('die', 84),\n",
       " ('lose', 84),\n",
       " ('marcel', 84),\n",
       " ('men', 83),\n",
       " ('absolutely', 83),\n",
       " ('notices', 83),\n",
       " ('shirt', 83),\n",
       " ('part', 82),\n",
       " ('smell', 82),\n",
       " ('eye', 82),\n",
       " ('promise', 82),\n",
       " ('fight', 82),\n",
       " ('nods', 82),\n",
       " ('inside', 82),\n",
       " ('living_room', 82),\n",
       " ('marry', 81),\n",
       " ('restaurant', 81),\n",
       " ('david', 81),\n",
       " ('paper', 81),\n",
       " ('hug', 81),\n",
       " ('actor', 81),\n",
       " ('alone', 80),\n",
       " ('upset', 80),\n",
       " ('weren', 79),\n",
       " ('stand', 79),\n",
       " ('walking', 79),\n",
       " ('sick', 79),\n",
       " ('sweetie', 79),\n",
       " ('figured', 78),\n",
       " ('special', 78),\n",
       " ('so', 78),\n",
       " ('definitely', 78),\n",
       " ('opens_door', 78),\n",
       " ('brought', 77),\n",
       " ('green', 77),\n",
       " ('meant', 77),\n",
       " ('class', 77),\n",
       " ('julie', 77),\n",
       " ('relax', 76),\n",
       " ('high_school', 76),\n",
       " ('brother', 76),\n",
       " ('stands', 76),\n",
       " ('enough', 76),\n",
       " ('calling', 76),\n",
       " ('hugs', 76),\n",
       " ('son', 76),\n",
       " ('message', 76),\n",
       " ('gasps', 76),\n",
       " ('hall', 75),\n",
       " ('could', 75),\n",
       " ('kisses', 75),\n",
       " ('shot', 74),\n",
       " ('dating', 74),\n",
       " ('yourself', 74),\n",
       " ('tried', 74),\n",
       " ('cry', 74),\n",
       " ('store', 74),\n",
       " ('drunk', 74),\n",
       " ('shut', 73),\n",
       " ('ever', 73),\n",
       " ('deal', 73),\n",
       " ('terrible', 73),\n",
       " ('forgot', 73),\n",
       " ('boss', 73),\n",
       " ('likes', 72),\n",
       " ('welcome', 72),\n",
       " ('song', 72),\n",
       " ('no', 72),\n",
       " ('small', 71),\n",
       " ('ya', 71),\n",
       " ('still', 71),\n",
       " ('shower', 71),\n",
       " ('fat', 71),\n",
       " ('smart', 71),\n",
       " ('gay', 71),\n",
       " ('while', 70),\n",
       " ('putting', 70),\n",
       " ('finish', 70),\n",
       " ('new_york', 70),\n",
       " ('fake', 70),\n",
       " ('high', 69),\n",
       " ('apparently', 69),\n",
       " ('days', 69),\n",
       " ('liked', 69),\n",
       " ('pizza', 69),\n",
       " ('plate', 69),\n",
       " ('fall', 68),\n",
       " ('either', 68),\n",
       " ('seven', 68),\n",
       " ('obviously', 68),\n",
       " ('asking', 68),\n",
       " ('seat', 68),\n",
       " ('serious', 68),\n",
       " ('cab', 68),\n",
       " ('whoa_whoa', 68),\n",
       " ('interrupting', 68),\n",
       " ('straight', 67),\n",
       " ('worse', 67),\n",
       " ('gift', 67),\n",
       " ('plan', 67),\n",
       " ('hour', 67),\n",
       " ('piece', 67),\n",
       " ('husband', 67),\n",
       " ('college', 67),\n",
       " ('worried', 67),\n",
       " ('motions', 67),\n",
       " ('fault', 67),\n",
       " ('afraid', 67),\n",
       " ('charlie', 67),\n",
       " ('ice', 66),\n",
       " ('duck', 66),\n",
       " ('finger', 66),\n",
       " ('worked', 66),\n",
       " ('call', 66),\n",
       " ('fast', 66),\n",
       " ('fingers', 66),\n",
       " ('monkey', 66),\n",
       " ('mark', 66),\n",
       " ('loved', 65),\n",
       " ('feet', 65),\n",
       " ('touch', 65),\n",
       " ('when', 65),\n",
       " ('past', 65),\n",
       " ('surprise', 65),\n",
       " ('lunch', 65),\n",
       " ('turkey', 65),\n",
       " ('picked', 65),\n",
       " ('dollars', 64),\n",
       " ('looked', 64),\n",
       " ('bought', 64),\n",
       " ('heart', 64),\n",
       " ('months', 64),\n",
       " ('candy', 64),\n",
       " ('suddenly', 64),\n",
       " ('from', 64),\n",
       " ('roommate', 64),\n",
       " ('massage', 64),\n",
       " ('cake', 64),\n",
       " ('list', 64),\n",
       " ('going_on', 63),\n",
       " ('wine', 63),\n",
       " ('feelings', 63),\n",
       " ('extra', 63),\n",
       " ('quit', 63),\n",
       " ('camera', 63),\n",
       " ('chicken', 63),\n",
       " ('cold', 63),\n",
       " ('fridge', 63),\n",
       " ('scared', 63),\n",
       " ('t', 63),\n",
       " ('should', 63),\n",
       " ('bucks', 63),\n",
       " ('pain', 62),\n",
       " ('himself', 62),\n",
       " ('body', 62),\n",
       " ('present', 62),\n",
       " ('sad', 62),\n",
       " ('sandwich', 62),\n",
       " ('doin', 62),\n",
       " ('figure', 61),\n",
       " ('building', 61),\n",
       " ('wondering', 61),\n",
       " ('grab', 61),\n",
       " ('kissing', 61),\n",
       " ('shoes', 61),\n",
       " ('buddy', 61),\n",
       " ('grandmother', 61),\n",
       " ('london', 61),\n",
       " ('bunch', 60),\n",
       " ('plus', 60),\n",
       " ('they_kiss', 60),\n",
       " ('smiling', 60),\n",
       " ('three', 60),\n",
       " ('forever', 60),\n",
       " ('sexy', 60),\n",
       " ('clock', 60),\n",
       " ('sign', 60),\n",
       " ('sweater', 60),\n",
       " ('lying', 60),\n",
       " ('feels', 60),\n",
       " ('daddy', 59),\n",
       " ('getting_married', 59),\n",
       " ('child', 59),\n",
       " ('fair', 59),\n",
       " ('sleeping', 59),\n",
       " ('falls', 59),\n",
       " ('stares', 59),\n",
       " ('light', 59),\n",
       " ('spend', 59),\n",
       " ('felt', 59),\n",
       " ('died', 59),\n",
       " ('counter', 59),\n",
       " ('closes_door', 59),\n",
       " ('middle', 58),\n",
       " ('very', 58),\n",
       " ('moved', 58),\n",
       " ('entire', 58),\n",
       " ('far', 58),\n",
       " ('hi', 58),\n",
       " ('lie', 58),\n",
       " ('underwear', 58),\n",
       " ('bob', 58),\n",
       " ('yesterday', 58),\n",
       " ('missed', 58),\n",
       " ('weekend', 58),\n",
       " ('writing', 58),\n",
       " ('realize', 57),\n",
       " ('purse', 57),\n",
       " ('words', 57),\n",
       " ('milk', 57),\n",
       " ('write', 57),\n",
       " ('be', 57),\n",
       " ('order', 57),\n",
       " ('needs', 57),\n",
       " ('kathy', 57),\n",
       " ('air', 56),\n",
       " ('worst', 56),\n",
       " ('lucky', 56),\n",
       " ('aunt', 56),\n",
       " ('note', 56),\n",
       " ('stuck', 56),\n",
       " ('nose', 56),\n",
       " ('sent', 56),\n",
       " ('smoke', 56),\n",
       " ('tiny', 56),\n",
       " ('completely', 56),\n",
       " ('interesting', 56),\n",
       " ('weeks', 56),\n",
       " ('business', 56),\n",
       " ('calls', 55),\n",
       " ('clear', 55),\n",
       " ('susan', 55),\n",
       " ('done', 55),\n",
       " ('lives', 55),\n",
       " ('carrying', 55),\n",
       " ('dressed', 55),\n",
       " ('flowers', 55),\n",
       " ('hangs', 55),\n",
       " ('save', 55),\n",
       " ('leg', 55),\n",
       " ('music', 55),\n",
       " ('shoulder', 55),\n",
       " ('keys', 55),\n",
       " ('happens', 55),\n",
       " ('tag', 55),\n",
       " ('porn', 55),\n",
       " ('control', 54),\n",
       " ('paul', 54),\n",
       " ('red', 54),\n",
       " ('case', 54),\n",
       " ('were', 54),\n",
       " ('spent', 54),\n",
       " ('smile', 54),\n",
       " ('stick', 54),\n",
       " ('cookies', 54),\n",
       " ('pauses', 54),\n",
       " ('incredible', 54),\n",
       " ('joey_tribbiani', 54),\n",
       " ('jumps', 54),\n",
       " ('good_bye', 54),\n",
       " ('plane', 54),\n",
       " ('boxes', 54),\n",
       " ('pointing', 53),\n",
       " ('catch', 53),\n",
       " ('beer', 53),\n",
       " ('bit', 53),\n",
       " ('glass', 53),\n",
       " ('yours', 53),\n",
       " ('pass', 53),\n",
       " ('bra', 53),\n",
       " ('young', 53),\n",
       " ('show', 53),\n",
       " ('closer', 53),\n",
       " ('lights', 53),\n",
       " ('best_friend', 53),\n",
       " ('sir', 53),\n",
       " ('hanging', 53),\n",
       " ('chef', 53),\n",
       " ('hospital', 53),\n",
       " ('favorite', 52),\n",
       " ('idiot', 52),\n",
       " ('smiles', 52),\n",
       " ('black', 52),\n",
       " ('gotten', 52),\n",
       " ('pocket', 52),\n",
       " ('pull', 52),\n",
       " ('meeting', 52),\n",
       " ('finds', 52),\n",
       " ('learn', 52),\n",
       " ('asleep', 52),\n",
       " ('questions', 52),\n",
       " ('santa', 52),\n",
       " ('boat', 51),\n",
       " ('turned', 51),\n",
       " ('barry', 51),\n",
       " ('push', 51),\n",
       " ('hits', 51),\n",
       " ('they_start', 51),\n",
       " ('wha', 51),\n",
       " ('decided', 51),\n",
       " ('blue', 51),\n",
       " ('desk', 51),\n",
       " ('wonderful', 51),\n",
       " ('tough', 51),\n",
       " ('ear', 51),\n",
       " ('secret', 51),\n",
       " ('children', 51),\n",
       " ('wall', 51),\n",
       " ('conversation', 51),\n",
       " ('damn', 51),\n",
       " ('woah', 51),\n",
       " ('mona', 51),\n",
       " ('oh', 50),\n",
       " ('presents', 50),\n",
       " ('horrible', 50),\n",
       " ('get', 50),\n",
       " ('once', 50),\n",
       " ('cards', 50),\n",
       " ('nervous', 50),\n",
       " ('kept', 50),\n",
       " ('instead', 50),\n",
       " ('swear', 50),\n",
       " ('glares_him', 50),\n",
       " ('mistake', 50),\n",
       " ('surprised', 50),\n",
       " ('good_luck', 50),\n",
       " ('appreciate', 50),\n",
       " ('transcribed', 50),\n",
       " ('star', 50),\n",
       " ('trip', 50),\n",
       " ('amy', 50),\n",
       " ('decide', 49),\n",
       " ('slept', 49),\n",
       " ('ran', 49),\n",
       " ('send', 49),\n",
       " ('five', 49),\n",
       " ('bigger', 49),\n",
       " ('staring', 49),\n",
       " ('bitch', 49),\n",
       " ('cup', 49),\n",
       " ('ladies', 49),\n",
       " ('strong', 49),\n",
       " ('bottle', 49),\n",
       " ('years_ago', 49),\n",
       " ('town', 49),\n",
       " ('school', 49),\n",
       " ('acting', 49),\n",
       " ('your', 48),\n",
       " ('clean', 48),\n",
       " ('thank_much', 48),\n",
       " ('step', 48),\n",
       " ('wrote', 48),\n",
       " ('sits_down', 48),\n",
       " ('drops', 48),\n",
       " ('enjoy', 48),\n",
       " ('jack', 48),\n",
       " ('goodbye', 48),\n",
       " ('quick', 48),\n",
       " ('accent', 48),\n",
       " ('long_time', 48),\n",
       " ('chick', 48),\n",
       " ('fired', 48),\n",
       " ('to', 47),\n",
       " ('mess', 47),\n",
       " ('cookie', 47),\n",
       " ('eating', 47),\n",
       " ('dream', 47),\n",
       " ('happening', 47),\n",
       " ('waiter', 47),\n",
       " ('key', 47),\n",
       " ('cover', 47),\n",
       " ('hasn', 47),\n",
       " ('dancing', 47),\n",
       " ('kissed', 47),\n",
       " ('pictures', 47),\n",
       " ('tomorrow_night', 47),\n",
       " ('single', 46),\n",
       " ('sort', 46),\n",
       " ('yep', 46),\n",
       " ('white', 46),\n",
       " ('move', 46),\n",
       " ('i', 46),\n",
       " ('ok', 46),\n",
       " ('owe', 46),\n",
       " ('divorce', 46),\n",
       " ('crap', 46),\n",
       " ('angrily', 46),\n",
       " ('ones', 46),\n",
       " ('video', 46),\n",
       " ('tired', 46),\n",
       " ('sounds_like', 45),\n",
       " ('angry', 45),\n",
       " ('seconds', 45),\n",
       " ('some', 45),\n",
       " ('ohhh', 45),\n",
       " ('month', 45),\n",
       " ('trouble', 45),\n",
       " ('breasts', 45),\n",
       " ('these', 45),\n",
       " ('band', 45),\n",
       " ('imagine', 45),\n",
       " ('invite', 45),\n",
       " ('speech', 45),\n",
       " ('bye_bye', 45),\n",
       " ('days_lives', 45),\n",
       " ('ralph_lauren', 45),\n",
       " ('stopped', 44),\n",
       " ('sit_down', 44),\n",
       " ('park', 44),\n",
       " ('museum', 44),\n",
       " ('raise', 44),\n",
       " ('pushes', 44),\n",
       " ('notice', 44),\n",
       " ('come_on', 44),\n",
       " ('grade', 44),\n",
       " ('sense', 44),\n",
       " ('early', 44),\n",
       " ('ride', 44),\n",
       " ('keeps', 44),\n",
       " ('checking', 44),\n",
       " ('divorced', 44),\n",
       " ('propose', 44),\n",
       " ('needed', 44),\n",
       " ('arms', 44),\n",
       " ('sets', 44),\n",
       " ('drive', 44),\n",
       " ('having_sex', 43),\n",
       " ('hoping', 43),\n",
       " ('roll', 43),\n",
       " ('news', 43),\n",
       " ('share', 43),\n",
       " ('works', 43),\n",
       " ('future', 43),\n",
       " ('bar', 43),\n",
       " ('romantic', 43),\n",
       " ('wonder', 43),\n",
       " ('invited', 43),\n",
       " ('assistant', 43),\n",
       " ('year_old', 43),\n",
       " ('need_talk', 43),\n",
       " ('jacket', 43),\n",
       " ('loves', 43),\n",
       " ('frank', 43),\n",
       " ('lesbian', 42),\n",
       " ('picking', 42),\n",
       " ('herself', 42),\n",
       " ('ahead', 42),\n",
       " ('slowly', 42),\n",
       " ('gang', 42),\n",
       " ('company', 42),\n",
       " ('comfortable', 42),\n",
       " ('daughter', 42),\n",
       " ('speak', 42),\n",
       " ('through', 42),\n",
       " ('worth', 42),\n",
       " ('jumping', 42),\n",
       " ('covers', 42),\n",
       " ('short', 42),\n",
       " ('rules', 42),\n",
       " ('explain', 42),\n",
       " ('plans', 42),\n",
       " ('guitar', 42),\n",
       " ('switch', 42),\n",
       " ('neck', 41),\n",
       " ('lived', 41),\n",
       " ('shoe', 41),\n",
       " ('legs', 41),\n",
       " ('known', 41),\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### find the top words for the vocabulary###\n",
    "top_words = Counter(word for new_lines in new_lines for word in new_lines)\n",
    "vocab = sorted(top_words.items(),key=lambda x:x[1],reverse = True)[:15000] \n",
    "vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training (8 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Gensim implementation of Word2Vec to train a model on the quotes. The training can be divided into 3 stages:\n",
    "\n",
    "1. Set up and configure your model. Define the parameters in such a way that the following conditions are satisfied:\n",
    "    - Ignores all words that have a total absolute frequency less than 2\n",
    "    - Dimensions of the embeddings: 100 \n",
    "    - Initial learning rate of 0.03 \n",
    "    - 20 negative samples \n",
    "    - Window size 3 \n",
    "    - The learning rate in the training will decrease as you apply more and more updates. Most of the time when starting with gradient descent the initial steps can be larger, and as we get close to the local minima it is best to use smaller steps. This adjustment is done internally using a learning rate scheduler. Make sure that the smallest learning rate does not go below 0.0001.\n",
    "    - Set the threshold for configuring which higher-frequency words are randomly down-sampled to 6e-5. This parameter forces the sampling to choose the very frequent words less often in the sampling.\n",
    "    - Set the hashfunction of the word2vec to the given function.\n",
    "    - Train on a single worker to make sure you get the same result as ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash(astring):\n",
    "    return ord(astring[0])\n",
    "\n",
    "w2v = ### your code ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Before training, Word2Vec requires us to build the vocabulary table by filtering out the unique words and doing some basic counts on them. If you look at the logs you can see the effect of `min_count` and `sample` on the word corpus. Use the `build_vocab` function to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.### your code ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finally, we get to train the model. Train the model for 100 epochs. This will take a while. As we do not plan to train the model any further, we call `init_sims()`, which will make the model much more memory-efficient by precomputing L2-norms of word weight vectors for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.### your code ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploration (4 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lecture, word embeddings are suited for similarity and analogy tasks. Let's explore some of that with our dataset: \n",
    "\n",
    "We look for the most similar words to the famous coffee shop where most of the episodes took place, namely `central_perk` and also for the ones similar to the character `joey`. If you have followed the exercise correctly until now, you should see that words like `laying` are similar to `central_perk` and the other main characters are also considered similar to `joey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###your code###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###your code###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the similarity of `mrs_green` to `rachel` (her mom) and `ross`  to `spaceship` (unrelated). The first one should have a high score whereas the second should have a low score. Finally look at the similarity of `smelly_cat` (a song from pheobe) and `song`, which should have a high value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###your code###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###your code###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###your code###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask our model to give us the word that does not belong to a list of words. Let's see from the list of all 5 characters which one is the most dissimilar? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_names= [\"joey\", \"rachel\", \"phoebe\", \"monica\", \"chandler\"]\n",
    "w2v.###your code###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on analogies: Which word is to `rachel` as `man` is to `women`? (print the top 3); you should get `chandler` and `monica` among the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.###your code###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use t-SNE to look at the distribution of our embeddings in the vector space for the character `joey`. Follow the instructions and fill in the blank in the `tsneplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsneplot(model: Type[Word2Vec], word: str):\n",
    "    \"\"\" \n",
    "    Uses seaborn to plot the results from the t-SNE dimensionality reduction for the top 10 most similar and dissimiliar words. \n",
    "    \"\"\"\n",
    "    embs = np.empty((0, 100), dtype=\"f\")    # to save all the embeddings\n",
    "    word_labels = [word]\n",
    "    color_list  = [\"green\"]\n",
    "\n",
    "    embs = np.append(   # adds the vector of the query word\n",
    "    \n",
    "    close_words = model.wv.# gets list of most similar words\n",
    "    all_sims = model.# gets list of most dissimilar words (get the sorted list of all the words and their similarity and choose the bottom 10)\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = # get the vector\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append(\"blue\")\n",
    "        embs = np.append(embs, wrd_vector, axis=0)\n",
    "        \n",
    "    # adds the vector for each of the furthest words to the array\n",
    "    for wrd_score in far_words:\n",
    "        wrd_vector = # get the vector\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append(\"red\")\n",
    "        embs = np.append(embs, wrd_vector, axis=0)\n",
    "    \n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = TSNE(   # with  n_components=2, learning_rate=200, random_state=42, perplexity=15, init=\"random\"\n",
    "    \n",
    "    # sets everything up to plot\n",
    "    df = pd.DataFrame({\"x\": [x for x in Y[:, 0]],\n",
    "                       \"y\": [y for y in Y[:, 1]],\n",
    "                       \"words\": word_labels,\n",
    "                       \"color\": color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    \n",
    "    # basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={\"s\": 40, \"facecolors\": df[\"color\"]}\n",
    "                    )\n",
    "    \n",
    "    # adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df[\"y\"][line],\n",
    "                 \"  \" + df[\"words\"][line].title(),\n",
    "                 horizontalalignment=\"left\",\n",
    "                 verticalalignment=\"bottom\", size=\"medium\",\n",
    "                 color=df[\"color\"][line],\n",
    "                 weight=\"normal\"\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title(\"t-SNE visualization for {}\".format(word.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsneplot(w2v, \"joey\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Multi-class Classification (1 + 3 + 2 = 6 points)\n",
    "In this task, we aim to classify consumer finance complaints into 12 pre-defined classes. Note that this is not a multi-label task, and we assume that each new complaint is assigned to one and only one category. The data comes from https://www.data.gov/ (US government’s open data) and contains complaints that are published after the company responds, confirming a commercial relationship with the consumer, or after 15 days, whatever comes first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 1: Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data to a pandas dataframe from `complaints.csv` (also located in the `data` folder); this is a rather large file of 206MB. Keep only the `Consumer complaint narrative` (input text) and `product` (labels). Remove the missing values, rename `Consumer complaint narrative` to `Narrative` for ease of use, and add a column encoding the product as an integer. This will represent your labels for classification and the mapping will be used later on. Create two dictionaries: one mapping the ids to products and one mapping products to their ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "      <td>I was sold access to an event digitally, of wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product  \\\n",
       "0                        Checking or savings account   \n",
       "1  Money transfer, virtual currency, or money ser...   \n",
       "2                              Vehicle loan or lease   \n",
       "3                        Checking or savings account   \n",
       "4                        Checking or savings account   \n",
       "\n",
       "                        Consumer complaint narrative  \n",
       "0                                                NaN  \n",
       "1  I was sold access to an event digitally, of wh...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "df1 = pd.read_csv('/Users/anureddy/Desktop/Sem01/DataScience_for_text_analytics/Assignments/Assignment03/Assignment_03/data/complaints.csv')\n",
    "df =  df1[['Product','Consumer complaint narrative']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/ksgfnxyn4jg66j5nhtx4v0mc0000gn/T/ipykernel_23074/1466441373.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={\"Consumer complaint narrative\": \"narrative\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#rename the column \n",
    "df.rename(columns={\"Consumer complaint narrative\": \"narrative\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(559381, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/ksgfnxyn4jg66j5nhtx4v0mc0000gn/T/ipykernel_23074/656685156.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "/var/folders/w9/ksgfnxyn4jg66j5nhtx4v0mc0000gn/T/ipykernel_23074/656685156.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['product_id'] = le.fit_transform(df['Product'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>narrative</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "      <td>I was sold access to an event digitally, of wh...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>I LEASED MY CAR ON XX/XX/XXXX AND MADE PAYMENT...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My original lender was XXXX XXXX XXXX ; this l...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I was misled by XXXX XXXX  and my degree did n...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>I purchased a vehicle 3 years ago. As of recen...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Product  \\\n",
       "1   Money transfer, virtual currency, or money ser...   \n",
       "5                               Vehicle loan or lease   \n",
       "6                                        Student loan   \n",
       "8                                        Student loan   \n",
       "10                              Vehicle loan or lease   \n",
       "\n",
       "                                            narrative  product_id  \n",
       "1   I was sold access to an event digitally, of wh...           5  \n",
       "5   I LEASED MY CAR ON XX/XX/XXXX AND MADE PAYMENT...          12  \n",
       "6   My original lender was XXXX XXXX XXXX ; this l...          11  \n",
       "8   I was misled by XXXX XXXX  and my degree did n...          11  \n",
       "10  I purchased a vehicle 3 years ago. As of recen...          12  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df['product_id'] = le.fit_transform(df['Product'])\n",
    "### create the dictionary #### \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the class distribution of the products. It is always a good idea to look at the relative number of instances for each class before performing any classification task. Use the `plot` function from the dataframe to show the number of instances in each class in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>narrative</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500658</th>\n",
       "      <td>Virtual currency</td>\n",
       "      <td>Coinbase account closed without reasonable exp...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485397</th>\n",
       "      <td>Virtual currency</td>\n",
       "      <td>Money was going to be transferred from XXXX to...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365601</th>\n",
       "      <td>Virtual currency</td>\n",
       "      <td>Coinbase rep, XXXX has been being an XXXX to m...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481443</th>\n",
       "      <td>Virtual currency</td>\n",
       "      <td>I first initiated a support ticket trying to r...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124073</th>\n",
       "      <td>Virtual currency</td>\n",
       "      <td>Hi, on XXXX XXXX, I paid XXXX $ in XXXX to my ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434230</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>Wells Fargo Bank legal department put my accou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434231</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>As described in Wells Fargo ( WF ) web site, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434238</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>My account became over drawn for around {$200....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434239</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>I am a senior citizen who has been vulnerable ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481476</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>SYNCB/JCP ACCOUNT NUMBERs : XXXX XXXX/XXXX/XXX...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165809 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Product  \\\n",
       "500658         Virtual currency   \n",
       "485397         Virtual currency   \n",
       "365601         Virtual currency   \n",
       "481443         Virtual currency   \n",
       "124073         Virtual currency   \n",
       "...                         ...   \n",
       "434230  Bank account or service   \n",
       "434231  Bank account or service   \n",
       "434238  Bank account or service   \n",
       "434239  Bank account or service   \n",
       "481476  Bank account or service   \n",
       "\n",
       "                                                narrative  product_id  \n",
       "500658  Coinbase account closed without reasonable exp...          13  \n",
       "485397  Money was going to be transferred from XXXX to...          13  \n",
       "365601  Coinbase rep, XXXX has been being an XXXX to m...          13  \n",
       "481443  I first initiated a support ticket trying to r...          13  \n",
       "124073  Hi, on XXXX XXXX, I paid XXXX $ in XXXX to my ...          13  \n",
       "...                                                   ...         ...  \n",
       "434230  Wells Fargo Bank legal department put my accou...           0  \n",
       "434231  As described in Wells Fargo ( WF ) web site, a...           0  \n",
       "434238  My account became over drawn for around {$200....           0  \n",
       "434239  I am a senior citizen who has been vulnerable ...           0  \n",
       "481476  SYNCB/JCP ACCOUNT NUMBERs : XXXX XXXX/XXXX/XXX...           0  \n",
       "\n",
       "[165809 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('product_id',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     31588\n",
       "11    27299\n",
       "1     27016\n",
       "3     18838\n",
       "0     14885\n",
       "5     11875\n",
       "12    11334\n",
       "2      9473\n",
       "9      8500\n",
       "8      1746\n",
       "6      1497\n",
       "10     1450\n",
       "7       292\n",
       "13       16\n",
       "Name: product_id, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.product_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>narrative</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9575</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>I have recently submitted a complaint with Equ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9598</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>I have written several letters to date regardi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10819</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>I contacted the Bankruptcy court asking their ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11138</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>I am a victim of identity theft. I have faxed ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11146</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Equifax, XXXX and XXXX credit reporting agency...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559352</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>I went down to the courthouse and spoke to the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559364</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>I have been a victim if inquiry issue,</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559365</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>I submitted all required information for ident...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559366</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>I was trying to get my credit reports from all...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559374</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Upon trying to obtain a credit report at a car...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31588 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Product                                          narrative  \\\n",
       "9575    Credit reporting  I have recently submitted a complaint with Equ...   \n",
       "9598    Credit reporting  I have written several letters to date regardi...   \n",
       "10819   Credit reporting  I contacted the Bankruptcy court asking their ...   \n",
       "11138   Credit reporting  I am a victim of identity theft. I have faxed ...   \n",
       "11146   Credit reporting  Equifax, XXXX and XXXX credit reporting agency...   \n",
       "...                  ...                                                ...   \n",
       "559352  Credit reporting  I went down to the courthouse and spoke to the...   \n",
       "559364  Credit reporting             I have been a victim if inquiry issue,   \n",
       "559365  Credit reporting  I submitted all required information for ident...   \n",
       "559366  Credit reporting  I was trying to get my credit reports from all...   \n",
       "559374  Credit reporting  Upon trying to obtain a credit report at a car...   \n",
       "\n",
       "        product_id  \n",
       "9575             4  \n",
       "9598             4  \n",
       "10819            4  \n",
       "11138            4  \n",
       "11146            4  \n",
       "...            ...  \n",
       "559352           4  \n",
       "559364           4  \n",
       "559365           4  \n",
       "559366           4  \n",
       "559374           4  \n",
       "\n",
       "[31588 rows x 3 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.product_id == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGzCAYAAADNKAZOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvT0lEQVR4nO3dfVjUdb7/8RcgDDcyKJog6w2sdVLSNCGUbtUQMq5aNyvbPEVqtbnQEbkuTfcU3raWe7wrKXIzaU951tqz3Wmrsni3HfEOpNTSrV02O8cA21QMFSbm+/tjf0xOAygIDHx4Pq6r62q+3898vu9585nxdc18vzM+lmVZAgAAMIyvtwsAAABoDYQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAHUpeXp58fHz097//3duleNi+fbt8fHy0ffv2i44dNWqURo0a1eo1AZ0ZIQcALnD8+HHNmzdPJSUl3i4FwGXq4u0CAKA9OX78uObPn6/o6GgNGzasSfe95ZZbdO7cOQUEBLROcQCahHdyALQJp9Op8+fPe7uMVuXr66vAwED5+vLSCrQHPBMBNMm8efPk4+OjI0eO6L777pPdblePHj00ffp0txDj4+OjjIwMvfHGG7rmmmtks9m0adMmSdKBAwc0btw42e12de3aVbfddpt2797tcazDhw9rzJgxCgoKUp8+fbRo0SI5nU6PcT4+Ppo3b57H9ujoaD388MNu206dOqUZM2YoOjpaNptNffr00UMPPaSvv/5a27dv1/XXXy9Jmjx5snx8fOTj46O8vLxL6k1D5+SsXr1aAwYMUFBQkBISEvTnP//5kuYDcHn4uApAs9x3332Kjo7W4sWLtXv3bj3//PM6efKkfvvb37rGbN26VW+++aYyMjLUs2dPRUdH6/Dhw7r55ptlt9s1a9Ys+fv76+WXX9aoUaO0Y8cOjRgxQpJUVlam0aNH67vvvtPs2bMVEhKi1atXKygoqNk1f/vtt7r55pv16aefasqUKRo+fLi+/vprvffee/rf//1fDRo0SAsWLFB2drYee+wx3XzzzZKkG264odnHXLNmjX7+85/rhhtuUGZmpv72t7/prrvuUnh4uPr27dvseQFcAgsAmmDu3LmWJOuuu+5y2/6LX/zCkmR99NFHlmVZliTL19fXOnz4sNu48ePHWwEBAdZf//pX17bjx49boaGh1i233OLalpmZaUmy9uzZ49pWUVFhhYWFWZKs0tJS13ZJ1ty5cz1q7d+/v5WWlua6nZ2dbUmy/vCHP3iMdTqdlmVZ1r59+yxJ1tq1ay/aix/atm2bJcnatm2bZVmWVVNTY/Xq1csaNmyYVV1d7Rq3evVqS5J16623NvkYAC4dH1cBaJb09HS320888YQk6YMPPnBtu/XWWxUbG+u6XVtbqy1btmj8+PH68Y9/7Nreu3dvPfDAA/rwww9VWVnpmmfkyJFKSEhwjbviiis0adKkZtf83//93xo6dKh++tOfeuzz8fFp9rwN2b9/vyoqKvT444+7nYz88MMPKywsrMWPB8AdIQdAs1x11VVutwcMGCBfX1+376+JiYlxG3PixAmdPXtWV199tcd8gwYNktPp1JdffilJ+uKLLzyOIane+16qv/71rxo8eHCz799UX3zxhSTPXvn7+7uFPACtg5ADoEXU907I5Zw/0xJqa2u9enwA3kXIAdAsn332mdvtzz//XE6nU9HR0Q3e54orrlBwcLCOHj3qse/IkSPy9fV1nYzbv39/j2NIqve+3bt316lTp9y21dTU6KuvvnLbNmDAAB06dKjB+qSW/diqf//+kjx75XA4VFpa2mLHAVA/Qg6AZsnJyXG7/cILL0iSxo0b1+B9/Pz8lJycrHfffdftY63y8nKtW7dON910k+x2uyTpjjvu0O7du7V3717XuBMnTuiNN97wmHfAgAHauXOn27bVq1d7vJMzYcIEffTRR3r77bc95rAsS5IUEhIiSR6hqTni4+N1xRVXKDc3VzU1Na7teXl5LTI/gMZxCTmAZiktLdVdd92l22+/XYWFhXr99df1wAMPaOjQoY3eb9GiRcrPz9dNN92kX/ziF+rSpYtefvllVVdXa8mSJa5xs2bN0n/+53/q9ttv1/Tp012XkPfv318ff/yx25yPPPKIHn/8cU2YMEFjx47VRx99pM2bN6tnz55u42bOnKnf//73uvfeezVlyhTFxcXpm2++0Xvvvafc3FwNHTpUAwYMULdu3ZSbm6vQ0FCFhIRoxIgRHucXXQp/f38tWrRIP//5zzVmzBhNnDhRpaWlWrt2LefkAG3B25d3AehY6i4h/+STT6x77rnHCg0Ntbp3725lZGRY586dc42TZKWnp9c7R3FxsZWSkmJ17drVCg4OtkaPHm3t2rXLY9zHH39s3XrrrVZgYKD1ox/9yFq4cKG1Zs0aj0vIa2trrSeffNLq2bOnFRwcbKWkpFiff/65xyXklmVZ//jHP6yMjAzrRz/6kRUQEGD16dPHSktLs77++mvXmHfffdeKjY21unTp0qTLyX94CXmdF1980YqJibFsNpsVHx9v7dy507r11lu5hBxoZT6W9f/fowWASzBv3jzNnz9fJ06c8HinBADaE87JAQAARuKcHAC4iHPnzun06dONjgkPD+fXx4F2hpADABexfv16TZ48udEx27Zt06hRo9qmIACXhHNyAOAivvrqKx0+fLjRMXFxcerevXsbVQTgUhByAACAkTjxGAAAGKlTn5PjdDp1/PhxhYaGtsovEAMAgJZnWZbOnDmjqKgo+fo2/H5Npw45x48fd/1ODgAA6Fi+/PJL9enTp8H9nTrkhIaGSvpnk+p+L6clOBwObdmyRcnJyfL392+xeTs6+uKJnniiJ/WjL57oiafO0pPKykr17dvX9e94Qzp1yKn7iMput7d4yAkODpbdbjd6kTUVffFETzzRk/rRF0/0xFNn68nFTjXhxGMAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAI3XxdgFAZxM9e6Pr/21+lpYkSIPnbVZ1rY8Xq7q4vz+b6u0SAKBJeCcHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACPxA52tqCP86OKF+AFGAIBJeCcHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIx0WSHn2WeflY+PjzIzM13bzp8/r/T0dPXo0UNdu3bVhAkTVF5e7na/Y8eOKTU1VcHBwerVq5dmzpyp7777zm3M9u3bNXz4cNlsNl155ZXKy8vzOH5OTo6io6MVGBioESNGaO/evZfzcAAAgEGaHXL27dunl19+Wddee63b9hkzZuj999/XW2+9pR07duj48eO6++67Xftra2uVmpqqmpoa7dq1S6+99pry8vKUnZ3tGlNaWqrU1FSNHj1aJSUlyszM1COPPKLNmze7xqxfv15ZWVmaO3euiouLNXToUKWkpKiioqK5DwkAABikWSHn22+/1aRJk/Sb3/xG3bt3d20/ffq01qxZo2XLlmnMmDGKi4vT2rVrtWvXLu3evVuStGXLFn3yySd6/fXXNWzYMI0bN04LFy5UTk6OampqJEm5ubmKiYnR0qVLNWjQIGVkZOiee+7R8uXLXcdatmyZHn30UU2ePFmxsbHKzc1VcHCwXn311cvpBwAAMESzfoU8PT1dqampSkpK0qJFi1zbi4qK5HA4lJSU5No2cOBA9evXT4WFhRo5cqQKCws1ZMgQRUREuMakpKRo2rRpOnz4sK677joVFha6zVE3pu5jsZqaGhUVFWnOnDmu/b6+vkpKSlJhYWGDdVdXV6u6utp1u7KyUpLkcDjkcDia04p61c1l87VabM620JI9aGz+1j5Oe2fz+35d1K2RjrBW2urvxjqpH33xRE88dZaeXOrja3LI+d3vfqfi4mLt27fPY19ZWZkCAgLUrVs3t+0REREqKytzjbkw4NTtr9vX2JjKykqdO3dOJ0+eVG1tbb1jjhw50mDtixcv1vz58z22b9myRcHBwQ3er7kWxjtbfM7W9MEHH7TJcfLz89vkOO3VkgTPbR1hrbTV+qjT2ddJQ+iLJ3riyfSenD179pLGNSnkfPnll5o+fbry8/MVGBjYrMK8ac6cOcrKynLdrqysVN++fZWcnCy73d5ix3E4HMrPz9fT+31V7fRpsXlb26F5Ka06f11fxo4dK39//1Y9Vns2eN7355bZfC0tjHd2iLXS2uujDuukfvTFEz3x1Fl6UvdJzMU0KeQUFRWpoqJCw4cPd22rra3Vzp07tWrVKm3evFk1NTU6deqU27s55eXlioyMlCRFRkZ6XAVVd/XVhWN+eEVWeXm57Ha7goKC5OfnJz8/v3rH1M1RH5vNJpvN5rHd39+/VRZDtdNH1bXt+x+uC7XVE6K1+t1R1LcmOsJaaeu/WWdfJw2hL57oiSfTe3Kpj61JJx7fdtttOnjwoEpKSlz/xcfHa9KkSa7/9/f3V0FBges+R48e1bFjx5SYmChJSkxM1MGDB92ugsrPz5fdbldsbKxrzIVz1I2pmyMgIEBxcXFuY5xOpwoKClxjAABA59akd3JCQ0M1ePBgt20hISHq0aOHa/vUqVOVlZWl8PBw2e12PfHEE0pMTNTIkSMlScnJyYqNjdWDDz6oJUuWqKysTE899ZTS09Nd77I8/vjjWrVqlWbNmqUpU6Zo69atevPNN7Vx40bXcbOyspSWlqb4+HglJCRoxYoVqqqq0uTJky+rIQAAwAzNurqqMcuXL5evr68mTJig6upqpaSk6MUXX3Tt9/Pz04YNGzRt2jQlJiYqJCREaWlpWrBggWtMTEyMNm7cqBkzZmjlypXq06ePXnnlFaWkfH9OwMSJE3XixAllZ2errKxMw4YN06ZNmzxORgYAAJ3TZYec7du3u90ODAxUTk6OcnJyGrxP//79L3qlxqhRo3TgwIFGx2RkZCgjI+OSawUAAJ0Hv10FAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgpCaFnJdeeknXXnut7Ha77Ha7EhMT9cc//tG1//z580pPT1ePHj3UtWtXTZgwQeXl5W5zHDt2TKmpqQoODlavXr00c+ZMfffdd25jtm/fruHDh8tms+nKK69UXl6eRy05OTmKjo5WYGCgRowYob179zbloQAAAMM1KeT06dNHzz77rIqKirR//36NGTNGP/nJT3T48GFJ0owZM/T+++/rrbfe0o4dO3T8+HHdfffdrvvX1tYqNTVVNTU12rVrl1577TXl5eUpOzvbNaa0tFSpqakaPXq0SkpKlJmZqUceeUSbN292jVm/fr2ysrI0d+5cFRcXa+jQoUpJSVFFRcXl9gMAABiiSSHnzjvv1B133KGrrrpK//Iv/6JnnnlGXbt21e7du3X69GmtWbNGy5Yt05gxYxQXF6e1a9dq165d2r17tyRpy5Yt+uSTT/T6669r2LBhGjdunBYuXKicnBzV1NRIknJzcxUTE6OlS5dq0KBBysjI0D333KPly5e76li2bJkeffRRTZ48WbGxscrNzVVwcLBeffXVFmwNAADoyLo09461tbV66623VFVVpcTERBUVFcnhcCgpKck1ZuDAgerXr58KCws1cuRIFRYWasiQIYqIiHCNSUlJ0bRp03T48GFdd911KiwsdJujbkxmZqYkqaamRkVFRZozZ45rv6+vr5KSklRYWNhozdXV1aqurnbdrqyslCQ5HA45HI7mtsJD3Vw2X6vF5mwLLdmDxuZv7eO0dza/79dF3RrpCGulrf5urJP60RdP9MRTZ+nJpT6+JoecgwcPKjExUefPn1fXrl319ttvKzY2ViUlJQoICFC3bt3cxkdERKisrEySVFZW5hZw6vbX7WtsTGVlpc6dO6eTJ0+qtra23jFHjhxptPbFixdr/vz5Htu3bNmi4ODgiz/4JloY72zxOVvTBx980CbHyc/Pb5PjtFdLEjy3dYS10lbro05nXycNoS+e6Ikn03ty9uzZSxrX5JBz9dVXq6SkRKdPn9bvf/97paWlaceOHU0u0BvmzJmjrKws1+3Kykr17dtXycnJstvtLXYch8Oh/Px8Pb3fV9VOnxabt7UdmpfSqvPX9WXs2LHy9/dv1WO1Z4PnfX9+mc3X0sJ4Z4dYK629PuqwTupHXzzRE0+dpSd1n8RcTJNDTkBAgK688kpJUlxcnPbt26eVK1dq4sSJqqmp0alTp9zezSkvL1dkZKQkKTIy0uMqqLqrry4c88MrssrLy2W32xUUFCQ/Pz/5+fnVO6ZujobYbDbZbDaP7f7+/q2yGKqdPqqubd//cF2orZ4QrdXvjqK+NdER1kpb/806+zppCH3xRE88md6TS31sl/09OU6nU9XV1YqLi5O/v78KCgpc+44ePapjx44pMTFRkpSYmKiDBw+6XQWVn58vu92u2NhY15gL56gbUzdHQECA4uLi3MY4nU4VFBS4xgAAADTpnZw5c+Zo3Lhx6tevn86cOaN169Zp+/bt2rx5s8LCwjR16lRlZWUpPDxcdrtdTzzxhBITEzVy5EhJUnJysmJjY/Xggw9qyZIlKisr01NPPaX09HTXOyyPP/64Vq1apVmzZmnKlCnaunWr3nzzTW3cuNFVR1ZWltLS0hQfH6+EhAStWLFCVVVVmjx5cgu2BgAAdGRNCjkVFRV66KGH9NVXXyksLEzXXnutNm/erLFjx0qSli9fLl9fX02YMEHV1dVKSUnRiy++6Lq/n5+fNmzYoGnTpikxMVEhISFKS0vTggULXGNiYmK0ceNGzZgxQytXrlSfPn30yiuvKCXl+/MBJk6cqBMnTig7O1tlZWUaNmyYNm3a5HEyMgAA6LyaFHLWrFnT6P7AwEDl5OQoJyenwTH9+/e/6FUao0aN0oEDBxodk5GRoYyMjEbHAACAzovfrgIAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYqYu3CwAAoK1Fz97o7RKa7O/Ppnq7hA6Hd3IAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIzUpJCzePFiXX/99QoNDVWvXr00fvx4HT161G3M+fPnlZ6erh49eqhr166aMGGCysvL3cYcO3ZMqampCg4OVq9evTRz5kx99913bmO2b9+u4cOHy2az6corr1ReXp5HPTk5OYqOjlZgYKBGjBihvXv3NuXhAAAAgzUp5OzYsUPp6enavXu38vPz5XA4lJycrKqqKteYGTNm6P3339dbb72lHTt26Pjx47r77rtd+2tra5Wamqqamhrt2rVLr732mvLy8pSdne0aU1paqtTUVI0ePVolJSXKzMzUI488os2bN7vGrF+/XllZWZo7d66Ki4s1dOhQpaSkqKKi4nL6AQAADNGlKYM3bdrkdjsvL0+9evVSUVGRbrnlFp0+fVpr1qzRunXrNGbMGEnS2rVrNWjQIO3evVsjR47Uli1b9Mknn+hPf/qTIiIiNGzYMC1cuFBPPvmk5s2bp4CAAOXm5iomJkZLly6VJA0aNEgffvihli9frpSUFEnSsmXL9Oijj2ry5MmSpNzcXG3cuFGvvvqqZs+efdmNAQAAHVuTQs4PnT59WpIUHh4uSSoqKpLD4VBSUpJrzMCBA9WvXz8VFhZq5MiRKiws1JAhQxQREeEak5KSomnTpunw4cO67rrrVFhY6DZH3ZjMzExJUk1NjYqKijRnzhzXfl9fXyUlJamwsLDBequrq1VdXe26XVlZKUlyOBxyOBzN7IKnurlsvlaLzdkWWrIHjc3f2sdp72x+36+LujXSEdZKW/3dWCf1oy+eLqcnFz4PO4pLeZydZZ1c6uNrdshxOp3KzMzUjTfeqMGDB0uSysrKFBAQoG7durmNjYiIUFlZmWvMhQGnbn/dvsbGVFZW6ty5czp58qRqa2vrHXPkyJEGa168eLHmz5/vsX3Lli0KDg6+hEfdNAvjnS0+Z2v64IMP2uQ4+fn5bXKc9mpJgue2jrBW2mp91Ons66Qh9MVTc3pS3/OwvWvKc9D0dXL27NlLGtfskJOenq5Dhw7pww8/bO4UbW7OnDnKyspy3a6srFTfvn2VnJwsu93eYsdxOBzKz8/X0/t9Ve30abF5W9uheSmtOn9dX8aOHSt/f/9WPVZ7Nnje9+eW2XwtLYx3doi10trrow7rpH70xdPl9OTC52FHcSnPwc6yTuo+ibmYZoWcjIwMbdiwQTt37lSfPn1c2yMjI1VTU6NTp065vZtTXl6uyMhI15gfXgVVd/XVhWN+eEVWeXm57Ha7goKC5OfnJz8/v3rH1M1RH5vNJpvN5rHd39+/VRZDtdNH1bXt+x+uC7XVE6K1+t1R1LcmOsJaaeu/WWdfJw2hL56a05P2/nyrT1Meo+nr5FIfW5OurrIsSxkZGXr77be1detWxcTEuO2Pi4uTv7+/CgoKXNuOHj2qY8eOKTExUZKUmJiogwcPul0FlZ+fL7vdrtjYWNeYC+eoG1M3R0BAgOLi4tzGOJ1OFRQUuMYAAIDOrUnv5KSnp2vdunV69913FRoa6jqHJiwsTEFBQQoLC9PUqVOVlZWl8PBw2e12PfHEE0pMTNTIkSMlScnJyYqNjdWDDz6oJUuWqKysTE899ZTS09Nd77I8/vjjWrVqlWbNmqUpU6Zo69atevPNN7Vx40ZXLVlZWUpLS1N8fLwSEhK0YsUKVVVVua62AgAAnVuTQs5LL70kSRo1apTb9rVr1+rhhx+WJC1fvly+vr6aMGGCqqurlZKSohdffNE11s/PTxs2bNC0adOUmJiokJAQpaWlacGCBa4xMTEx2rhxo2bMmKGVK1eqT58+euWVV1yXj0vSxIkTdeLECWVnZ6usrEzDhg3Tpk2bPE5GBgAAnVOTQo5lXfySu8DAQOXk5CgnJ6fBMf3797/oWeKjRo3SgQMHGh2TkZGhjIyMi9YEAAA6H367CgAAGImQAwAAjETIAQAARrqsn3UAvC169saLDwIAdEq8kwMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAI3XxdgEAAODiomdvvOgYm5+lJQnS4HmbVV3r0wZVNe7vz6Z69fi8kwMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYKQmh5ydO3fqzjvvVFRUlHx8fPTOO++47bcsS9nZ2erdu7eCgoKUlJSkzz77zG3MN998o0mTJslut6tbt26aOnWqvv32W7cxH3/8sW6++WYFBgaqb9++WrJkiUctb731lgYOHKjAwEANGTJEH3zwQVMfDgAAMFSTQ05VVZWGDh2qnJycevcvWbJEzz//vHJzc7Vnzx6FhIQoJSVF58+fd42ZNGmSDh8+rPz8fG3YsEE7d+7UY4895tpfWVmp5ORk9e/fX0VFRfr1r3+tefPmafXq1a4xu3bt0s9+9jNNnTpVBw4c0Pjx4zV+/HgdOnSoqQ8JAAAYqMlfBjhu3DiNGzeu3n2WZWnFihV66qmn9JOf/ESS9Nvf/lYRERF65513dP/99+vTTz/Vpk2btG/fPsXHx0uSXnjhBd1xxx36j//4D0VFRemNN95QTU2NXn31VQUEBOiaa65RSUmJli1b5gpDK1eu1O23366ZM2dKkhYuXKj8/HytWrVKubm5zWoGAHhbQ1/41t6+5O1C3v7CN6AhLfqNx6WlpSorK1NSUpJrW1hYmEaMGKHCwkLdf//9KiwsVLdu3VwBR5KSkpLk6+urPXv26Kc//akKCwt1yy23KCAgwDUmJSVFzz33nE6ePKnu3bursLBQWVlZbsdPSUnx+PjsQtXV1aqurnbdrqyslCQ5HA45HI7LffgudXPZfK0Wm7MttGQPGpu/JY9j8+tYPf6hujXSEdZKa6+PHx6nrY7X3jS0ptvzWvHW3+py1kpHf+1oSHtbJ621Ni513hYNOWVlZZKkiIgIt+0RERGufWVlZerVq5d7EV26KDw83G1MTEyMxxx1+7p3766ysrJGj1OfxYsXa/78+R7bt2zZouDg4Et5iE2yMN7Z4nO2prY6pyk/P7/F5lqS0GJTeVVHWCttfc5bS66TjuRia7o9rhVvnw/ZnLViymtHQ9rLOmmttXH27NlLGtepfrtqzpw5bu/+VFZWqm/fvkpOTpbdbm+x4zgcDuXn5+vp/b6qdravt5Ubc2heSqvOX9eXsWPHyt/fv0XmHDxvc4vM4y02X0sL450dYq209vqo0xrrpCNpaE2357XSVmvjhy5nrXT0146GtLd10lpro+6TmItp0ZATGRkpSSovL1fv3r1d28vLyzVs2DDXmIqKCrf7fffdd/rmm29c94+MjFR5ebnbmLrbFxtTt78+NptNNpvNY7u/v3+rvJhWO33a3WfnjWmrf1Bast8dqb+N6Qhrpa0DR2s9L9u7i62D9rhWvP13as5aaW89bGntZZ201tq41Hlb9HtyYmJiFBkZqYKCAte2yspK7dmzR4mJiZKkxMREnTp1SkVFRa4xW7duldPp1IgRI1xjdu7c6faZW35+vq6++mp1797dNebC49SNqTsOAADo3Joccr799luVlJSopKRE0j9PNi4pKdGxY8fk4+OjzMxMLVq0SO+9954OHjyohx56SFFRURo/frwkadCgQbr99tv16KOPau/evfqf//kfZWRk6P7771dUVJQk6YEHHlBAQICmTp2qw4cPa/369Vq5cqXbR03Tp0/Xpk2btHTpUh05ckTz5s3T/v37lZGRcfldAQAAHV6TP67av3+/Ro8e7bpdFzzS0tKUl5enWbNmqaqqSo899phOnTqlm266SZs2bVJgYKDrPm+88YYyMjJ02223ydfXVxMmTNDzzz/v2h8WFqYtW7YoPT1dcXFx6tmzp7Kzs92+S+eGG27QunXr9NRTT+mXv/ylrrrqKr3zzjsaPHhwsxqBhi9dbSnt+RJYAIB5mhxyRo0aJctq+NI0Hx8fLViwQAsWLGhwTHh4uNatW9foca699lr9+c9/bnTMvffeq3vvvbfxggEAQKfEb1cBAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGKmLtwsA0DFEz97YJsex+VlakiANnrdZ1bU+lzXX359NbaGqAHREhBwAwGVpqwD8Qy0ZiGEmPq4CAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADBShw85OTk5io6OVmBgoEaMGKG9e/d6uyQAANAOdOiQs379emVlZWnu3LkqLi7W0KFDlZKSooqKCm+XBgAAvKxDfxngsmXL9Oijj2ry5MmSpNzcXG3cuFGvvvqqZs+e7eXqAHibt76kDkD70GFDTk1NjYqKijRnzhzXNl9fXyUlJamwsLDe+1RXV6u6utp1+/Tp05Kkb775Rg6Ho8VqczgcOnv2rLo4fFXr5Fs463RxWjp71klfLkBPPNGT+tEXT/TEU3vryT/+8Y9WmffMmTOSJMuyGh3XYUPO119/rdraWkVERLhtj4iI0JEjR+q9z+LFizV//nyP7TExMa1SIzw94O0C2iF64ome1I++eKInntpTT3oubd35z5w5o7CwsAb3d9iQ0xxz5sxRVlaW67bT6dQ333yjHj16yMen5RJvZWWl+vbtqy+//FJ2u73F5u3o6IsneuKJntSPvniiJ546S08sy9KZM2cUFRXV6LgOG3J69uwpPz8/lZeXu20vLy9XZGRkvfex2Wyy2Wxu27p169ZaJcputxu9yJqLvniiJ57oSf3oiyd64qkz9KSxd3DqdNirqwICAhQXF6eCggLXNqfTqYKCAiUmJnqxMgAA0B502HdyJCkrK0tpaWmKj49XQkKCVqxYoaqqKtfVVgAAoPPq0CFn4sSJOnHihLKzs1VWVqZhw4Zp06ZNHicjtzWbzaa5c+d6fDTW2dEXT/TEEz2pH33xRE880RN3PtbFrr8CAADogDrsOTkAAACNIeQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQk4ryMnJUXR0tAIDAzVixAjt3bvX2yV5zeLFi3X99dcrNDRUvXr10vjx43X06FFvl9WuPPvss/Lx8VFmZqa3S/G6//u//9O//uu/qkePHgoKCtKQIUO0f/9+b5flNbW1tXr66acVExOjoKAgDRgwQAsXLrzojxKaZufOnbrzzjsVFRUlHx8fvfPOO277LctSdna2evfuraCgICUlJemzzz7zTrFtpLGeOBwOPfnkkxoyZIhCQkIUFRWlhx56SMePH/dewV5CyGlh69evV1ZWlubOnavi4mINHTpUKSkpqqio8HZpXrFjxw6lp6dr9+7dys/Pl8PhUHJysqqqqrxdWruwb98+vfzyy7r22mu9XYrXnTx5UjfeeKP8/f31xz/+UZ988omWLl2q7t27e7s0r3nuuef00ksvadWqVfr000/13HPPacmSJXrhhRe8XVqbqqqq0tChQ5WTk1Pv/iVLluj5559Xbm6u9uzZo5CQEKWkpOj8+fNtXGnbaawnZ8+eVXFxsZ5++mkVFxfrD3/4g44ePaq77rrLC5V6mYUWlZCQYKWnp7tu19bWWlFRUdbixYu9WFX7UVFRYUmyduzY4e1SvO7MmTPWVVddZeXn51u33nqrNX36dG+X5FVPPvmkddNNN3m7jHYlNTXVmjJlitu2u+++25o0aZKXKvI+Sdbbb7/tuu10Oq3IyEjr17/+tWvbqVOnLJvNZv3Xf/2XFypsez/sSX327t1rSbK++OKLtimqneCdnBZUU1OjoqIiJSUlubb5+voqKSlJhYWFXqys/Th9+rQkKTw83MuVeF96erpSU1Pd1ktn9t577yk+Pl733nuvevXqpeuuu06/+c1vvF2WV91www0qKCjQX/7yF0nSRx99pA8//FDjxo3zcmXtR2lpqcrKytyeR2FhYRoxYgSvuxc4ffq0fHx8WvVHqdujDv2zDu3N119/rdraWo+flYiIiNCRI0e8VFX74XQ6lZmZqRtvvFGDBw/2djle9bvf/U7FxcXat2+ft0tpN/72t7/ppZdeUlZWln75y19q3759+rd/+zcFBAQoLS3N2+V5xezZs1VZWamBAwfKz89PtbW1euaZZzRp0iRvl9ZulJWVSVK9r7t1+zq78+fP68knn9TPfvYz43+Z/IcIOWgz6enpOnTokD788ENvl+JVX375paZPn678/HwFBgZ6u5x2w+l0Kj4+Xr/61a8kSdddd50OHTqk3NzcThty3nzzTb3xxhtat26drrnmGpWUlCgzM1NRUVGdtidoGofDofvuu0+WZemll17ydjltjo+rWlDPnj3l5+en8vJyt+3l5eWKjIz0UlXtQ0ZGhjZs2KBt27apT58+3i7Hq4qKilRRUaHhw4erS5cu6tKli3bs2KHnn39eXbp0UW1trbdL9IrevXsrNjbWbdugQYN07NgxL1XkfTNnztTs2bN1//33a8iQIXrwwQc1Y8YMLV682NultRt1r6287nqqCzhffPGF8vPzO927OBIhp0UFBAQoLi5OBQUFrm1Op1MFBQVKTEz0YmXeY1mWMjIy9Pbbb2vr1q2KiYnxdkled9ttt+ngwYMqKSlx/RcfH69JkyappKREfn5+3i7RK2688UaPrxf4y1/+ov79+3upIu87e/asfH3dX6b9/PzkdDq9VFH7ExMTo8jISLfX3crKSu3Zs6fTvu5K3weczz77TH/605/Uo0cPb5fkFXxc1cKysrKUlpam+Ph4JSQkaMWKFaqqqtLkyZO9XZpXpKena926dXr33XcVGhrq+ow8LCxMQUFBXq7OO0JDQz3OSQoJCVGPHj069blKM2bM0A033KBf/epXuu+++7R3716tXr1aq1ev9nZpXnPnnXfqmWeeUb9+/XTNNdfowIEDWrZsmaZMmeLt0trUt99+q88//9x1u7S0VCUlJQoPD1e/fv2UmZmpRYsW6aqrrlJMTIyefvppRUVFafz48d4rupU11pPevXvrnnvuUXFxsTZs2KDa2lrXa294eLgCAgK8VXbb8/blXSZ64YUXrH79+lkBAQFWQkKCtXv3bm+X5DWS6v1v7dq13i6tXeES8n96//33rcGDB1s2m80aOHCgtXr1am+X5FWVlZXW9OnTrX79+lmBgYHWj3/8Y+vf//3frerqam+X1qa2bdtW7+tIWlqaZVn/vIz86aeftiIiIiybzWbddttt1tGjR71bdCtrrCelpaUNvvZu27bN26W3KR/L6mRfnQkAADoFzskBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJH+H9z05Ze6YW3iAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "df.hist(column='product_id')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that product_id=4(Credit Reporting) is having the max. complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done the exercise correctly you should observe a class imbalance with `credit reporting` having the most complaints. This can result in some difficulties for standard algorithms, making them biased towards the majority class and treating the minority classes as outliers and unimportant. One way to overcome this problem is by using **undersampling** or **oversampling**. However, this is beyond the scope of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Product</th>\n",
       "      <th>narrative</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "      <td>I was sold access to an event digitally, of wh...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>I LEASED MY CAR ON XX/XX/XXXX AND MADE PAYMENT...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>My original lender was XXXX XXXX XXXX ; this l...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>I was misled by XXXX XXXX  and my degree did n...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>I purchased a vehicle 3 years ago. As of recen...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165804</th>\n",
       "      <td>559376</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>I had called Citibank on XXXX/XXXX/XXXX around...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165805</th>\n",
       "      <td>559377</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>I contacted FedLoan Servicing via their \" Cont...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165806</th>\n",
       "      <td>559378</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>Over the life of my Student Loans through XXXX...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165807</th>\n",
       "      <td>559379</td>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>I was on automatic payment for my car loan. In...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165808</th>\n",
       "      <td>559380</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>I made a payment to CITI XXXX Credit Card on X...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165809 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                            Product  \\\n",
       "0            1  Money transfer, virtual currency, or money ser...   \n",
       "1            5                              Vehicle loan or lease   \n",
       "2            6                                       Student loan   \n",
       "3            8                                       Student loan   \n",
       "4           10                              Vehicle loan or lease   \n",
       "...        ...                                                ...   \n",
       "165804  559376                            Bank account or service   \n",
       "165805  559377                                       Student loan   \n",
       "165806  559378                                       Student loan   \n",
       "165807  559379                                      Consumer Loan   \n",
       "165808  559380                                        Credit card   \n",
       "\n",
       "                                                narrative  product_id  \n",
       "0       I was sold access to an event digitally, of wh...           5  \n",
       "1       I LEASED MY CAR ON XX/XX/XXXX AND MADE PAYMENT...          12  \n",
       "2       My original lender was XXXX XXXX XXXX ; this l...          11  \n",
       "3       I was misled by XXXX XXXX  and my degree did n...          11  \n",
       "4       I purchased a vehicle 3 years ago. As of recen...          12  \n",
       "...                                                   ...         ...  \n",
       "165804  I had called Citibank on XXXX/XXXX/XXXX around...           0  \n",
       "165805  I contacted FedLoan Servicing via their \" Cont...          11  \n",
       "165806  Over the life of my Student Loans through XXXX...          11  \n",
       "165807  I was on automatic payment for my car loan. In...           2  \n",
       "165808  I made a payment to CITI XXXX Credit Card on X...           3  \n",
       "\n",
       "[165809 rows x 4 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: Text Representation and Training the Classifier \n",
    "Before performing any classification we need to split our data into train and test sets. Use `sklearn` to save 20 percent of the data for the test and the rest for training. Make sure to input the index of the data frame to retrieve the indices of the test and train. To ensure reproducibility, use `random_state=42`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              1\n",
       "1              5\n",
       "2              6\n",
       "3              8\n",
       "4             10\n",
       "           ...  \n",
       "165804    559376\n",
       "165805    559377\n",
       "165806    559378\n",
       "165807    559379\n",
       "165808    559380\n",
       "Name: index, Length: 165809, dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df12['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Product', 'narrative', 'product_id'], dtype='object')"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df12.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df12[['index','Product','narrative']]\n",
    "df_test = df12[['product_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[['Product','narrative']]\n",
    "df_test = df[['product_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#labels =df12.product_id.unique()\n",
    "#indices = df12['index']\n",
    "X_train, X_test, y_train, y_test, indices_train,indices_test = train_test_split(df_train,df_test, df.index,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132647, 2)\n",
      "(33162, 2)\n",
      "(132647, 1)\n",
      "(33162, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform any sort of classification task, we first need to convert our raw text into some vector representation. Let's use the `TfidfVectorizer` from `sklearn` to convert the `narrative` column into TF-IDF vectors. When transforming the text keep the following in mind:\n",
    "- use the logarithmic form for frequency\n",
    "- remove accents (ASCII) \n",
    "- lowercase all characters \n",
    "- remove `English` stop words \n",
    "- ignore terms that have a document frequency strictly less than 10\n",
    "- smooth IDF weights by adding one to document frequencies \n",
    "- output row should have unit L2 norm\n",
    "- set the encoding to `Latin-1`\n",
    "- extract both uni-grams and bi-grams \n",
    "- build a vocabulary that only considers the top 10.000 features\n",
    "Keep in mind that the vectorizer should be trained **only** on the training data, and the test data should be transformed using the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423723    My Private student loan with XXXX XXXX was tra...\n",
       "443219    I am rejecting this response because : I have ...\n",
       "34891     This particular account situation that is late...\n",
       "492831    I have written Transunion for help but to no a...\n",
       "553372    I had a consumer Boat loan that I have had at ...\n",
       "                                ...                        \n",
       "20010     On XX/XX/2019 I transferred {$500.00} from my ...\n",
       "477955    XXXX XXXX XXXX taken in XXXX was added to pare...\n",
       "466647    EARLIER IN THE YEAR I CHECKED MY CREDIT REPORT...\n",
       "489829    I will like to file a CFPB Complaint against W...\n",
       "497490    I 'm still in school, getting ready to do my i...\n",
       "Name: narrative, Length: 132647, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132647, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(lowercase=True,strip_accents='ascii',stop_words='english',min_df=10,norm='l2',encoding='Latin-1',smooth_idf=True,sublinear_tf=True,ngram_range = (1,2),max_features=10000)\n",
    "X_train = tfidf.fit_transform(X_train['narrative'])\n",
    "X_train.shape # should be (132647, 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test1 = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data transformation, we attain the features and labels, to train the classifier. In our case, we use **Naive Bayes Classifier**. \n",
    "- use `MultinomialNB` from sklearn to classify the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anureddy/.local/share/virtualenvs/spark_test-F1e0py6d/lib/python3.10/site-packages/sklearn/utils/validation.py:1141: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33162, 1)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id\n",
      "4             6302\n",
      "11            5475\n",
      "1             5346\n",
      "3             3772\n",
      "0             2942\n",
      "5             2338\n",
      "12            2251\n",
      "2             1874\n",
      "9             1792\n",
      "8              371\n",
      "6              340\n",
      "10             300\n",
      "7               55\n",
      "13               4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# examine class distribution\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [33162, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# calculate accuracy of class predictions\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics\n\u001b[0;32m----> 3\u001b[0m metrics\u001b[39m.\u001b[39;49maccuracy_score(y_test, y_pred)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_test-F1e0py6d/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    199\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    202\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_test-F1e0py6d/lib/python3.10/site-packages/sklearn/metrics/_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_test-F1e0py6d/lib/python3.10/site-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     60\u001b[0m     \u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m     87\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/spark_test-F1e0py6d/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [33162, 2]"
     ]
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = np.argmax(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Model Evaluation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our model using the held-out test data. We are going to look at the confusion matrix to show the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "conf = ### create the confusion matrix ### \n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(conf, annot=True, fmt='d',\n",
    "            xticklabels=###products names from the dictionary ###,\n",
    "            yticklabels=### products names from the dictionary###)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the predictions end up on the diagonal (predicted label = actual label). The diagonal shows the correct classified classes. However, there are several misclassifications, specifically `Checking or savings account` is often confused with `Bank account or service`. Let's take a look at why this happens. For this, we look at 5 misclassified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = 'Bank account or service'\n",
    "predicted = 'Checking or savings account'\n",
    "### print only the top 5 \n",
    "df###choose the ones that have an actual label of Bank account or service and the predicted label of Checking or savings account ###)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of the misclassified complaints are complaints that are not easy to distinguish. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Auto-Complete (2 + 5 + 4 = 11 points)\n",
    "Let's get even more practical! In this problem set, you will build your own auto-completion system that you see every day while using search engines.\n",
    "\n",
    "[google]: https://www.thedad.com/wp-content/uploads/2018/05/screen-shot-2018-05-12-at-2-01-56-pm.png \"google auto complete\"\n",
    "\n",
    "![google]\n",
    "\n",
    "By the end of this assignment, you will develop a simple prototype of such a system using n-gram language models. At the heart of the system is a language model that assigns the probability to a sequence of words. We take advantage of this probability calculation to predict the next word. \n",
    "\n",
    "The problem set contains 3 main parts:\n",
    "\n",
    "1. Load and preprocess data (tokenize and split into train and test)\n",
    "2. Develop n-gram based language models by estimating the conditional probability of the next word.\n",
    "3. Evaluate the model by computing the perplexity score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 1: Load and Preprocess Data \n",
    "We use a subset of English tweets to train our model. Run the cell below to load the data and observe a few lines of it. Notice that tweets are saved in a text file, where tweets are separated by `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "with open(\"data/twitter.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "print(\"First 500 characters of the data:\")\n",
    "display(data[0:500])\n",
    "print(\"-------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to separate the tweets and split them into train and test set. Apply the following pre-processing steps:\n",
    "\n",
    "1. Split data into sentences using \"\\n\" as the delimiter and remove the leading and trailing spaces (drop empty sentences)\n",
    "2. Tokenize the sentences into words using SpaCy and lowercase them. (notice that we do not remove stop words or punctuations.) \n",
    "3. Divide the sentences into 80 percent training and 20 percent test set. No validation set is required, although in a real-world application it is best to set aside part of the data for hyperparameter tuning.\n",
    "4. To limit the vocabulary and remove potential spelling mistakes, make a vocabulary of the words that appear at least 2 times. The rest of the words will be replaced by the `<unk>` symbol. This is a crucial step since if your model encounters a word that it never saw during training, it won't have an input word that helps determining the next word for suggestion. We use the `<unk>` word for **out of Vocabulary (OOV)** words. Keep in mind that we built the vocabulary on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = #split\n",
    "sentences = #remove spaces and drop empty sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [] # list of list of the tokens in a sentence \n",
    "##Your Code###   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "Random(4).shuffle(tokenized_corpus)\n",
    "\n",
    "train = ##Your Code###\n",
    "test = ##Your Code###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "flatten_corpus = ### Flatten the train corpus ### \n",
    "word_counts = ### count the number of each token ### \n",
    "vocab = []\n",
    "\n",
    "### keep only the ones with frequency bigger than 2 ### \n",
    "print(len(vocab)) ### should be 14930 ### \n",
    "train_replaced = []\n",
    "test_replaced = []\n",
    "for sentence in train:\n",
    "    ### adjust the sentence to contain the word in the vocabulary and <unk> for the rest #### \n",
    "for sentence in test:\n",
    "    ### adjust the sentence to contain the word in the vocabulary and <unk> for the rest #### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: N-gram Based Language Model: \n",
    "In this section, you will develop the n-grams language model. We assume that the probability of the next word depends only on the previous n-gram or previous n words. We compute this probability by counting the occurrences in the corpus.\n",
    "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ can be estimated as follows:\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})}  $$\n",
    "\n",
    "The numerator is the number of times word 't' appears after the n-gram, and the denominator is the number of times the n-gram occurs in the corpus, where $C(\\cdots)$ is a count function. Later, we add k-smoothing to avoid errors when any counts are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the problem of probability estimation we divide the problem into 3 parts. In the following you will: \n",
    "1. Implement a function that computes the counts of n-grams for an arbitrary number n.\n",
    "2. Estimate the probability of a word given the prior n-words using the n-gram counts.\n",
    "3. Calculate probabilities for all possible words.\n",
    "The steps are detailed below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by implementing a function that computes the counts of n-grams for an arbitrary number n.\n",
    "- Prepend necessary starting markers `<s>` to indicate the beginning of the sentence. In the case of a bi-gram model, you need to prepend two start tokens `<s><s>` to be able to predict the first word. \"hello world\"-> \"`<s><s>`hello world\".\n",
    "- Append an end token `<e>` so that the model can predict when to finish a sentence.\n",
    "- Create a dictionary to store all the n-gram counts (called n_gram in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def n_grams_counts(corpus, n):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the corpus given the parameter n \n",
    "    \n",
    "    data: List of lists of words (your tokenized corpus)\n",
    "    n: n in the n-gram\n",
    "    \n",
    "    Returns: A dictionary that maps a tuple of n words to its frequency\n",
    "    \"\"\"\n",
    "    start_token='<s>'\n",
    "    end_token = '<e>'\n",
    "    n_grams = defaultdict(int)\n",
    "    for sentence in corpus: \n",
    "        sentence = ### add start and end token ###\n",
    "        # convert list to tuple so it can be used a the key in the dictionary \n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        ###iterate over the n-grams in the sentence, you can use the range() function, and increament the counts in the\n",
    "        ## n_grams dictionary, where the key is the n_gram and the value is count \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to estimate the probability of a word given the prior n words using the n-gram counts, based on the formula given at the beginning of this task. To deal with the problem of zero division we add k-smoothing. K-smoothing adds a positive constant $k$ to each numerator and $k \\times |vocabulary size|$ in the denominator. Below we will define a function that takes in a dictionary `n_gram_cnt`, where the key is the n-gram, and the value is the count of that n-gram, plus a dictionary for `plus_current_gram_cnt`, which you'll use to find the count for the previous n-gram plus the current word. Notice that these dictionaries are computed using the previous function `n_grams_counts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(word, prev_n_gram, \n",
    "                         n_gram_cnts, n_plus1_gram_cnts, vocab_size):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    word: next word\n",
    "    prev_n_gram: previous n gram\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab_size: number of words in the vocabulary\n",
    "    \n",
    "    Returns: A probability\n",
    "    \"\"\"\n",
    "    k=1.0\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "    \n",
    "    prev_n_gram_cnt =  # get the previous n-gram count from the dictionary \n",
    "    denominator = # denominator with the previous n-gram count and k-smoothing\n",
    "    n_plus1_gram =  # add the current word to the n-gram \n",
    "    n_plus1_gram_cnt =  # get the current n-gram count using the dictionary\n",
    "    numerator = #calculate the numerator with k-smoothing\n",
    "    prob =\n",
    "    \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the functions we have defined to calculate probabilities for all possible words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities(prev_n_gram, n_gram_cnts, plus_current_gram_cnts, vocab):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities for all the words in the vocabulary given the previous n-gram \n",
    "    prev_n_gram: previous n-gram\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cntsplus_current_gram_cnt: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab: List of words\n",
    "    \n",
    "    Returns: A dictionary mapping from next words to the probability.\n",
    "    \"\"\"\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "    \n",
    "    vocab =  # add <e> <unk> to the vocabulary\n",
    "    vocabulary_size = #compute the size \n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocab:\n",
    "        ### compute the probability \n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Predict the probability of the all possible words after the unigram \"the\"\n",
    "sentences = [['the', 'moon', 'and', 'stars', 'are','shining','bright'],\n",
    "             ['the', 'moon', 'is', 'shinnig','tonight'],\n",
    "             ['mars','and' ,'moon', 'are', 'plants'],\n",
    "             ['the' ,'moon', 'is','a', 'plant']]\n",
    "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]+ sentences[3]))\n",
    "unigram_counts = n_grams_counts(sentences, 1)\n",
    "bigram_counts = n_grams_counts(sentences, 2)\n",
    "print(\"The word 'moon' should have the highest probability, if it is not the case, re-visit your previous functions.\")\n",
    "probabilities([\"the\"], unigram_counts, bigram_counts, unique_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Evaluation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we use the perplexity score to evaluate your model on the test set.\n",
    "The perplexity score of the test set on an n-gram model is defined as follows: \n",
    "\n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } $$\n",
    "- where $N$ is the length of the sentence. ($N-1$ is used because in the code we start from the index 0).\n",
    "- $n$ is the number of words in the n-gram.\n",
    "\n",
    "Notice that we have already computed this probability. \n",
    "\n",
    "The higher the probabilities are, the lower the perplexity will be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(sentence, n_gram_cnts, plus_current_gram_cnts, vocab_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    sentence: List of strings\n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab_size: number of unique words in the vocabulary\n",
    "    k: positive smoothing constant\n",
    "    \n",
    "    Returns: Perplexity score for a single sentence \n",
    "    \"\"\"\n",
    "    \n",
    "    n =  # get the number 'n' in  n-gram  from n_gram_cnts  \n",
    "    \n",
    "    sentence =  # prepend <s> and append <e>\n",
    "    sentence = tuple(sentence)\n",
    "    N =# length of sentence \n",
    "    \n",
    "   \n",
    "    product_pi = 1.0 \n",
    "    \n",
    "    ### Compute the product of probabilites ###\n",
    "    \n",
    "    for t in range(n, N): \n",
    "        n_gram =# get the n-gram before the predicted word (n-gram before t )\n",
    "        word =  # get the word to be predicted (position t)\n",
    "        prob = probability(\n",
    "        product_pi *= # Update the product of the probabilities\n",
    "    \n",
    "    perplexity = product_pi**(1/float(N)) # Take the Nth root of the product\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to find the perplexity of a bi-gram model on the first instance of training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = ### your code ###\n",
    "trigram_counts = ### your code ###\n",
    "\n",
    "perplexity_train = perplexity(train_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
    "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
    "\n",
    "perplexity_test = perplexity(test_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")# the preprexity for the train sample should be much lower\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use the model we created to generate an auto-complete system that makes suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(up_to_here, n_gram_cnts, plus_current_gram_cnts, vocab , start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    up_to_here: the sentence so far, must have length > n \n",
    "    n_gram_cnts: dictionary of counts of n-grams\n",
    "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
    "    vocab: List of words\n",
    "    start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns: (most likely next word,  probability) \n",
    "    \"\"\"    \n",
    "    n = len(list(n_gram_cnts.keys())[0]) # get the number 'n' in  n-gram  from n_gram_cnts     \n",
    "    previous_n_gram = previous_tokens[-n:] # get the last 'n' words as the previous n-gram from the input sentence\n",
    "\n",
    "    \n",
    "    probabs = # Estimate the probabilities for each word in the vocabulary\n",
    "    \n",
    "    probabs = \n",
    "    ### sort the probability for higher to lower and return the highest probability word,probability tuple\n",
    "    #if start_with is specified then return the highest probability word that starts with that specific character \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model based on the bi-gram model created on the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = ['i', 'like']\n",
    "start_with = 'g'\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,\n",
    "                            trigram_counts, list(vocab), start_with=start_with)\n",
    "suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = ['i', 'like', 'to']\n",
    "start_with = None\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,\n",
    "                            trigram_counts, list(vocab), start_with=start_with)\n",
    "suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hello\", \"my\", \"name\", \"is\"]\n",
    "start_with = None\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,\n",
    "                            trigram_counts, list(vocab), start_with=start_with)\n",
    "suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hello\", \"my\", \"name\", \"is\"]\n",
    "start_with = 'sa'\n",
    "suggestion = suggest_a_word(previous_tokens, bigram_counts,\n",
    "                            trigram_counts, list(vocab), start_with=start_with)\n",
    "suggestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying learning has the advantage that at the beginning of training where the model weights are usually\n",
    "chosen at random, the algorithm can fastly converge towards a minimum. After more and more steps, it can be\n",
    "assumed that the solution is near such a minimum, and only smaller steps are taken, such that the algorithm can \n",
    "now \"fine-tune\" model parameters, instead of altering weights massively, potentially stepping over the optimal\n",
    "the solution again and again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Model Evaluation & Comparison (1 + 2 + 2 + 2 = 7 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we want to evaluate and compare the performance of three different imaginary spam mail classifiers. The file `spam_ham_dataset_predictions.csv` consists of a dataset of e-mails with the labels `ham (0)` and `spam (1)` which was taken over from [Kaggle](https://www.kaggle.com/code/syamkakarla/spam-mail-classifier/data). Additionally, predictions from three different models (A, B and C) were added to the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 1: Class Distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data and looking at it first. Since we want to evaluate the performance of the given classifiers, one of the important aspects to know is how the classes are distributed within the dataset. Therefore, we extract the true distribution of classes from the gold labels as well as the predicted distributions of classes from the predicted labels of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/spam_ham_dataset_predictions.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = df[[### your code ###\n",
    "\n",
    "# renaming the rows for a nicer table and plot\n",
    "class_distribution = class_distribution.rename(index={\n",
    "    \"label_num\": \"Gold Labels\",\n",
    "    \"prediction_model_A\": \"Model A\",\n",
    "    \"prediction_model_B\": \"Model B\",\n",
    "    \"prediction_model_C\": \"Model C\"\n",
    "})\n",
    "\n",
    "# display the class distribution table\n",
    "class_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "class_distribution ### your code ###\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the distribution of classes in the dataset is very unbalanced. There are far more \"ham\" mails than spam mails in the dataset. Model A comes closest to the real distribution, while Model B predicts even fewer spam mails than actually exist. An extreme case is Model C, which classifies all mails as unproblematic \"ham\" mails and assumes no spam mails in the data set.\n",
    "\n",
    "Obviously, based on the predicted distributions, we cannot yet estimate how many labels were actually predicted correctly. Therefore, we calculate this in the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which model made the most correct predictions, we want to calculate the accuracy in the next step. In general, the metric of the accuracy is defined as follows:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{correct classifications}}{\\text{all classifications}}\\\\\n",
    "$$.\n",
    "\n",
    "In the case of binary classification, the accuracy can be calculated as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} &= \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\\\\\n",
    "\\text{Accuracy} &= \\frac{\\text{true positives} + \\text{true negatives}}{\\text{true positives} + \\text{false positives} + \\text{true negatives} + \\text{false negatives}}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define methods that return TP, TN, FP and FN for a given model from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\") -> int:\n",
    "    return ### your code ###\n",
    "\n",
    "def true_negatives(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\") -> int:\n",
    "    return ### your code ###\n",
    "\n",
    "def false_positives(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\") -> int:\n",
    "    return ### your code ###\n",
    "\n",
    "def false_negatives(df: pd.DataFrame, prediction_column: str, gold_label_column: str = \"label_num\") -> int:\n",
    "    return ### your code ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the method for calculating the accuracy using parameters for the values for TP, TN, FP and FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(### your code ###) -> float:\n",
    "    return ### your code ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having defined the methods, we can now calculate the accuracy for each model. To be able to reuse the calculated values and additionally depict them in a DataFrame, we store the results in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for storing all results\n",
    "evaluation_results = {}\n",
    "\n",
    "for model in [\"A\", \"B\", \"C\"]:\n",
    "    # column with the predicted labels from the model in the df\n",
    "    prediction_column = f\"prediction_model_{model}\"\n",
    "\n",
    "    # save TP, FP, TN and FN for this model\n",
    "    evaluation_results[model] = {\n",
    "        ### your code ###\n",
    "    }\n",
    "\n",
    "    # save accuracy for this model\n",
    "    evaluation_results[model][\"accuracy\"] = ### your code ###\n",
    "\n",
    "# create a temporary DataFrame for displaying the results in a table\n",
    "pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that all models have a very high accuracy. This can be explained by the uneven distribution of the data, which is roughly reflected by all models.\n",
    "\n",
    "We also see that Model C has the highest accuracy and is therefore the best classifier according to this metric. However, since Model C does not recognize spam mails at all, but classifies all mails as \"ham\", this model will not add any value in practice. The use of the model would have no effect.\n",
    "\n",
    "Therefore, in the next step we want to look at other metrics with which we can compare the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Precision, Recall, F-measure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two very commonly used metrics for evaluating classifiers are precision and recall. \n",
    "\n",
    "Precision measures the percentage of the items that the classifier detected as positive that are actually positive according to the gold labels. Precision is defined as follows:\n",
    "$$\n",
    "\\text{Precision (\\textit{P})} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "Recall measures the percentage of positive items that the classifier was able to detect as positive. Recall is defined as follows:\n",
    "$$\n",
    "\\text{Recall (\\textit{R})} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define methods to calculate precision and recall based on the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(### your code ###) -> float:\n",
    "    return ### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(### your code ###) -> float:\n",
    "    return ### your code ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate precision and recall for the classifiers to see which classifier performs better overall.\n",
    "\n",
    "*Note*: Since we know that Model C does not generate true positives, both recall and precision will be 0. Besides, we already determined that the model is unsuitable for application. Therefore, we can exclude the model from the following calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude Model C from the following calculations\n",
    "evaluation_results.pop(\"C\", None)\n",
    "\n",
    "for model in evaluation_results:\n",
    "    # save precision and recall for this model\n",
    "    evaluation_results[model][\"precision\"] = ### your code ###\n",
    "    evaluation_results[model][\"recall\"] = ### your code ###\n",
    "\n",
    "pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculations should show that Model B has a significantly higher precision than Model A. However, Model A has a higher recall. Thus, we cannot easily decide which model is better. Therefore, we want to combine the two metrics in order to be able to compare the classifiers on the basis of a single value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One frequently used combination of precision and recall is the F-score. The F-Score is defined as follows:\n",
    "$$\n",
    "F_\\beta = \\frac{(\\beta^2 + 1) P R}{\\beta^2 P + R}\n",
    "$$\n",
    "The $\\beta$ parameter in the formula can be used to weight the importance between precision and recall.\n",
    "\n",
    "The most commonly used value for $\\beta$ is $1$. The resulting metric is called $F_1$ score.\n",
    "$$\n",
    "F_1 = \\frac{2 P R}{P + R}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a method to calculate the F-score based on the required parameters at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_beta(### your code ###) -> float:\n",
    "    return ### your code ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to calculate the $F_1$ score for the classifiers to see which one performs better overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in evaluation_results:\n",
    "    # save the F_1 score for this model\n",
    "    evaluation_results[model][\"F_1\"] = ### your code ###\n",
    "\n",
    "pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After correct calculation you should see that the $F_1$ score of Model A is higher than that of Model B. Accordingly, Model A is the better classifier if we want to weight precision and recall equally."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 4: Adapting the metric to the use case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not stop at that point. Is Model A really better for spam detection than Model B?\n",
    "\n",
    "Consider why Model B might be better than Model A for the spam detection use case in practice. Consider how the metric could be easily adapted for the purpose of spam detection. \n",
    "\n",
    "Calculate an adjusted metric for the models and briefly explain the adjustment and the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in evaluation_results:\n",
    "    evaluation_results[model][### metric name ###] = ### your code ###\n",
    "\n",
    "pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your brief explanation:** \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_test-F1e0py6d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40321ea76b929190c1fa1dda337674cf3c2d5597d57747f8f194229e6c4b62aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
